{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13-1550/13-1550/blob/main/Evaluate_Topic_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiEt4nWsd7Zl"
      },
      "source": [
        "### Evaluate Topic Model in Python: Latent Dirichlet Allocation (LDA)\\\n",
        "##### A step-by-step guide to building interpretable topic models\n",
        "\n",
        "** **\n",
        "*Preface: This article aims to provide consolidated information on the underlying topic and is not to be considered as the original work. The information and the code are repurposed through several online articles, research papers, books, and open-source code*\n",
        "** **\n",
        "\n",
        "In the previous [article](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0), I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using Gensim implementation.\n",
        "\n",
        "Pursuing on that understanding, in this article, we’ll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.\n",
        "\n",
        "### Why evaluate topic models?\n",
        "\n",
        "![img](https://tinyurl.com/y3xznjwq)\n",
        "\n",
        "We know probabilistic topic models, such as LDA, are popular tools for text analysis, providing both a predictive and latent topic representation of the corpus. However, there is a longstanding assumption that the latent space discovered by these models is generally meaningful and useful, and that evaluating such assumptions is challenging due to its unsupervised training process. Besides, there is a no-gold standard list of topics to compare against every corpus.\n",
        "\n",
        "Nevertheless, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods. To do so, one would require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if “the correct thing” has been learned about the corpus, an implicit knowledge and “eyeballing” approaches are used. Ideally, we’d like to capture this information in a single metric that can be maximized, and compared.\n",
        "\n",
        "Let’s take a look at roughly what approaches are commonly used for the evaluation:\n",
        "\n",
        "**Eye Balling Models**\n",
        "- Top N words\n",
        "- Topics / Documents\n",
        "\n",
        "**Intrinsic Evaluation Metrics**\n",
        "- Capturing model semantics\n",
        "- Topics interpretability\n",
        "\n",
        "**Human Judgements**\n",
        "- What is a topic\n",
        "\n",
        "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
        "- Is model good at performing predefined tasks, such as classification\n",
        "\n",
        "Natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to cleanse ambiguity reduces the language to an unnatural form. In this article, we’ll explore more about topic coherence, an intrinsic evaluation metric, and how you can use it to quantitatively justify the model selection.\n",
        "\n",
        "### What is Topic Coherence?\n",
        "\n",
        "Before we understand topic coherence, let’s briefly look at the perplexity measure. Perplexity as well is one of the intrinsic evaluation metric, and is widely used for language model evaluation. It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set.\n",
        "\n",
        "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
        "\n",
        "However, recent studies have shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated.\n",
        "\n",
        "*Optimizing for perplexity may not yield human interpretable topics*\n",
        "\n",
        "This limitation of perplexity measure served as a motivation for more work trying to model the human judgment, and thus *Topic Coherence*.\n",
        "\n",
        "The concept of topic coherence combines a number of measures into a framework to evaluate the coherence between topics inferred by a model. But before that…\n",
        "\n",
        "#### What is topic coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
        "\n",
        "#### What is coherence?\n",
        "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But …\n",
        "\n",
        "### Coherence Measures\n",
        "Let’s take quick look at different coherence measures, and how they are calculated:\n",
        "\n",
        "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
        "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
        "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
        "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
        "6. `C_a` is based on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
        "\n",
        "There is, of course, a lot more to the concept of topic model evaluation, and the coherence measure. However, keeping in mind the length, and purpose of this article, let’s apply these concepts into developing a model that is at least better than with the default parameters. Also, we’ll be re-purposing already available online pieces of code to support this exercise instead of re-inventing the wheel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bVflhy7d7Zu"
      },
      "source": [
        "### Model Implementation\n",
        "1. Loading Data\n",
        "2. Data Cleaning\n",
        "3. Phrase Modeling: Bi-grams and Tri-grams\n",
        "4. Data Transformation: Corpus and Dictionary\n",
        "5. Base Model\n",
        "6. Hyper-parameter Tuning\n",
        "7. Final model\n",
        "8. Visualize Results\n",
        "\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file\n",
        "\n",
        "** **\n",
        "#### Step 1: Loading Data\n",
        "** **\n",
        "\n",
        "For this tutorial, we’ll use the dataset of papers published in NIPS conference. The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "zmF6lK6Gd7Zx",
        "outputId": "7f398e68-1f37-41de-8eaa-70e9614168e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0faac78e-9cf3-4d93-bf08-80e30e90b9e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0faac78e-9cf3-4d93-bf08-80e30e90b9e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0faac78e-9cf3-4d93-bf08-80e30e90b9e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0faac78e-9cf3-4d93-bf08-80e30e90b9e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2b04397c-a8b7-453f-917d-91ce7bd45c72\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b04397c-a8b7-453f-917d-91ce7bd45c72')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2b04397c-a8b7-453f-917d-91ce7bd45c72 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KjHZqAod7Z0"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7sQaDnXzd7Z1",
        "outputId": "14f1344b-b5ef-48bb-b329-7b240a201b07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             paper_text\n",
              "1379  The Effect of Singularities in a Learning\\nMac...\n",
              "3714  Scalable Training of Mixture Models via Corese...\n",
              "1283  Feature Selection by Maximum Marginal\\nDiversi...\n",
              "3756  Learning Sparse Representations of High\\nDimen...\n",
              "1053  Fast, large-scale transformation-invariant\\ncl..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-afca6b04-966d-41fb-a114-027d8a8e25bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1379</th>\n",
              "      <td>The Effect of Singularities in a Learning\\nMac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3714</th>\n",
              "      <td>Scalable Training of Mixture Models via Corese...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>Feature Selection by Maximum Marginal\\nDiversi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3756</th>\n",
              "      <td>Learning Sparse Representations of High\\nDimen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>Fast, large-scale transformation-invariant\\ncl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-afca6b04-966d-41fb-a114-027d8a8e25bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-afca6b04-966d-41fb-a114-027d8a8e25bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-afca6b04-966d-41fb-a114-027d8a8e25bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e98c275-3307-43e2-9b86-bce79ec013f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e98c275-3307-43e2-9b86-bce79ec013f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e98c275-3307-43e2-9b86-bce79ec013f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Margin Maximizing Loss Functions\\n\\nSaharon Rosset\\nWatson Research Center\\nIBM\\nYorktown, NY, 10598\\nsrosset@us.ibm.com\\n\\nJi Zhu\\nDepartment of Statistics\\nUniversity of Michigan\\nAnn Arbor, MI, 48109\\njizhu@umich.edu\\n\\nTrevor Hastie\\nDepartment of Statistics\\nStanford University\\nStanford, CA, 94305\\nhastie@stat.stanford.edu\\n\\nAbstract\\nMargin maximizing properties play an important role in the analysis of classi?cation models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis,\\nand practically interesting because it presents a clear geometric interpretation of\\nthe models being built. We formulate and prove a suf?cient condition for the\\nsolutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM,\\nthe exponential loss of AdaBoost and logistic regression loss. We also generalize\\nit to multi-class classi?cation problems, and present margin maximizing multiclass versions of logistic regression and support vector machines.\\n\\n1\\n\\nIntroduction\\n\\nAssume we have a classi?cation ?learning? sample {x i , yi }ni=1 with yi ? {?1, +1}. We\\nwish to build\\n\\u0001 a model F (x) for\\n\\u0001this data by minimizing (exactly or approximately) a loss\\ncriterion i C(yi , F (xi )) = i C(yi F (xi )) which is a function of the margins yi F (xi )\\nof this model on this data. Most common classi?cation modeling approaches can be cast\\nin this framework: logistic regression, support vector machines, boosting and more. The\\nmodel F (x) which these methods actually build is a linear combination of dictionary functions coming from a dictionary H which can be large or even in?nite:\\n\\u0002\\nF (x) =\\n?j hj (x)\\nhj ?H\\n\\nand our prediction at point x based on this model is sgnF (x).\\nWhen |H| is large, as is the case in most boosting or kernel SVM applications, some regularization is needed to control the ?complexity? of the model F (x) and the resulting over?tting. Thus, it is common that the quantity actually minimized on the data is a regularized\\nversion of the loss function:\\n\\u0002\\n?\\n?(?)\\n= min\\n(1)\\nC(yi ? \\u0002 h(xi )) + ?\\u0002?\\u0002p\\n?\\n\\np\\n\\ni\\n\\nwhere the second term penalizes for the lp norm of the coef?cient vector ? (p ? 1 for\\nconvexity, and in practice usually p ? {1, 2}), and ? ? 0 is a tuning regularization parameter. The 1- and 2-norm support vector machine training problems with slack can be cast in\\nthis form ([6], chapter 12). In [8] we have shown that boosting approximately follows the\\n\\n\\f?path? of regularized solutions traced by (1) as the regularization parameter ? varies, with\\nthe appropriate loss and an l1 penalty.\\n?\\nThe main question that we answer in this paper is: for what loss functions does ?(?)\\nconverge to an ?optimal? separator as ? ? 0? The de?nition of ?optimal? which we will\\nuse depends on the lp norm used for regularization, and we will term it the ?lp -margin\\nmaximizing separating hyper-plane?. More concisely, we will investigate for which loss\\nfunctions and under which conditions we have:\\n?\\n?(?)\\nlim\\n(2)\\n= arg max min yi ? \\u0002 h(xi )\\n?\\n??0 \\u0002?(?)\\u0002\\n\\u0004?\\u0004p =1 i\\nThis margin maximizing property is interesting for three distinct reasons. First, it gives us\\na geometric interpretation of the ?limiting? model as we relax the regularization. It tells\\nus that this loss seeks to optimally separate the data by maximizing a distance between a\\nseparating hyper-plane and the ?closest? points. A theorem by Mangasarian [7] allows us\\nto interpret lp margin maximization as lq distance maximization, with 1/p + 1/q = 1, and\\nhence make a clear geometric interpretation. Second, from a learning theory perspective\\nlarge margins are an important quantity ? generalization error bounds that depend on\\nthe margins have been generated for support vector machines ([10] ? using l2 margins)\\nand boosting ( [9] ? using l1 margins). Thus, showing that a loss function is ?margin\\nmaximizing? in this sense is useful and promising information regarding this loss function?s\\npotential for generating good prediction models. Third, practical experience shows that\\nexact or approximate margin maximizaion (such as non-regularized kernel SVM solutions,\\nor ?in?nite? boosting) may actually lead to good classi?cation prediction models. This is\\ncertainly not always the case, and we return to this hotly debated issue in our discussion.\\nOur main result is a suf?cient condition on the loss function, which guarantees that (2)\\nholds, if the data is separable, i.e. if the maximum on the RHS of (2) is positive. This\\ncondition is presented and proven in section 2. It covers the hinge loss of support vector\\nmachines, the logistic log-likelihood loss of logistic regression, and the exponential loss,\\nmost notably used in boosting. We discuss these and other examples in section 3. Our result\\ngeneralizes elegantly to multi-class models and loss functions. We present the resulting\\nmargin-maximizing versions of SVMs and logistic regression in section 4.\\n\\n2\\n\\nSuf?cient condition for margin maximization\\n\\nThe following theorem shows that if the loss function vanishes ?quickly? enough, then it\\nwill be margin-maximizing as the regularization vanishes. It provides us with a uni?ed\\nmargin-maximization theory, covering SVMs, logistic regression and boosting.\\nTheorem 2.1 Assume the data {xi , yi }ni=1 is separable, i.e. ?? s.t. mini yi ? \\u0002 h(xi ) > 0.\\nLet C(y, f ) = C(yf ) be a monotone non-increasing loss function depending on the margin\\nonly.\\nIf ?T > 0 (possibly T = ? ) such that:\\nC(t ? [1 ? \\u0003])\\nlim\\n(3)\\n= ?, ?\\u0003 > 0\\nt\\u0005T\\nC(t)\\nThen C is a margin maximizing loss function in the sense that any convergence point of the\\n?\\n?(?)\\nnormalized solutions \\u0004?(?)\\u0004\\nto the regularized problems (1) as ? ? 0 is an lp margin?\\np\\nmaximizing separating hyper-plane. Consequently, if this margin-maximizing hyper-plane\\nis unique, then the solutions converge to it:\\n?\\n?(?)\\n(4)\\n= arg max min yi ? \\u0002 h(xi )\\nlim\\n?\\n??0 \\u0002?(?)\\u0002\\n\\u0004?\\u0004p =1 i\\np\\n\\n\\fProof We prove the result separately for T = ? and T < ?.\\na. T = ?:\\n??0\\n\\n?\\nLemma 2.2 \\u0002?(?)\\u0002\\np ?? ?\\nProof Since T = ? then C(m) > 0 ?m > 0, and limm?? C(m) = 0. Therefore, for\\n?\\nloss+penalty to vanish as ? ? 0, \\u0002?(?)\\u0002\\np must diverge, to allow the margins to diverge.\\nLemma 2.3 Assume ?1 , ?2 are two separating models, with \\u0002?1 \\u0002p = \\u0002?2 \\u0002p = 1, and ?1\\nseparates the data better, i.e.: 0 < m2 = mini yi h(xi )\\u0002 ?2 < m1 = mini yi h(xi )\\u0002 ?1 .\\nThen ?U = U (m1 , m2 ) such that\\n\\u0002\\n\\u0002\\nC(yi h(xi )\\u0002 (t?1 )) <\\nC(yi h(xi )\\u0002 (t?2 ))\\n?t > U,\\ni\\n\\ni\\n\\nIn words, if ?1 separates better than ?2 then scaled-up versions of ?1 will incur smaller\\nloss than scaled-up versions of ?2 , if the scaling factor is large enough.\\n2)\\nProof Since condition (3) holds with T = ?, there exists U such that ?t > U, C(tm\\nC(tm1 ) >\\nn. Thus from C being non-increasing we immediately get:\\n\\u0002\\n\\u0002\\n?t > U,\\nC(yi h(xi )\\u0002 (t?1 )) ? n ? C(tm1 ) < C(tm2 ) <\\nC(yi h(xi )\\u0002 (t?2 ))\\n\\ni\\n\\ni\\n?\\n\\n?(?)\\nProof of case a.: Assume ? ? is a convergence point of \\u0004?(?)\\u0004\\nas ? ? 0, with \\u0002? ? \\u0002p = 1.\\n?\\np\\n? p = 1 and bigger minimal lp margin. Denote the\\nNow assume by contradiction ?? has \\u0002?\\u0002\\nminimal margins for the two models by m? and m,\\n? respectively, with m? < m.\\n?\\nBy continuity of the minimal margin in ?, there exists some open neighborhood of ? ? on\\nthe lp sphere:\\nN? ? = {? : \\u0002?\\u0002p = 1, \\u0002? ? ? ? \\u00022 < ?}\\nand an \\u0003 > 0, such that:\\n\\nmin yi ? \\u0002 h(xi ) < m\\n? ? \\u0003, ?? ? N? ?\\ni\\n\\nNow by lemma 2.3 we get that exists U = U (m,\\n? m\\n? ? \\u0003) such that t?? incurs smaller loss\\nthan t? for any t > U, ? ? N? ? . Therefore ? ? cannot be a convergence point of\\n\\n?\\n?(?)\\n.\\n?\\n\\u0004?(?)\\u0004\\np\\n\\nb. T < ?\\nLemma 2.4 C(T ) = 0 and C(T ? ?) > 0, ?? > 0.\\nProof From condition (3),\\n\\nC(T ?T \\u0003)\\nC(T )\\n\\n= ?. Both results follow immediately, with ? = T \\u0003.\\n\\n? \\u0002 h(xi ) = T\\nLemma 2.5 lim??0 mini yi ?(?)\\nProof Assume by contradiction that there is a sequence ?1 , ?2 , ... \\t 0 and \\u0003 > 0 s.t.\\n? j )\\u0002 h(xi ) ? T ? \\u0003.\\n?j, mini yi ?(?\\n? p = 1 and m\\nPick any separating normalized model ?? i.e. \\u0002?\\u0002\\n? := mini yi ??\\u0002 h(xi ) > 0.\\np C(T ?\\u0003)\\nwe get:\\nThen for any ? < m\\n?\\nTp\\n\\u0002\\nT ? p\\nT\\nC(yi ??\\u0002 h(xi )) + ?\\u0002 ?\\u0002\\n< C(T ? \\u0003)\\nm\\n?\\nm\\n? p\\ni\\n\\n\\fsince the ?rst term (loss) is 0 and the penalty is smaller than C(T ? \\u0003) by condition on ?.\\n?\\u0003)\\n? j ), since\\n? p C(T\\nand so we get a contradiction to optimality of ?(?\\nBut ?j0 s.t. ?j0 < m\\n0\\nTp\\n\\u0002\\n?\\nwe assumed mini yi ?(?j0 ) h(xi ) ? T ? \\u0003 and thus:\\n\\u0002\\n? j )\\u0002 h(xi )) ? C(T ? \\u0003)\\nC(yi ?(?\\n0\\ni\\n\\n? \\u0002 h(xi ) ? T . It remains to prove equality.\\nWe have thus proven that lim inf ??0 mini yi ?(?)\\n? \\u0002 h(xi ) > T .\\nAssume by contradiction that for some value of ? we have m := mini yi ?(?)\\nT ?\\n?\\nThen the re-scaled model m ?(?) has the same zero loss as ?(?), but a smaller penalty,\\nT ?\\nT ?\\n?\\n?\\n\\u0002?(?)\\u0002 < \\u0002?(?)\\u0002.\\nSo we get a contradiction to optimality of ?(?).\\n?(?)\\u0002 = m\\nsince \\u0002 m\\n?\\n\\n?(?)\\nas ? ? 0, with \\u0002? ? \\u0002p = 1.\\nProof of case b.: Assume ? ? is a convergence point of \\u0004?(?)\\u0004\\n?\\np\\n? p = 1 and bigger minimal margin. Denote the\\nNow assume by contradiction ?? has \\u0002?\\u0002\\nminimal margins for the two models by m? and m,\\n? respectively, with m? < m.\\n?\\n? j)\\n?(?\\n?\\nLet ?1 , ?2 , ... \\t 0 be a sequence along which \\u0004?(?\\n? j )\\u0004p ? ? . By lemma 2.5 and our\\nT\\nT\\nT\\n?\\n?\\nassumption, \\u0002?(?j )\\u0002p ? m? > m\\n? . Thus, ?j0 such that ?j > j0 , \\u0002?(?j )\\u0002p > m\\n? and\\nconsequently:\\n\\u0002\\n\\u0002\\nT ? p\\nT ?\\n? j )\\u0002p > ?( T )p =\\n? j )\\u0002 h(xi )) + ?\\u0002?(?\\nC(yi ?(?\\nC(yi ?h(x\\n?\\u0002\\ni )) + ?\\u0002\\np\\nm\\n?\\nm\\n?\\nm\\n? p\\ni\\ni\\n\\n? j ).\\nSo we get a contradiction to optimality of ?(?\\nThus we conclude for both cases a. and b. that any convergence point of\\n\\n?\\n?(?)\\n?\\n\\u0004?(?)\\u0004\\np\\n\\nmust\\n\\n?\\n?(?)\\n\\u0002 \\u0004?(?)\\u0004\\n\\u0002p\\n?\\np\\n\\nmaximize the lp margin. Since\\n= 1, such convergence points obviously exist.\\nIf the lp -margin-maximizing separating hyper-plane is unique, then we can conclude:\\n?\\n?(?)\\n? ?? := arg max min yi ? \\u0002 h(xi )\\n?\\n\\u0004?\\u0004p =1 i\\n\\u0002?(?)\\u0002p\\nNecessity results\\nA necessity result for margin maximization on any separable data seems to require either\\nadditional assumptions on the loss or a relaxation of condition (3). We conjecture that if we\\nalso require that the loss is convex and vanishing (i.e. limm?? C(m) = 0) then condition\\n(3) is suf?cient and necessary. However this is still a subject for future research.\\n\\n3\\n\\nExamples\\n\\nSupport vector machines\\nSupport vector machines (linear or kernel) can be described as a regularized problem:\\n\\u0002\\n(5)\\n[1 ? yi ? \\u0002 h(xi )]+ + ?\\u0002?\\u0002pp\\nmin\\n?\\n\\ni\\n\\nwhere p = 2 for the standard (?2-norm?) SVM and p = 1 for the 1-norm SVM. This\\nformulation is equivalent to the better known ?norm minimization? SVM formulation in\\nthe sense that they have the same set of solutions as the regularization parameter ? varies\\nin (5) or the slack bound varies in the norm minimization formulation.\\n\\n\\fThe loss in (5) is termed ?hinge loss? since it?s linear for margins less than 1, then ?xed\\nat 0 (see ?gure 1). The theorem obviously holds for T = 1, and it veri?es our knowledge\\nthat the non-regularized SVM solution, which is the limit of the regularized solutions,\\nmaximizes the appropriate margin (Euclidean for standard SVM, l1 for 1-norm SVM).\\nNote that our theorem indicates that the squared hinge loss (AKA truncated squared loss):\\nC(yi , F (xi )) = [1 ? yi F (xi )]2+\\nis also a margin-maximizing loss.\\nLogistic regression and boosting\\nThe two loss functions we consider in this context are:\\n(6)\\n(7)\\n\\nExponential :\\nLog likelihood :\\n\\nCe (m) = exp(?m)\\nCl (m) = log(1 + exp(?m))\\n\\nThese two loss functions are of great interest in the context of two class classi?cation: C l\\nis used in logistic regression and more recently for boosting [4], while Ce is the implicit\\nloss function used by AdaBoost - the original and most famous boosting algorithm [3] .\\nIn [8] we showed that boosting approximately follows the regularized path of solutions\\n?\\n?(?)\\nusing these loss functions and l1 regularization. We also proved that the two loss\\nfunctions are very similar for positive margins, and that their regularized solutions converge\\nto margin-maximizing separators. Theorem 2.1 provides a new proof of this result, since\\nthe theorem?s condition holds with T = ? for both loss functions.\\nSome interesting non-examples\\nCommonly used classi?cation loss functions which are not margin-maximizing include any\\n1\\npolynomial loss function: C(m) = m\\n, C(m) = m2 , etc. do not guarantee convergence of\\nregularized solutions to margin maximizing solutions.\\nAnother interesting method in this context is linear discriminant analysis. Although it does\\nnot correspond to the loss+penalty formulation we have described, it does ?nd a ?decision\\nhyper-plane? in the predictor space.\\nFor both polynomial loss functions and linear discriminant analysis it is easy to ?nd examples which show that they are not necessarily margin maximizing on separable data.\\n\\n4\\n\\nA multi-class generalization\\n\\nOur main result can be elegantly extended to versions of multi-class logistic regression and\\nsupport vector machines, as follows. Assume the response is now multi-class, with K ? 2\\npossible values i.e. yi ? {c1 , ..., cK }. Our model consists of a ?prediction? for each class:\\n\\u0002 (k)\\n?j hj (x)\\nFk (x) =\\nhj ?H\\n\\nwith the obvious prediction rule at x being arg maxk Fk (x).\\nThis gives rise to a K ? 1 dimensional ?margin? for each observation. For y = ck , de?ne\\nthe margin vector as:\\n(8)\\n\\nm(ck , f1 , ..., fK ) = (fk ? f1 , ..., fk ? fk?1 , fk ? fk+1 , ..., fk ? fK )\\u0002\\n\\nAnd our loss is a function of this K ? 1 dimensional margin:\\n\\u0002\\nI{y = ck }C(m(ck , f1 , ..., fK ))\\nC(y, f1 , ..., fK ) =\\nk\\n\\n\\f3\\nhinge\\nexponential\\nlogistic\\n\\n6\\n\\n2.5\\n\\n5\\n4\\n\\n2\\n\\n3\\n1.5\\n\\n2\\n1\\n\\n1\\n\\n0\\n?2\\n?2\\n\\n?1\\n0.5\\n\\n?1\\n\\n0\\n\\n0\\n\\n1\\n\\n1\\n2\\n\\n0\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\n\\n0.5\\n\\n1\\n\\n1.5\\n\\n2\\n3\\n\\n2\\n\\n3\\n\\nFigure 1: Margin maximizing loss functions for 2-class problems (left) and the SVM 3-class loss\\nfunction of section 4.1 (right)\\nThe lp -regularized problem is now:\\n\\u0002\\n\\u0002\\n?\\n(9) ?(?)\\nC(yi , h(xi )\\u0002 ? (1) , ..., h(xi )\\u0002 ? (K) ) + ?\\n\\u0002? (k) \\u0002pp\\n= arg min\\n? (1) ,...,? (K)\\n\\ni\\n\\nk\\n\\n?\\nWhere ?(?)\\n= (??(1) (?), ..., ??(K) (?))\\u0002 ? RK?|H| .\\nIn this formulation, the concept of margin maximization corresponds to maximizing the\\nminimal of all n ? (K ? 1) normalized lp -margins generated by the data:\\n(10)\\n\\nmax\\n\\n(K) \\u0004p =1\\n\\u0004? (1) \\u0004p\\np +...+\\u0004?\\np\\n\\nmin min h(xi )\\u0002 (? (yi ) ? ? (k) )\\ni\\n\\nyi \\b=ck\\n\\nNote that this margin maximization problem still has a natural geometric interpretation, as\\nh(xi )\\u0002 (? (yi ) ? ? (k) ) > 0 ?i, k \\n= yi implies that the hyper-plane h(x)\\u0002 (? (j) ? ? (k) ) = 0\\nsuccessfully separates classes j and k for any two classes.\\nHere is a generalization of the optimal separation theorem 2.1 to multi-class models:\\nTheorem 4.1 Assume C(m) is commutative and decreasing in each coordinate, then if\\n?T > 0 (possibly T = ? ) such that:\\n(11)\\n\\nlimt\\u0005T\\n\\nC(t[1 ? \\u0003], tu1 , ...tuK?2 )\\n= ?,\\nC(t, tv1 , ..., tvK?2 )\\n?\\u0003 > 0, u1 ? 1, ..., uK?2 ? 1, v1 ? 1, ...vK?2 ? 1\\n\\nThen C is a margin-maximizing loss function for multi-class models, in the sense that any\\n?\\n?(?)\\n, attains the optimal separaconvergence point of the normalized solutions to (9), \\u0004?(?)\\u0004\\n?\\np\\ntion as de?ned in (10)\\nIdea of proof The proof is essentially identical to the two class case, now considering the\\nn ? (K ? 1) margins on which the loss depends. The condition (11) implies that as the\\nregularization vanishes the model is determined by the minimal margin, and so an optimal\\nmodel puts the emphasis on maximizing that margin.\\n\\n\\fCorollary 4.2 In the 2-class case, theorem 4.1 reduces to theorem 2.1.\\nProof The loss depends on ? (1) ? ? (2) , the penalty on \\u0002? (1) \\u0002pp + \\u0002? (2) \\u0002pp . An optimal\\nsolution to the regularized problem must thus have ? (1) + ? (2) = 0, since by transforming:\\n? (1) ? ? (1) ?\\n\\n? (1) + ? (2)\\n? (1) + ? (2)\\n, ? (2) ? ? (2) ?\\n2\\n2\\n\\nwe are not changing the loss, but reducing the penalty, by Jensen?s inequality:\\n\\u0002? (1) ?\\n\\n? (1) + ? (2) p\\n? (1) + ? (2) p\\n? (1) ? ? (2) p\\n\\u0002p + \\u0002? (2) ?\\n\\u0002p = 2\\u0002\\n\\u0002p ? \\u0002? (1) \\u0002pp + \\u0002? (2) \\u0002pp\\n2\\n2\\n2\\n\\nSo we can conclude that ??(1) (?) = ???(2) (?) and consequently that the two margin maximization tasks (2), (10) are equivalent.\\n4.1\\n\\nMargin maximization in multi-class SVM and logistic regression\\n\\nHere we apply theorem 4.1 to versions of multi-class logistic regression and SVM.\\nFor logistic regression, we use a slightly different formulation than the ?standard? logistic\\nregression models, which uses class K as a ?reference? class, i.e. assumes that ? (K) = 0.\\nThis is required for non-regularized ?tting, since without it the solution is not uniquely\\nde?ned. However, using regularization as in (9) guarantees that the solution will be unique\\nand consequently we can ?symmetrize? the model ? which allows us to apply theorem\\n4.1. So the loss function we use is (assume y = ck belongs to class k):\\nefk\\n=\\nef1 + ... + efK\\n= log(ef1 ?fk + ... + efk?1 ?fk + 1 + efk+1 ?fk + ... + efK ?fk )\\n\\n(12) C(y, f1 , ..., fK ) = ? log\\n\\nwith the linear model: fj (xi ) = h(xi )\\u0002 ? (j) . It is not dif?cult to verify that condition (11)\\nholds for this loss function with T = ?, using the fact that log(1 + \\u0003) = \\u0003 + O(\\u00032 ). The\\nsum of exponentials which results from applying this ?rst-order approximation satis?es\\n(11), and as \\u0003 ? 0, the second order term can be ignored.\\nFor support vector machines, consider a multi-class loss which is a natural generalization\\nof the two-class loss:\\nC(m) =\\n\\n(13)\\n\\nK?1\\n\\u0002\\n\\n[1 ? mj ]+\\n\\nj=1\\n\\nWhere mj is the j?th component of the multi-margin m as in (8). Figure 1 shows this loss\\nfor K = 3 classes as a function of the two margins. The loss+penalty formulation using 13\\nis equivalent to a standard optimization formulation of multi-class SVM (e.g. [11]):\\nmax c\\ns.t. h(xi )\\u0002 (? (yi ) ? ? (k) ) ? c(1 ? ?ik ), i ? {1, ...n}, k ? {1, ..., K}, ck \\n= yi\\n\\u0002\\n\\u0002\\n?ik ? 0 ,\\n?ik ? B ,\\n\\u0002? (k) \\u0002pp = 1\\ni,k\\n\\nk\\n\\nAs both theorem 4.1 (using T = 1) and the optimization formulation indicate, the regularized solutions to this problem converge to the lp margin maximizing multi-class solution.\\n\\n\\f5\\n\\nDiscussion\\n\\nWhat are the properties we would like to have in a classi?cation loss function? Recently\\nthere has been a lot of interest in Bayes-consistency of loss functions and algorithms ([1]\\nand references therein), as the data size increases. It turns out that practically all ?reasonable? loss functions are consistent in that sense, although convergence rates and other\\nmeasures of ?degree of consistency? may vary.\\nMargin maximization, on the other hand, is a ?nite sample optimality property of loss functions, which is potentially of decreasing interest as sample size grows, since the training\\ndata-set is less likely to be separable. Note, however, that in very high dimensional predictor spaces, such as those typically used by boosting or kernel SVM, separability of any\\n?nite-size data-set is a mild assumption, which is violated only in pathological cases.\\nWe have shown that the margin maximizing property is shared by some popular loss functions used in logistic regression, support vector machines and boosting. Knowing that\\nthese algorithms ?converge?, as regularization vanishes, to the same model (provided they\\nuse the same regularization) is an interesting insight. So, for example, we can conclude\\nthat 1-norm support vector machines, exponential boosting and l1 -regularized logistic regression all facilitate the same non-regularized solution, which is an l1 -margin maximizing\\nseparating hyper-plane. From Mangasarian?s theorem [7] we know that this hyper-plane\\nmaximizes the l? distance from the closest points on either side.\\nThe most interesting statistical question which arises is: are these ?optimal? separating\\nmodels really good for prediction, or should we expect regularized models to always do\\nbetter in practice? Statistical intuition supports the latter, as do some margin-maximizing\\nexperiments by Breiman [2] and Grove and Schuurmans [5]. However it has also been observed that in many cases margin-maximization leads to reasonable prediction models, and\\ndoes not necessarily result in over-?tting. We have had similar experience with boosting\\nand kernel SVM. Settling this issue is an intriguing research topic, and one that is critical in determining the practical importance of our results, as well as that of margin-based\\ngeneralization error bounds.\\n\\nReferences\\n[1]\\n\\nBartlett, P., Jordan, M. & McAuliffe, J. (2003). Convexity, Classi?cation and Risk Bounds.\\nTechnical reports, dept. of Statistics, UC Berkeley.\\n[2] Breiman, L. (1999). Prediction games and arcing algorithms. Neural Computation 7:1493-1517.\\n[3] Freund, Y. & Scahpire, R.E. (1995). A decision theoretic generalization of on-line learning and\\nan application to boosting. Proc. of 2nd Eurpoean Conf. on Computational Learning Theory.\\n[4] Friedman, J. H., Hastie, T. & Tibshirani, R. (2000). Additive logistic regression: a statistical\\nview of boosting. Annals of Statistics 28, pp. 337-407.\\n[5] Grove, A.J. & Schuurmans, D. (1998). Boosting in the limit: Maximizing the margin of learned\\nensembles. Proc. of 15th National Conf. on AI.\\n[6] Hastie, T., Tibshirani, R. & Friedman, J. (2001). Elements of Stat. Learning. Springer-Verlag.\\n[7] Mangasarian, O.L. (1999). Arbitrary-norm separating plane. Operations Research Letters, Vol.\\n24 1-2:15-23\\n[8] Rosset, R., Zhu, J & Hastie, T. (2003). Boosting as a regularized path to a maximum margin\\nclassi?er. Technical report, Dept. of Statistics, Stanford Univ.\\n[9] Scahpire, R.E., Freund, Y., Bartlett, P. & Lee, W.S. (1998). Boosting the margin: a new explanation for the effectiveness of voting methods. Annals of Statistics 26(5):1651-1686\\n[10] Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer.\\n[11] Weston, J. & Watkins, C. (1998). Multi-class support vector machines. Technical report CSDTR-98-04, dept of CS, Royal Holloway, University of London.\\n\\n\\f\",\n          \"Designing neurophysiology experiments to optimally\\nconstrain receptive field models along parametric\\nsubmanifolds.\\nJeremy Lewi ?\\nSchool of Bioengineering\\nGeorgia Institute of Technology\\njeremy@lewi.us\\n\\nRobert Butera\\nSchool of Electrical and Computer Engineering\\nGeorgia Institute of Technology\\nrbutera@ece.gatech.edu\\n\\nDavid M. Schneider\\nDepartments of Neurobiology and Psychology\\nColumbia University\\ndms2159@columbia.edu\\n\\nSarah M. N. Woolley\\nDepartment of Psychology\\nColumbia University\\nsw2277@columbia.edu\\n\\nLiam Paninski ?\\nDepartment of Statistics and Center for Theoretical Neuroscience\\nColumbia University\\nliam@stat.columbia.edu\\n\\nAbstract\\nSequential optimal design methods hold great promise for improving the efficiency of neurophysiology experiments. However, previous methods for optimal\\nexperimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive field).\\nHere we describe how to use stronger prior information, in the form of parametric models of the receptive field, in order to construct optimal stimuli and further\\nimprove the efficiency of our experiments. For example, if we believe that the\\nreceptive field is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial\\nfrequency, etc.) using as few experimental trials as possible. More generally, we\\nmay believe a priori that the receptive field lies near a known sub-manifold of the\\nfull parameter space; in this case, our method chooses stimuli in order to reduce\\nthe uncertainty along the tangent space of this sub-manifold as rapidly as possible.\\nApplications to simulated and real data indicate that these methods may in many\\ncases improve the experimental efficiency.\\n\\n1\\n\\nIntroduction\\n\\nA long standing problem in neuroscience has been collecting enough data to robustly estimate the\\nresponse function of a neuron. One approach to this problem is to sequentially optimize a series\\nof experiments as data is collected [1, 2, 3, 4, 5, 6]. To make optimizing the design tractable, we\\ntypically need to assume our knowledge has some nice mathematical representation. This restriction\\noften makes it difficult to include the types of prior beliefs held by neurophysiologists; for example\\nthat the receptive field has some parametric form such as a Gabor function [7]. Here we consider\\n?\\n?\\n\\nhttp://www.lewilab.org\\nhttp://www.stat.columbia.edu/?liam/\\n\\n1\\n\\n\\f~ t , Ct )\\np(?|?\\n\\n~ b , Cb )\\np(?|?\\nT?~ M,t M\\n\\n?2\\n\\n?\\n~ M,t\\n\\nM\\n\\n?\\n~t\\n?2\\n\\n?2\\n?1\\n\\n?1\\n\\n?1\\n\\nFigure 1: A schematic illustrating how we use the manifold to improve stimulus design. Our\\nmethod begins with a Gaussian approximation of the posterior on the full model space after t trials,\\n~ ?t , C t ). The left panel shows an example of this Gaussian distribution when dim(?)\\n~ = 2. The\\np(?|~\\nnext step involves constructing the tangent space approximation of the manifold M on which ?~ is believed to lie, as illustrated in the middle plot; M is indicated in blue. The MAP estimate (blue dot) is\\nprojected onto the manifold to obtain ?\\n~ M,t (green dot). We then compute the tangent space (dashed\\nred line) by taking the derivative of the manifold at ?\\n~ M,t . The tangent space is the space spanned by\\nvectors in the direction parallel to M at ?\\n~ M,t . By definition, in the neighborhood of ?\\n~ M,t , moving\\nalong the manifold is roughly equivalent to moving along the tangent space. Thus, the tangent space\\n~ ?b,t , Cb,t ) by evaluprovides a good local approximation of M. In the right panel we compute p(?|~\\n~ ?t , C t ) on the tangent space. The resulting distribution concentrates its mass on models\\nating p(?|~\\n~ ?t , C t ) and close to the manifold.\\nwhich are probable under p(?|~\\n\\nthe problem of incorporating this strong prior knowledge into an existing algorithm for optimizing\\nneurophysiology experiments [8].\\nWe start by assuming that a neuron can be modeled as a generalized linear model (GLM). Our\\nprior knowledge defines a subset of all GLMs in which we expect to find the best model of the\\nneuron. We represent this class as a sub-manifold in the parameter space of the GLM. We use the\\nmanifold to design an experiment which will provide the largest reduction in our uncertainty about\\nthe unknown parameters. To make the computations tractable we approximate the manifold using\\nthe tangent space evaluated at the maximum a posteriori (MAP) estimate of the parameters projected\\nonto the manifold. Despite this rather crude approximation of the geometry of the manifold, our\\nsimulations show that this method can significantly improve the informativeness of our experiments.\\nFurthermore, these methods work robustly even if the best model does not happen to lie directly on\\nthe manifold.\\n\\n2\\n\\nMethods\\n\\nWe begin by summarizing the three key elements of an existing algorithm for optimizing neurophysiology experiments. A more thorough discussion is available in [8]. We model the neuron?s\\nresponse function as a mapping between the neuron?s input at time t, ~st , and its response, rt . We\\ndefine the input rather generally as a vector which may consist of terms corresponding to a stimulus,\\ne.g. an image or a sound, or the past activity of the neuron itself, {rt?1 , rt?2 , . . .}. The response, rt ,\\nis typically a non-negative integer corresponding to the number of spikes observed in a small time\\nwindow. Since neural responses are typically noisy, we represent the response function as a con~ In this context, optimizing the experimental design means picking\\nditional distribution, p(rt |~st , ?).\\nthe input for which observing the response will provide the most information about the parameters\\n?~ defining the conditional response function.\\n2\\n\\n\\f~ can be adequately\\nThe first important component of this algorithm is the assumption that p(rt |~st , ?)\\napproximated by a generalized linear model [9, 10]. The likelihood of the response depends on the\\nfiring rate, ?t , which is a function of the input,\\n\\u0010\\n\\u0011\\n?t = E(rt ) = f ?~T ~st ,\\n(1)\\nwhere f () is some nonlinear function which is assumed known1 . To identify the response function,\\n~ One important property of the GLM\\nwe need to estimate the coefficients of the linear projection, ?.\\nis that we can easily derive sufficient conditions to ensure the log-likelihood is concave [11].\\nThe second key component of the algorithm is that we may reasonably approximate the posterior on\\n?~ as Gaussian. This approximation is justified by the log-concavity of the likelihood function and\\nasymptotic normality of the posterior distribution given sufficient data [12]. As a result, we can re~ 1:t , s1:t ) ? p(?|~\\n~ ?t , C t ) [8].\\ncursively compute a Gaussian approximation of the full posterior, p(?|r\\nHere (~\\n?t , C t ) denote the mean and covariance matrix of our Gaussian approximation: ?\\n~ t is set to\\n~ and C t to the inverse Hessian of the log-posterior at ?\\nthe MAP estimate of ?,\\n~ t.\\nThe final component is an efficient method for picking the optimal input on the next trial, ~st+1 .\\nSince the purpose of an experiment is to identify the best model, we optimize the design by max~ rt+1 |~st+1 ). The\\nimizing the conditional mutual information between rt+1 and ?~ given ~st+1 , I(?;\\nmutual information measures how much we expect observing the response to ~st+1 will reduce our\\n~ We pick the optimal input by maximizing the mutual information with respect\\nuncertainty about ?.\\nto ~st+1 ; as discussed in [8], this step, along with the updating of the posterior mean and covariance\\n(~\\n?t , C t ), may be computed efficiently enough for real-time implementation in many cases.\\n2.1\\n\\nOptimizing experiments to reduce uncertainty along parameter sub-manifolds.\\n\\nFor the computation of the mutual information to be tractable, the space of candidate models, ?,\\nmust have some convenient form so that we can derive a suitable expression for the mutual information. Intuitively, to select the optimal design, we need to consider how much information an\\nexperiment provides about each possible model. Evaluating the mutual information entails an integral over model space, ?. The problem with incorporating prior knowledge is that if we restrict\\nthe model to some complicated subset of model space we will no longer be able to efficiently integrate over the set of candidate models. We address this problem by showing how local geometric\\napproximations to the parameter sub-manifold can be used to guide optimal sampling while still\\nmaintaining a flexible, tractable representation of the posterior distribution on the full model space.\\nIn many experiments, neurophysiologists expect a-priori that the receptive field of a neuron will have\\nsome low-dimensional parametric structure; e.g the receptive field might be well-approximated by\\na Gabor function [13], or by a difference of Gaussians [14], or by a low rank spatiotemporal matrix\\n[15, 13]. We can think of this structure as defining a sub-manifold, M, of the full model space, ?,\\nM = {?~ : ?~ = ?(~?), ?~?}.\\n\\n(2)\\n\\nThe vector, ~?, essentially enumerates the points on the manifold and ?() is a function which maps\\nthese points into ? space. A natural example is the case where we wish to enforce the constraint\\nthat ?~ has some parametric form, e.g. a Gabor function. The basic idea is that we want to run\\nexperiments which can identify exactly where on the manifold the optimal model lies.\\nSince M can have some arbitrary nonlinear shape, computing the informativeness of a stimulus\\nusing just the models on the manifold is not easy. Furthermore, if we completely restrict our attention\\nto models in M then we ignore the possibility that our prior knowledge is incorrect. Hence, we do\\nnot force the posterior distribution of ?~ to only have support on the manifold. Rather, we maintain a\\nGaussian approximation of the posterior on the full space, ?. However, when optimizing our stimuli\\nwe combine our posterior with our knowledge of M in order to do a better job of maximizing the\\ninformativeness of each experiment.\\n1\\nIt is worth noting that this simple GLM can be generalized in a number of directions; we may include\\nspike-history effects, nonlinear input terms, and so on [10].\\n\\n3\\n\\n\\f~ st+1 , s1:t , r1:t ) entails an integral over model space\\nComputing the mutual information I(rt+1 ; ?|~\\nweighted by the posterior probability on each model. We integrate over model space because the\\ninformativeness of an experiment clearly depends on what we already know (i.e. the likelihood we\\nassign to each model given the data and our prior knowledge). Furthermore, the informativeness of\\nan experiment will depend on the outcome. Hence, we use what we know about the neuron to make\\npredictions about the experimental outcome. Unfortunately, since the manifold in general has some\\narbitrary nonlinear shape we cannot easily compute integrals over the manifold. Furthermore, we\\ndo not want to continue to restrict ourselves to models on the manifold if the data indicates our prior\\nknowledge is wrong.\\nWe can solve both problems by making use of the tangent space of the manifold, as illustrated in\\nFigure 1 [16]. The tangent space is a linear space which provides a local approximation of the\\nmanifold. Since the tangent space is a linear subspace of ?, integrating over ?~ in the tangent space\\nis much easier than integrating over all ?~ on the manifold; in fact, the methods introduced in [8]\\nmay be applied directly to this case. The tangent space is a local linear approximation evaluated at\\na particular point, ?\\n~ M,t , on the manifold. For ?\\n~ M,t we use the projection of ?\\n~ t onto the manifold\\n(i.e., ?\\n~ M,t is the closest point in M to ?\\n~ t ). Depending on the manifold, computing ?\\n~ M,t can be\\nnontrivial; the examples considered in this paper, however, all have tractable numerical solutions to\\nthis problem.\\nThe challenge is representing the set of models close to ?\\n~ M,t in a way that makes integrating over the\\nmodels tractable. To find models on the manifold close to ?\\n~ M,t we want to perturb the parameters\\n~? about the values corresponding to ?\\n~ M,t . Since ? is in general nonlinear, there is no simple\\nexpression for the combination of all such perturbations. However, we can easily approximate the\\nset of ?~ resulting from these perturbations by taking linear combinations of the partial derivatives\\nof ? with respect to ~?. The partial derivative is the direction in ? in which ?~ moves if we perturb\\none of the manifold?s parameters. Thus, the subspace formed by linear combinations of the partial\\nderivatives approximates the set of models on the manifold close to ?\\n~ M,t . This subspace is the\\ntangent space,\\n\\nT?~ M,t M = {?~ : ?~ = ?\\n~ M,t + B~b, ?~b ? Rdim(M) }\\n\\n\\u0012\\u0014\\nB = orth\\n\\n??\\n??\\n...\\n??1\\n??d\\n\\n\\u0015\\u0013\\n,\\n\\n(3)\\n\\nwhere orth is an orthonormal basis for the column space of its argument. Here Tx M denotes the\\ntangent space at the point x. The columns of B denote the direction in which ?~ changes if we perturb\\none of the manifold?s parameters. (In general, the directions corresponding to changes in different\\nparameters are not independent; to avoid this redundancy we compute a set of basis vectors for the\\nspace spanned by the partial derivatives.)\\nWe now use our Gaussian posterior on the full parameter space to compute the posterior likelihood\\nof the models in the tangent space. Since the tangent space is a subspace of ?, restricting our\\n~ ?t , C t ), to the tangent space means we are taking a slice through our\\nGaussian approximation, p(?|~\\nGaussian approximation of the posterior. Mathematically, we are conditioning on ?~ ? T?~ M,t M.\\nThe result is a Gaussian distribution on the tangent space whose parameters may be obtained using\\nthe standard Gaussian conditioning formula:\\n\\u001a\\nN (~b; ?\\n~ b,t , Cb,t ) if ? ~b s.t ?~ = ?\\n~ M,t + B~b\\n~\\nptan (?|~\\n?b,t , Cb,t ) =\\n(4)\\n0\\nif\\n?~ ?\\n/ T?~ M,t\\n?\\n~ b,t = ?Cb,t B T C ?1\\n?M,t ? ?\\n~ t)\\nt (~\\n\\n?1\\nCb,t = (B T C ?1\\nt B)\\n\\n(5)\\n\\nwhere N denotes a normal distribution with the specified parameters. Now, rather than optimizing\\n~ 1:t , s1:t , M) on the nonlinear manifold M\\nthe stimulus by trying to squeeze the uncertainty p(?|r\\ndown as much as possible (a very difficult task in general), we pick the stimulus which best reduces\\n~ ?b,t , Cb,t ) on the vector space T?~ . We can solve this latter problem dithe uncertainty ptan (?|~\\nM,t\\nrectly using the methods presented in [8]. Finally, to handle the possibility that ?~ ?\\n/ M, every so\\n~ ?t , C t ). This simple modification enoften we optimize the stimulus using the full posterior p(?|~\\nsures that asymptotically we do not ignore directions orthogonal to the manifold; i.e., that we do not\\n4\\n\\n\\ft=500\\n\\nt=750\\n\\nt=1000\\n\\n?\\n\\ninfo. max.\\nfull\\nFrequency(KHz)\\n\\ni.i.d.\\n\\ninfo. max.\\ntan. space\\n\\nt=250\\n\\n4\\n2\\n0\\n?2\\n\\n6\\n4\\n2\\n?20 ?10\\n0\\nTime(ms)\\n\\nFigure 2: MAP estimates of a STRF obtained using three designs: the new info. max. tangent\\nspace design described in the text; an i.i.d. design; and an info. max. design which did not use\\nthe assumption that ?~ corresponds to a low rank STRF. In each case, stimuli were chosen under\\nthe spherical power contraint, ||~st ||2 = c. The true STRF (fit to real zebrafinch auditory responses\\nand then used to simulate the observed data) is shown in the last column. (For convenience we\\nrescaled the coefficients to be between -4 and 4). We see that using the tangent space to optimize the\\ndesign leads to much faster convergence to the true parameters; in addition, either infomax design\\nsignificantly outperforms the iid design here. In this case the true STRF did not in fact lie on the\\nmanifold M (chosen to be the set of rank-2 matrices here); thus, these results also show that our\\nknowledge of M does not need to be exact in order to improve the experimental design.\\nget stuck obsessively sampling along the incorrect manifold. As a result, ?t will always converge\\nasymptotically to the true parameters, even when ? 6? M .\\nTo summarize, our method proceeds as follows:\\n0. Initial conditions: start with a log-concave (approximately Gaussian) posterior given t previous trials, summarized by the posterior mean, ?\\n~ t and covariance, C t .\\n1. Compute ?\\n~ M,t , the projection of ?\\n~ t on the manifold. (The procedure for computing ?\\n~ M,t\\ndepends on the manifold.)\\n2. Compute the tangent space of M at ?\\n~ M,t using Eqn. 3.\\n~ ?b,t , Cb,t ), using the standard\\n3. Compute the posterior restricted to the tangent space, ptan (?|~\\nGaussian conditioning formula (Eqn. 5).\\n4. Apply the methods in [8] to find the optimal t + 1 stimulus, and observe the response rt+1 .\\n5. Update the posterior by recursively updating the posterior mean and covariance: ?\\n~t ?\\n?\\n~ t+1 and C t ? C t+1 (again, as in [8]), and return to step 1.\\n\\n3\\n3.1\\n\\nResults\\nLow rank models\\n\\nTo test our methods in a realistic, high-dimensional setting, we simulated a typical auditory neurophysiology [17, 15, 18] experiment. Here, the objective is to to identify the spectro-temporal\\nreceptive field (STRF) of the neuron. The input and receptive field of the neuron are usually represented in the frequency domain because the cochlea is known to perform a frequency decomposition\\nof sound. The STRF, ?(?, ?), is a 2-d filter which relates the firing rate at time t to the amount of\\n5\\n\\n\\fenergy at frequency ? and time t ? ? in the stimulus. To incorporate this spectrotemporal model in\\nthe standard GLM setting, we simply vectorize the matrix ?(?, ?).\\nEstimating the STRF can be quite difficult due to its high dimensionality. Several researchers,\\nhowever, have shown that low-rank assumptions can be used to produce accurate approximations of\\nthe receptive field while significantly reducing the number of unknown parameters [19, 13, 15, 20].\\nA low rank assumption is a more general version of the space-time separable assumption that is often\\nused when studying visual receptive fields [21]. Mathematically, a low-rank assumption means that\\nthe matrix corresponding to the STRF can be written as a sum of rank one matrices,\\n? = M at ?~ = U V T\\n\\n(6)\\n\\nwhere M at indicates the matrix formed by reshaping the vector ?~ to form the STRF. U and V are\\nlow-rank matrices with orthonormal columns. The columns of U and V are the principal components\\nof the column and row spaces of ? respectively, and encode the spectral and temporal properties of\\nthe STRF, respectively.\\nWe simulated an auditory experiment using an STRF fitted to the actual response of a neuron in the\\nMesencephalicus lateralis pars dorsalis (MLd) of an adult male zebra finch [18]. To reduce the dimensionality we sub-sampled the STRF in the frequency domain and shortened it in the time domain\\nto yield a 20 ? 21 STRF. We generated synthetic data by sampling a Poisson process whose instantaneous firing rate was set to the output of a GLM with exponential nonlinearity and ?~ proportional\\nto the true measured zebra finch STRF.\\nFor the manifold we used the set of ?~ corresponding to rank-2 matrices. For the STRF we used,\\nthe rank-2 assumption turns out to be rather accurate. We also considered manifolds of rank-1 and\\nrank-5 matrices (data not shown), but rank-2 did slightly better. The manifold of rank r matrices\\nis convenient because we can easily project any ?~ onto M by reshaping ?~ as a matrix and then\\ncomputing its singular-value-decomposition (SVD). ?\\n~ M,t is the matrix formed by the first r singular\\nvectors of ?\\n~ t . To compute the tangent space, Eqn. 3, we compute the derivative of ?~ with respect to\\neach component of the matrices U and V . Using these derivatives we can linearly approximate the\\neffect on ? of perturbing the parameters of its principal components.\\nIn Figure 3.1 we compare the effectiveness of different experimental designs by plotting the MAP\\nestimate ?\\n~ t on several trials. The results clearly show that using the tangent space to design the\\nexperiments leads to much faster convergence to the true parameters. Furthermore, using the assumption that the STRF is rank-2 is beneficial even though the true STRF here is not in fact rank-2.\\n3.2\\n\\nReal birdsong data\\n\\nWe also tested our method by using it to reshuffle the data collected during an actual experiment\\nto find an ordering which provided a faster decrease in the error of the fitted model. During the\\nexperiments, we recorded the responses of MLd neurons when the songs of other birds and ripple\\nnoise were presented to the bird (again, as previously described in [18]). We compared a design\\nwhich randomly shuffled the trials to a design which used our info. max. algorithm to select the\\norder in which the trials are processed. We then evaluated the fitted model by computing the expected\\nP\\n~ ? denotes all the observations made\\nlog-likelihood of the spike trains, ? E?|~\\ns? , ?).\\n~ ?t ,C t log p(r? |~\\nwhen inputs in a test set are played to the bird.\\nTo constrain the models we assume the STRF is low-rank and that its principal components are\\nsmooth. The smoothing prior means that if we take the Fourier transform of the principal components, the Fourier coefficients of high frequencies should be zero with high probability. In other\\nwords, each principal component (the columns of U and V ) should be a linear combination of sinusoidal functions with low frequencies. In this case we can write the STRF as\\n? = F ??? T T T .\\n\\n(7)\\n\\nEach column of F and T is a sine or cosine function representing one of the basis functions of\\nthe principal spectral (columns of F ) or temporal (columns of T ) components of the STRF. Each\\ncolumn of ? and ? determines how we form one of the principal components by combining sine and\\ncosine functions. ? is a diagonal matrix which specifies the projection of ? onto each principal\\n6\\n\\n\\fE?log p(r|st,?t)\\n\\n0\\n\\n?0.5\\n\\nshuffled:\\nInfo. Max. full:\\nInfo. Max. Tan: rank=2\\n\\n?1 3\\n10\\n\\n4\\n\\ntrial\\n\\n10\\n\\nFigure 3: Plots comparing the performance of an info. max. design, an info. max. design which\\nuses the tangent space, and a shuffled design. The manifold was the set of rank 2 matrices. The plot\\nshows the expected log-likelihood (prediction accuracy) of the spike trains in response to a birdsong\\nin the test set. Using a rank 2 manifold to constrain the model produces slightly better fits of the\\ndata.\\n\\ncomponent. The unknown parameters in this case are the matrices ?, ?, and ?. The sinusoidal\\nfunctions corresponding to the columns of F and T should have frequencies {0, . . . , fo,f mf } and\\n{0, . . . , fo,t mt } respectively. fo,f and fo,t are the fundamental frequencies and are set so that 1\\nperiod corresponds to the dimensions of the STRF. mf and mt are the largest integers such that\\nfo,f mf and fo,t mt are less than the Nyquist frequency. Now to enforce a smoothing prior we can\\nsimply restrict the columns of F and T to sinusoids with low frequencies. To project ? onto the\\nmanifold we simply need to compute ?, ? and ? by evaluating the SVD of F T ?T .\\nThe results, Figure 3, show that both info. max. designs significantly outperform the randomly\\nshuffled design. Furthermore, incorporating the low-rank assumption using the tangent space improves the info. max. design, albeit only slightly; the estimated STRF?s are shown in Figure 4.\\nIt is worth noting that in an actual online experiment, we would expect a larger improvement with\\nthe info. max. design, since during the experiment we would be free to pick any input. Thus, the\\ndifferent designs could choose radically different stimulus sets; in contrast, when re-analyzing the\\ndata offline, all we can do is reshuffle the trials, but the stimulus sets remain the same in the info.\\nmax. and iid settings here.\\n\\n4\\n\\nConclusion\\n\\nWe have provided a method for incorporating detailed prior information in existing algorithms for\\nthe information-theoretic optimal design of neurophysiology experiments. These methods use realistic assumptions about the neuron?s response function and choose significantly more informative\\nstimuli, leading to faster convergence to the true response function using fewer experimental trials.\\nWe expect that the inclusion of this strong prior information will help experimentalists contend with\\nthe high dimensionality of neural response functions.\\n\\n5\\n\\nAcknowledgments\\n\\nWe thank Vincent Vu and Bin Yu for helpful conversations. JL is supported by the Computational Science Graduate Fellowship Program administered by the DOE under contract DE-FG0297ER25308 and by the NSF IGERT Program in Hybrid Neural Microsystems at Georgia Tech via\\ngrant number DGE-0333411. LP is supported by an NSF CAREER award and a Gatsby Initiative\\nin Brain Circuitry Pilot Grant.\\n7\\n\\n\\fTrial 2500 Trial 5000 Trial 7500\\n\\nshuffled\\n\\nTrial 1000\\n\\nTrial 10k\\n\\nTrial 20k\\n\\nTrial 50k x 10?3\\n2\\n0\\n\\nInfo. Max. Tan Info. Max.\\nfull\\nrank=2\\nFrequency (KHz)\\n\\n?2\\n\\n6\\n4\\n2\\n?40 ?20 0\\nTime(ms)\\n\\nFigure 4: The STRFs estimated using the bird song data. We plot ?\\n~ t for trials in the interval over\\nwhich the expected log-likelihood of the different designs differed the most in Fig. 3. The info. max.\\ndesigns converge slightly faster than the shuffled design. In these results, we smoothed the STRF by\\nonly using frequencies less than or equal to 10fo,f and 2fo,t .\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n\\n[11]\\n[12]\\n[13]\\n[14]\\n[15]\\n[16]\\n[17]\\n[18]\\n[19]\\n[20]\\n[21]\\n\\nP. Foldiak, Neurocomputing 38?40, 1217 (2001).\\nR. C. deCharms, et al., Science 280, 1439 (1998).\\nT. Gollisch, et al., Journal of Neuroscience 22, 10434 (2002).\\nF. Edin, et al., Journal of Computational Neuroscience 17, 47 (2004).\\nC. Machens, et al., Neuron 47, 447 (2005).\\nK. N. O?Connor, et al., Journal of Neurophysiology 94, 4051 (2005).\\nD. L. Ringach, J Neurophysiol 88, 455 (2002).\\nJ. Lewi, et al., Neural Computation 21 (2009).\\nE. Simoncelli, et al., The Cognitive Neurosciences, M. Gazzaniga, ed. (MIT Press, 2004).\\nL. Paninski, et al., Computational Neuroscience: Theoretical Insights into Brain Function\\n(Elsevier, 2007), chap. Statistical models for neural encoding, decoding, and optimal stimulus\\ndesign.\\nL. Paninski, Network: Computation in Neural Systems 15, 243 (2004).\\nL. Paninski, Neural Computation 17, 1480 (2005).\\nA. Qiu, et al., J Neurophysiol 90, 456 (2003).\\nC. Enroth-Cugell, et al., Journal of Physiology 187, 517 (1966).\\nJ. F. Linden, et al., Journal of Neurophysiology 90, 2660 (2003).\\nJ. M. Lee, Introduction to Smooth Manifolds (Springer, 2000).\\nF. E. Theunissen, et al., Journal of Neuroscience 20, 2315 (2000).\\nS. M. Woolley, et al., The Journal of Neuroscience 26, 2499 (2006).\\nD. A. Depireux, et al., Journal of Neurophysiology 85, 1220 (2001).\\nM. B. Ahrens, et al., Network 19, 35 (2008).\\nG. C. DeAngelis, et al., J Neurophysiol 69, 1091 (1993).\\n\\n8\\n\\n\\f\",\n          \"505\\n\\nCONNECTING TO THE PAST\\nBruce A. MacDonald, Assistant Professor\\nKnowledge Sciences Laboratory, Computer Science Department\\nThe University of Calgary, 2500 University Drive NW\\nCalgary, Alberta T2N IN4\\nABSTRACT\\nRecently there has been renewed interest in neural-like processing systems, evidenced for example in the two volumes Parallel Distributed Processing edited by Rumelhart and McClelland,\\nand discussed as parallel distributed systems, connectionist models, neural nets, value passing\\nsystems and multiple context systems. Dissatisfaction with symbolic manipulation paradigms\\nfor artificial intelligence seems partly responsible for this attention, encouraged by the promise\\nof massively parallel systems implemented in hardware. This paper relates simple neural-like\\nsystems based on multiple context to some other well-known formalisms-namely production\\nsystems, k-Iength sequence prediction, finite-state machines and Turing machines-and presents\\nearlier sequence prediction results in a new light.\\n\\n1\\n\\nINTRODUCTION\\n\\nThe revival of neural net research has been very strong, exemplified recently by Rumelhart\\nand McClelland!, new journals and a number of meetings G ? The nets are also described as\\nparallel distributed systems!, connectionist models 2 , value passing systems3 and multiple context\\nlearning systems4 ,5,6,7,8,9. The symbolic manipulation paradigm for artificial intelligence does\\nnot seem to have been as successful as some hoped!, and there seems at last to be real promise\\nof massively parallel systems implemented in hardware. However, in the flurry of new work it\\nis important to consolidate new ideas and place them solidly alongside established ones. This\\npaper relates simple neural-like systems to some other well-known notions-namely production\\nsystems, k-Iength sequence prediction, finite-state machines and Turing machines-and presents\\nearlier results on the abilities of such networks in a new light.\\nThe general form of a connectionist system lO is simplified to a three layer net with binary\\nfixed weights in the hidden layer, thereby avoiding many of the difficulties-and challengesof the recent work on neural nets, The hidden unit weights are regularly patterned using a\\ntemplate. Sophisticated, expensive learning algorithms are avoided, and a simple method is\\nused for determining output unit weights. In this way we gain some of the advantages of multilayered nets, while retaining some of the simplicity of two layer net training methods. Certainly\\nnothing is lost in computational power-as I will explain-and the limitations of two layer\\nnets are not carried over to the simplified three layer one. Biological systems may similarly\\navoid the need for learning algorithms such as the \\\"simulated annealing\\\" method commonly\\nused in connectionist models l l . For one thing, biological systems do not have the same clearly\\ndistinguished training phase.\\nBriefly, the simplified net b is a production system implemented as three layers of neuron-like\\nunits; an output layer, an input layer, and a hidden layer for the productions themselves. Each\\nhidden production unit potentially connects a predetermined set of inputs to any output. A\\nk-Iength sequence predictor is formed once Ie levels of delay unit are introduced into the input\\nlayer. k-Iength predictors are unable to distinguish simple sequences such as ba . .. a and aa ... a\\nsince after Ie or more characters the system has forgotten whether an a or b appeared first. If\\nthe k-Iength predictor is augmented with \\\"auxiliary\\\" actions, it is able to learn this and other\\nregular languages, since the auxiliary actions can be equivalent to states, and can be inputs to\\naAmong them the 1st International Conference on Neural Nets, San Diego,CA, June 21-24, 1987, and this\\ncon.ference.\\nbRoughly equivalent to a single context system in Andreae's multiple context system 4. 5,6,7,8,9. See also\\nMacDonald 12 .\\n\\n@)\\n\\nAmerican Institute of Physics 1988\\n\\n\\f506\\n\\nFigure 1: The general form of a connectionist system 10 .\\n(a) Form of a unit\\n\\n(a) Operations within a unit\\n\\nin~uts ;::; L'\\\" excitation-.I\\nweIghts\\nsum\\n\\n1:.. aCtiVation--W'\\\" output\\n\\n?--==\\nTypical F\\n\\nTypical f\\n\\nthe production units enabling predictions to depend on previous states 7 . By combining several\\naugmented sequence predictors a Thring machine tape can be simulated along with a finite-state\\ncontroller 9 , giving the net the computational power of a Universal Turing machine. Relatively\\nsimple neural-like systems do not lack computational ability. Previous implementations 7,9 of\\nthis ability are production system equivalents to the simplified nets.\\n1.1\\n\\nOrganization of the paper\\n\\nThe next section briefly reviews the general form of connectionist systems. Section 2 simplifies\\nthis, then section 3 explains that the result is equivalent to a production system dealing only\\nwith inputs and outputs of the net. Section 4 extends the simplified version, enabling it to learn\\nto predict sequences. Section 5 explains how the computational power of the sequence predictor\\ncan be increased to that of a Thring machine if some input units receive auxiliary actions; in fact\\nthe system can learn to be a TUring machine. Section 6 discusses the possibility of a number of\\nnets combining their outputs, forming an overall net with \\\"association areas\\\".\\n1.2\\n\\nGeneral form of a connectionist system\\n\\nFigure 1 shows the general form of a connectionist system unit, neuron or ce1l 10 . In the figure\\nunit i has inputs, which are the outputs OJ of possibly all units in the network, and an output of\\nits own, 0i' The net input excitation, net\\\" is the weighted sum of inputs, where !Vij is the weight\\nconnecting the output from unit j as an input to unit i. The activation, ai of the unit is some\\nfunction Fi of the net input excitation. Typically Fi is semilinear, that is non-decreasing and\\ndifferentiable 13 , and is the same function for all, or at least large groups of units. The output is\\na function fi of the activation; typically some kind of threshold function. I will assume that the\\nquantities vary over discrete time steps, so for example the activation at time t + 1 is ai (t + 1)\\nand is given by Fi((neti(t)).\\nIn general there is no restriction on the connections that may be made between units.\\nUnits not connected directly to inputs or outputs are hidden units. In more complex nets\\nthan those described in this paper, there may be more than one type of connection. Figure 2\\nshows a common connection topology, where there are three layers of units-input, hidden and\\noutput-with no cycles of connection.\\nThe net is trained by presenting it with input combinations, each along with the desired\\noutput combination. Once trained the system should produce the desired outputs given just\\n\\n\\f507\\n\\nFigure 2: The basic structure of a three layer connectionist system.\\n\\ninput units\\n\\nhidden\\nunits\\n\\noutput units\\n\\ninputs . During training the weights are adjusted in some fashion that reduces the discrepancy\\nbetween desired and actual output. The general method is lO :\\n(1)\\n\\nwhere t; is the desired, \\\"training\\\" activation . Equation 1 is a general form of Hebb's classic\\nrule for adjusting the weight between two units with high activations lO ? The weight adjustment\\nis the product of two functions, one that depends on the desired and actual activations--often\\njust the difference-and another that depends on the input to that weight and the weight itself.\\nAs a simple example suppose 9 is the difference and h as just the output OJ. Then the weight\\nchange is the product of the output error and the input excitation to that weight:\\n\\nwhere the constant T} determines the learning rate. This is the Widrow-Hoff or Delta rule which\\nmay be used in nets without hidden units. 1o\\nThe important contribution of recent work on connectionist systems is how to implement\\nequation 1 in hidden units; for which there are no training signals ti directly available . The\\nBoltzmann learning method iteratively varies both weights and hidden unit training activations\\nusing the controlled, gradually decreasing randomizing method \\\"simulated annealing\\\" 14. Backpropagation 13 is also iterative, performing gradient descent by propagating training signal errors\\nback through the net to hidden units. I will avoid the need to determine training signals for\\nhidden units, by fixing the weights of hidden units in section 2 below.\\n\\n2\\n\\nSIMPLIFIED SYSTEM\\n\\nAssume these simplifications are made to the general connectionist system of section 1.2:\\n1. The system has three layers, with the topology shown in Figure 2 (ie no cycles)\\n2. All hidden layer unit weights are fixed, say at unity or zero\\n3. Each unit is a linear threshold unit lO , which means the activation function for all units\\nis the identity function, giving just net;, a weighted sum of the inputs, and the output\\nfunction is a simple binary threshold of the form:\\n\\n!- I\\n\\noutput\\n\\nthreshold /\\n\\n?\\nactivation\\n\\n\\f508\\n\\nso that the output is binary; on or oft'. Hidden units will have thresholds requiring all\\ninputs to be active for the output to be active (like an AND gate) while output units will\\nhave thresholds requiring only 1 or two active highly weighted inputs for an output to be\\ngenerated (like an OR gate). This is in keeping with the production system view of the\\nnet, explained in section 3.\\n4. Learning-which now occurs only at the output unit weights-gives weight adjustments\\naccording to:\\nWij\\nWij\\n\\n1\\n\\nif ai =\\n\\nOJ\\n\\n=1\\n\\n0 otherwise\\n\\nso that weights are turned on if their input and the unit output are on, and off otherwise.\\nThat is, Wij = ai A OJ. A simple example is given in Figure 3 in section 3 below.\\nThis simple form of net can be made probabilistic by replacing 4 with 4' below:\\n4'. Adjust weights so that Wij estimates the conditional probability of the unit i output being\\non when output j is on. That is,\\nWij\\n\\n= estimate of P(odoj).\\n\\nThen, assuming independence of the inputs to a unit, an output unit is turned on when the\\nconditional probability of occurrence of that output exceeds the threshold of the output\\nfunction.\\nOnce these simplifications are made, there is no need for learning in the hidden units. Also no\\niterative learning is required; weights are either assigned binary values, or estimate conditional\\nprobabilities. This paper presents some of the characteristics of the simplified net. Section 6\\ndiscusses the motivation for simplifying neural nets in this way.\\n\\n3\\n\\nPRODUCTION SYSTEMS\\n\\nThe simplified net is a kind of simple production system. A production system comprises a\\nglobal database, a set of production rules and a control system 15 . The database for the net is\\nthe system it interacts with, providing inputs as reactions to outputs from t.he net. The hidden\\nunits of the network are the production rules, which have the form\\nIF\\n\\nprecondition\\n\\nTHEN\\n\\naction\\n\\nThe precondition is satisfied when the input excitation exceeds the threshold of a hidden unit.\\nThe actions are represented by the output units which the hidden production units activate.\\nThe control system of a production system chooses the rule whose action to perform, from the\\nset of rules whose preconditions have been met. In a neural net the control system is distributed\\nthroughout the net in the output units. For example, the output units might form a winner-takeall net. In production systems more complex control involves forward and backward chaining to\\nchoose actions that seek goals . This is discussed elsewhere4.12.16. Figure 3 illust.rates a simple\\nproduction implemented as a neural net. As the figure shows, the inputs to hidden units are\\njust the elements of the precondition. When the appropriate input combination is present the\\nassociated hidden (production) unit is fired. Once weights have been leamed connecting hidden\\nunits to output units, firing a production results in output. The simplified neural net is directly\\nequivalent to a production system whose elements are inputs and outputs e .\\nSome production systems have symbolic elements, such as variables, which can be given\\nvalues by production actions. The neural net cannot directly implement this, since it can\\nhave outputs only from a predetermined set. However, we will see later that extensions t.o the\\nframework enable this and other abilities.\\nCThis might be referred to as a \\\"sensory-motor\\\" production system, since when implemented ill a l'eal system\\nsuch as a robot, it deals only with sensed inputs and executable motor actions, which may include the auxiliary\\nactions of section 4.3.\\n\\n\\f509\\n\\nFigure 3: A production implemented in a simplified neural net .\\n(a) A production rule\\nrr==~--r=======~~~==~\\nIF\\n\\nIcloudy I Ipressure falling I\\nAND\\n\\nTHEN\\n\\nIit will rain I\\n\\n(b) The rule implemented as a hidden unit. The threshold of the hidden unit is 2 so it is.\\nan AND gate. The threshold of the output unit is 1 so it is an OR gate. The learned\\nweight will be 0 or 1 if the net is not probabilistic, otherwise it will be an estimate of\\nP(it will rainlclouds AND pressure falling)\\n\\nIt will\\nrain\\n\\nweight\\n\\nFigure 4: A net that predicts the next character in a sequence, based on only the last character .\\n(a) The net . Production units (hidden units) have been combined with input units.\\nFor example this net could predict the sequence abcabcabc . . .. Productions have the\\nform : IF last character is . .. THEN next character will be . . .. The learning rule is\\nWij\\n1 if (inputj AND outputi). Output is ai\\n~R WijOj\\n\\n=\\n\\n=\\n\\ninput\\n\\nneural net\\n\\noutput\\n\\na\\n\\na\\nb\\nc\\n\\nb\\n\\nc\\n\\n(b) Learning procedure.\\n1. Clamp inputs and outputs to desired values\\n2. System calculates weight values\\n3. Repeat 4 and 4 for all required input/output combinations\\n\\n4\\n\\nSEQUENCE PREDICTION\\n\\nA production system or neural net can predict sequences. Given examples of a repeating sequence, productions are learned which predict future events on the basis of recent ones . Figure 4\\nshows a trivially simple sequence predictor. It predicts the next character of a sequence based\\non the previous one. The figure also gives the details of the learning procedure for the simplified\\nnet. The net need be trained only once on each input combination, then it will \\\"predict\\\" as\\nan output every character seen after the current one. The probabilistic form of the net would\\nestimate conditional probabilities for the next character , conditional on the current one. Many\\n\\n\\f510\\n\\nFigure 5: Using delayed inputs, a neural net can implement a k-length sequence predictor.\\n(a) A net with the last three characters as input.\\ninput\\n\\nhidden\\n\\noutput\\n\\na':\\\"\\\"'\\\"-......:;;;;;;;::::~;:-\\n\\na\\n\\n{ a'\\n\\na\\n\\nb':-;-'----..\\\",\\n\\n{:';;g 'J.,o\\n\\nb\\n\\ne\\\"\\n\\n{e'_~\\n0-\\\"\\ne -_ _ _\\n\\nc\\n\\nz\\n\\n2nd last\\n(b) An example production.\\n~----------------------------------,\\n\\nIF\\n\\nlast three characters were ~ THEN\\n\\n0\\n\\npresentations of each possible character pair would be needed to properly estimate the probabilities. The net would be learning the probability distribution of character pairs. A predictor like\\nthe one in Figure 4 can be extended to a general k-Iength 17 predictor so long as inputs delayed\\nby 1,2, ... , k steps are available. Then, as illustrated in Figure 5 for 3-length prediction, hidden\\nproduction units represent all possible combinations of k symbols. Again output weights are\\ntrained to respond to previously seen input combinations, here of three characters. These delays\\ncan be provided by dedicated neural nets d , such as that shown in Figure 6. Note that the net\\nis assumed to be synchronously updated, so that the input from feedback around units is not\\nchanged until one step after the output changes. There are various ways of implementing delay\\nin neurons, and Andreae 4 investigates some of them for the same purpose-delaying inputs-in\\na more detailed simulation of a similar net.\\n4.1\\n\\nOther work on sequence prediction in neural nets\\n\\nFeldman and Ballard 2 find connectionist systems initially not suited to representing changes\\nwith time. One form of change is sequence, and they suggest two methods for representing\\nsequence in nets. The first is by units connected to each other in sequence so that sequential\\ntasks are represented by firing these units in succession. The second method is to buffer the\\ninputs in time so that inputs from the recent past are available as well as current inputs; that\\nis, delayed inputs are available as suggested above. An important difference is the necessary\\nlength of the buffer; Feldman and Ballard suggest the buffer be long enough to hold a phrase of\\nnatural language, but I expect to use buffers no longer than about 7, after Andreae 4 . Symbolic\\ninputs can represent more complex information effectively giving the length seven buffers more\\ninformation than the most recent seven simple inputs, as discussed in section 5.\\nThe method of back-propagation13 enables recurrent networks to learn sequential tasks in a\\ndFeldman and Ballard2 give some dedicated neural net connections for a variety of flUlctions\\n\\n\\f511\\n\\nFigure 6: Inputs can be delayed by dedicated neural subnets. A two stage delay is shown.\\n(a) Delay network.\\n\\n(b) Timing diagram for (a).\\n\\n--.r-\\n\\n1.0\\n\\nIL..-_-_-_-_-_-.:-_-.:-_-.:-_-_-_-_..._ tml\\n_?_e_\\n\\nB -r-0.5\\n\\n0.75 ~ 0.375 ....,L-_ _ __\\n\\nA\\n\\nC\\nD\\n\\n__________\\n\\nE\\n\\noriginal signal\\ndelay of one step\\n\\n~r--------r--------~------~L\\n\\ndelay of two steps\\n\\nmanner similar to the first suggestion in the last paragraph, where sequences of connected units\\nrepresent sequenced events. In one example a net learns to complete a sequence of characters;\\nwhen given the first two characters of a six character sequence the next four are output. Errors\\nmust be propagated around cycles in a recurrent net a number of times.\\nSeriality may also be achieved by a sequence of states of distributed activation 18. An example\\nis a net playing both sides of a tic-tac-toe game 18 . The sequential nature of the net's behavior is\\nderived from the sequential nature of the responses to the net's actions; tic-tac-toe moves. A net\\ncan model sequence internally by modeling a sequential part of its environment. For example,\\na tic-tac-toe playing net can have a model of its opponent.\\nk-Iength sequence predictors are unable to learn sequences which do not repeat more frequently that every k characters. Their k-Iength context includes only information about the last\\nk events. However, there are two ways in which information from before the kth last input can\\nbe retained in the net. The first method latches some inputs, while the second involves auxiliary\\nactions.\\n4.2\\n\\nLatch units\\n\\nInputs can be latched and held indefinitely using the combination shown in Figure 7. Not all\\ninputs would normally be latched. Andreae 4 discusses this technique of \\\"threading\\\" latched\\nevents among non-latched events, giving the net both information arbitrarily far back in its\\ninput-output history and information from the immediate past . Briefly, the sequence ba . .. a\\ncan be distinguished from aa ... a if the first character is latched . However, this is an ad hoc\\nsolution to this problem e .\\n4-3\\n\\nAuxiliary actions\\n\\nWhen an output is fed back into the net as an input signal, this enables the system to choose the\\nnext output at least partly based on the previous one, as indicated in Figure 8. If a particular\\nfed back output is also one without external manifestation, or whose external manifestation\\nis independent of the task being performed, then that output is an auxiliary action. It Las\\n\\\"The interested reader should refer to Andreae 4 where more extensive analysis is given.\\n\\n\\f512\\n\\nFigure 7: Threading. A latch circuit remembers an event until another comes along. This is a\\ntwo input latch, e.g. for two letters a and b, but any number of units may be similarly connected.\\nIt is formed from a mutual inhibition layer, or winner-take-all connection, along with positive\\nfeedback to keep the selected output activated when the input disappears.\\n\\na\\nb---;~..!!J\\n\\nFigure 8: Auxiliary actions-the S outputs-are fed back to the inputs of a net, enabling the\\nnet to remember a state. Here both part of a net and an example of a production are shown.\\nThere are two types of action, characters and S actions.\\n\\nSinputs\\n\\nS outputs\\n\\ncharacter inputs\\nIF\\n\\nS input is\\n\\n[?l] and character input is 0\\n\\ncharacter outputs\\n\\nTHEN\\n\\noutput character\\n\\nlliJ and S [ill\\n\\nno direct effect on the task the system is performing since it evokes no relevant inputs, and\\nso can be used by the net as a symbolic action. If an auxiliary action is latched at the input\\nthen the symbolic information can be remembered indefinitely, being lost only when another\\nauxiliary action of that kind is input and takes over the latch. Thus auxiliary actions can act\\nlike remembered states; the system performs an action to \\\"remind\\\" itself to be in a particular\\nstate. The figure illustrates this for a system that predicts characters and state changes given\\nthe previous character and state. An obvious candidate for auxiliary actions is speech. So\\nthe blank oval in the figure would represent the net's environment, through which its own\\nspeech actions are heard. Although it is externally manifested, speech has no direct effect on\\nour physical interactions with the world. Its symbolic ability not only provides the power of\\nauxiliary actions, but also includes other speakers in the interaction.\\n\\n5\\n\\nSIMULATING ABSTRACT AUTOMATA\\n\\nThe example in Figure 8 gives the essence of simulating a finite state automaton with a production system or its neural net equivalent . It illustrates the transition function of an automaton;\\nthe new state and output are a function of the previous state and input. Thus a neural net can\\nsimulate a finite state automaton, so long as it has additional, auxiliary actions.\\nA Thring machine is a finite state automaton controller plus an unbounded memory. A\\nneural net could simulate a 'lUring machine in two ways, and both ways have been demonstrated\\nwith production system implementations-equivalent to neural nets----(;alled \\\"multiple context\\nlearning systems\\\"', briefly explained in section 6. The first Thring machine simulation 7 has the\\nsystem simulate only the finite state controller, but is able to use an unbounded external memory\\nfSee John Andreae's and his colleagues' work4 ,5,6,7,8,9,12 ,16\\n\\n\\f513\\n\\nFigure 9: Multiple context learning system implementation as multiple neural nets. Each:3\\nlayer net has the simplified form presented above, with a number of elaborations such as extra\\nconnections for goal-seeking by forward and backward chaining .\\n\\nOutput\\nchannels\\n\\nfrom the real world, much like the paper of Turing's original work 19 . The second simnlat.ion[\\\" 1'2\\nembeds the memory in the multiple context learning system, along with a counter for accessing\\nthis simulated memory. Both learn all the productions-equivalent to learning output unit\\nweights-required for the simulations. The second is able to add internal memory as required,\\nup to a limit dependent on the size of the network (which can easily be large enough to allow 70\\nyears of computation!). The second could also employ external memory as the first did. Briefly,\\nthe second simulation comprised multiple sequence predictors which predicted auxiliary actions\\nfor remembering the state of the controller, and the current memory position . The memory\\nelement is updated by relearning the production representing that element; the precondition is\\nthe address and the production action the stored item.\\n\\n6\\n\\nMULTIPLE SYSTEMS FORM ASSOCIATION AREAS\\n\\nA multiple context learning system is production system version of a multiple neural net, although a simple version has been implemented as a simulated net 4 ?20 . It effectively comprises\\nseveral nets--or \\\"association\\\" areas-which may have outputs and inputs in common, as indicated in Figure 9. Hidden unit weights are specified by templates ; one for each net . A template\\ngives the inputs to have a zero weight for the hidden units of a net and the inputs to have a\\nweight of unity. Delayed and latched inputs are also available . The actual outputs are selected\\nfrom the combined predictions of the nets in a winner-take-all fashion .\\nI see the design for real neural nets, say as controllers for real robots, requiring a large\\ndegree of predetermined connectivity. A robot controller could not be one three layer net wit.h\\nevery input connected to every hidden unit in turn connected to every output. There will\\nneed to be some connectivity constraints so the net reflects the functional specialization in the\\ncontrol requirements 9 . The multiple context learning system has all the hidden layer connections\\npredetermined, but allows output connections to be learned. This avoids the \\\"credit assignment\\\"\\nproblem and therefore also the need for learning algorithms such as Boltzmann learning and\\nback-propagation. However, as the multiple context learning system has auxiliary actions, and\\ndelayed and latched inputs, it does not lack computational power. Future work in this area\\nshould investigate , for example, the ability of different kinds of nets to learn auxiliary act.ions.\\nThis may be difficult as symbolic actions may not be provided in training inputs and output.s .\\n9 For\\n\\nexample a controller for a robot body would have to deal with vision, manipulation, motion, etc.\\n\\n\\f514\\n\\n7\\n\\nCONCLUSION\\n\\nThis paper has presented a sImplified three layer connectionist model, with fixed weights for\\nhidden units, delays and latches for inputs, sequence prediction ability, auxiliary \\\"state\\\" actions,\\nand the ability to use internal and external memory. The result is able to learn to simulate a\\nTuring machine. Simple neural-like systems do not lack computational power.\\nACKNOWLEDGEMENTS\\n\\nThis work is supported by the Natural Sciences and Engineering Council of Canada.\\nREFERENCES\\n1. Rumelhart,D.E. and McClelland,J .L . Parallel distributed processing. Volumes 1 and 2. MIT\\n\\n2.\\n3.\\n4.\\n5.\\n\\n6.\\n7.\\n8.\\n9.\\n10.\\n11.\\n12.\\n13.\\n14.\\n15.\\n16.\\n\\n17.\\n18.\\n\\n19.\\n20.\\n\\nPress. (1986)\\nFeldman,J .A. and Ballard,D.H. Connectionist models and their properties. Cognitive Science\\n6, pp.205-254. (1982)\\nFahlman,S .E. Three Flavors of Parallelism. Proc.4th Nat.Conf. CSCSI/SCSEIO, Saskatoon.\\n(1982)\\nAndreae,J .H. Thinking with the teachable machine. Academic Press. (1977)\\nAndreae,J.H. Man-Machine Studies Progress Reports UC-DSE/1-28. Dept Electrical and\\nElectronic Engineering, Univ. Canterbury, Christchurch, New Zealand. editor. (1972-87)\\n(Also available from NTIS, 5285 Port Royal Rd, Springfield, VA 22161)\\nAndreae,J .H. and Andreae,P.M. Machine learning with a multiple context. Proc.9th\\nInt.Conf.on Cybernetics and Society. Denver. October. pp.734-9. (1979)\\nAndreae,J.H. and Cleary,J.G. A new mechanism for a brain. Int.J.Man-Machine Studies\\n8(1): pp.89-1l9. (1976)\\nAndreae,P.M. and Andreae,J.H. A teachable machine in the real world. Int.J.Man-Machine\\nStudies 10: pp.301-12. (1978)\\nMacDonald,B.A. and Andreae,J .H. The competence of a multiple context learning system.\\nInt.J.Gen.Systems 7: pp.123-37. (1981)\\nRumelhart,D.E., Hinton,G.E. and McClelland,J .L. A general framework for parallel distributed processing. chapter 2 in Rumelhart and McClelland l , pp.45-76. (1986)\\nHinton,G.E. and Sejnowski,T.L. Learning and relearning in Boltzmann machines . chapter 7\\nin Rumelhart and McClelland l , pp.282-317. (1986)\\nMacDonald,B.A. Designing teachable robots. PhD thesis, University of Canterbury,\\nChristchurch, New Zealand. (1984)\\nRumelhart,D.E., Hinton,G.E. and Williams,R.J. Learning Internal Representations by Error\\nPropagation. chapter 8 in Rumelhart and McClelland l , pp.318-362. (1986)\\nAckley,D.H., Hinton,G.E. and Sejnowski,T.J. A Learning Algorithm for Boltzmann Machines. Cognitive Science 9, pp.147-169. (1985)\\nNilsson,N.J. Principles of Artificial Intelligence. Tioga. (1980)\\nAndreae,J .H. and MacDonald,B~_A. Expert control for a robot body. Research Report\\n87/286/34 Dept. of Computer Science, University of Calgary, Alberta, Canada, T2N-1N4.\\n(1987)\\nWitten,I.H. Approximate, non-deterministic modelling of behaviour sequences. Int. 1. General Systems, vol. 5 pp.1-12 . (1979)\\nRumelhart,D.E.,Smolensky,P.,McClelland,J.L. and Hinton,G .E. Schemata and Sequential\\nthought Processes in PDP Models. chapter 14, vol 2 in Rumelhart and McClelland 1 . pp.757. (1986)\\nThring,A.M. On computable numbers, with an application to the entscheidungsproblem.\\nProc. London Math. Soc. vol 42(3). pp. 230-65. (1936)\\nDowd,R.B. A digital simulation of mew-brain. Report no. UC-DSE/10 5 . pp.25-46. (1977)\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'title', 'abstract',\n",
        "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
        "\n",
        "# sample only 100 papers\n",
        "papers = papers.sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW37wtIFd7Z2"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NAKCOjKRd7Z2",
        "outputId": "70244f7f-03f3-4a74-baa5-9bc7443ae8b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1379    the effect of singularities in a learning\\nmac...\n",
              "3714    scalable training of mixture models via corese...\n",
              "1283    feature selection by maximum marginal\\ndiversi...\n",
              "3756    learning sparse representations of high\\ndimen...\n",
              "1053    fast large-scale transformation-invariant\\nclu...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1379</th>\n",
              "      <td>the effect of singularities in a learning\\nmac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3714</th>\n",
              "      <td>scalable training of mixture models via corese...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>feature selection by maximum marginal\\ndiversi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3756</th>\n",
              "      <td>learning sparse representations of high\\ndimen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>fast large-scale transformation-invariant\\nclu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWNNgdm3d7Z3"
      },
      "source": [
        "##### Tokenize words and further clean-up text\n",
        "\n",
        "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39iFLryyd7Z4",
        "outputId": "e8801387-0f54-48b9-a956-53b3f7f0eef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'effect', 'of', 'singularities', 'in', 'learning', 'machine', 'when', 'the', 'true', 'parameters', 'do', 'not', 'lie', 'on', 'such', 'singularities', 'sumio', 'watanabe', 'precision', 'and', 'intelligence', 'laboratory', 'tokyo', 'institute', 'of', 'technology', 'nagatsuta', 'midori', 'ku']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fITQnyzKd7Z5"
      },
      "source": [
        "** **\n",
        "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
        "** **\n",
        "\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
        "\n",
        "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold.\n",
        "\n",
        "*The higher the values of these param, the harder it is for words to be combined.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qPQE9XuFd7Z6"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v0Xny4Rd7Z7"
      },
      "source": [
        "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
        "\n",
        "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJh_az8fd7Z8",
        "outputId": "94976a84-f63d-4537-cda5-7539f6033bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6A3pz9and7Z8"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnuPzmh1d7Z9"
      },
      "source": [
        "Let's call the functions in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z82geqQd7Z-",
        "outputId": "2169c49c-747e-4837-d77e-bbe23cca1756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLNK7mgZd7Z-",
        "outputId": "05fe754d-0f5b-4a70-e000-814c4896e591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['effect', 'singularity', 'learn', 'machine', 'true', 'parameter', 'lie', 'singularity', 'sumio', 'mathematical', 'neuroscience', 'riken', 'abstract', 'lot', 'learn', 'machine', 'hide', 'variable', 'use', 'information', 'science', 'singularity', 'parameter', 'space', 'singularitie', 'fisher_information', 'matrix', 'become', 'degenerate', 'result']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFkhXtpd7Z_"
      },
      "source": [
        "** **\n",
        "#### Step 4: Data transformation: Corpus and Dictionary\n",
        "** **\n",
        "\n",
        "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLw3RJbZd7aA",
        "outputId": "12569c17-6eba-4102-8f48-6767dc7f1fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 3), (6, 1), (7, 3), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 4), (17, 4), (18, 5), (19, 2), (20, 4), (21, 2), (22, 7), (23, 4), (24, 2), (25, 2), (26, 1), (27, 1), (28, 1), (29, 3)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hFkxZI2d7aA"
      },
      "source": [
        "** **\n",
        "#### Step 5: Base Model\n",
        "** **\n",
        "\n",
        "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
        "\n",
        "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
        "\n",
        "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OX0JnOPhd7aB"
      },
      "outputs": [],
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=10,\n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l-lSVzZd7aC"
      },
      "source": [
        "** **\n",
        "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
        "\n",
        "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ES1_5oBd7aC",
        "outputId": "b6379a79-6979-45b9-842b-2096e5eec3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.016*\"model\" + 0.010*\"learn\" + 0.009*\"image\" + 0.009*\"use\" + 0.008*\"set\" + '\n",
            "  '0.007*\"training\" + 0.007*\"distribution\" + 0.006*\"error\" + 0.006*\"parameter\" '\n",
            "  '+ 0.006*\"object\"'),\n",
            " (1,\n",
            "  '0.018*\"model\" + 0.014*\"object\" + 0.009*\"datum\" + 0.008*\"intervention\" + '\n",
            "  '0.008*\"set\" + 0.008*\"view\" + 0.007*\"time\" + 0.007*\"change\" + 0.006*\"show\" + '\n",
            "  '0.006*\"variable\"'),\n",
            " (2,\n",
            "  '0.016*\"feature\" + 0.010*\"function\" + 0.010*\"learn\" + 0.008*\"result\" + '\n",
            "  '0.008*\"set\" + 0.007*\"method\" + 0.007*\"class\" + 0.007*\"use\" + 0.007*\"space\" '\n",
            "  '+ 0.006*\"problem\"'),\n",
            " (3,\n",
            "  '0.008*\"input\" + 0.007*\"chip\" + 0.006*\"time\" + 0.006*\"function\" + '\n",
            "  '0.006*\"figure\" + 0.006*\"model\" + 0.006*\"use\" + 0.005*\"set\" + 0.005*\"weight\" '\n",
            "  '+ 0.005*\"neural\"'),\n",
            " (4,\n",
            "  '0.013*\"model\" + 0.012*\"pool\" + 0.012*\"input\" + 0.010*\"image\" + '\n",
            "  '0.009*\"layer\" + 0.008*\"training\" + 0.007*\"feature\" + 0.007*\"output\" + '\n",
            "  '0.007*\"dataset\" + 0.007*\"use\"'),\n",
            " (5,\n",
            "  '0.010*\"function\" + 0.009*\"use\" + 0.008*\"learn\" + 0.008*\"model\" + '\n",
            "  '0.008*\"loss\" + 0.008*\"set\" + 0.006*\"result\" + 0.006*\"problem\" + '\n",
            "  '0.005*\"algorithm\" + 0.005*\"show\"'),\n",
            " (6,\n",
            "  '0.013*\"learn\" + 0.009*\"unit\" + 0.009*\"use\" + 0.009*\"input\" + 0.008*\"layer\" '\n",
            "  '+ 0.007*\"rule\" + 0.007*\"neuron\" + 0.007*\"response\" + 0.007*\"network\" + '\n",
            "  '0.007*\"model\"'),\n",
            " (7,\n",
            "  '0.016*\"model\" + 0.011*\"datum\" + 0.011*\"use\" + 0.009*\"distribution\" + '\n",
            "  '0.007*\"set\" + 0.007*\"sample\" + 0.006*\"time\" + 0.006*\"parameter\" + '\n",
            "  '0.006*\"learn\" + 0.006*\"cluster\"'),\n",
            " (8,\n",
            "  '0.016*\"cascade\" + 0.014*\"use\" + 0.009*\"stage\" + 0.007*\"current\" + '\n",
            "  '0.007*\"update\" + 0.006*\"set\" + 0.006*\"complexity\" + 0.006*\"design\" + '\n",
            "  '0.005*\"boost\" + 0.005*\"large\"'),\n",
            " (9,\n",
            "  '0.011*\"network\" + 0.011*\"model\" + 0.008*\"matrix\" + 0.007*\"time\" + '\n",
            "  '0.007*\"use\" + 0.007*\"set\" + 0.007*\"topic\" + 0.006*\"function\" + 0.006*\"edge\" '\n",
            "  '+ 0.006*\"feature\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l0lXY61d7aD"
      },
      "source": [
        "#### Compute Model Perplexity and Coherence Score\n",
        "\n",
        "Let's calculate the baseline coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2fryRM8d7aD",
        "outputId": "9c40fbbb-0f38-486b-acef-f67b7c2b3460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.3035608453485744\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWLRk2Dgd7aE"
      },
      "source": [
        "** **\n",
        "#### Step 6: Hyperparameter tuning\n",
        "** **\n",
        "First, let's differentiate between model hyperparameters and model parameters :\n",
        "\n",
        "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
        "\n",
        "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
        "\n",
        "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters:\n",
        "- Number of Topics (K)\n",
        "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
        "- Dirichlet hyperparameter beta: Word-Topic Density\n",
        "\n",
        "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o1DUXJK7d7aE"
      },
      "outputs": [],
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "\n",
        "    return coherence_model_lda.get_coherence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6-0yCqld7aF"
      },
      "source": [
        "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orkpUeC7d7aG",
        "outputId": "e779e05c-2a2c-43c3-d87f-f14f60975c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 540/540 [1:46:46<00:00, 11.86s/it]\n",
            "\n",
            "  0%|          | 1/540 [00:07<1:09:16,  7.71s/it]\u001b[A\n",
            "  0%|          | 2/540 [00:16<1:17:23,  8.63s/it]\u001b[A\n",
            "  1%|          | 3/540 [00:25<1:15:31,  8.44s/it]\u001b[A\n",
            "  1%|          | 4/540 [00:34<1:17:55,  8.72s/it]\u001b[A\n",
            "  1%|          | 5/540 [00:43<1:19:37,  8.93s/it]\u001b[A\n",
            "  1%|          | 6/540 [00:52<1:18:41,  8.84s/it]\u001b[A\n",
            "  1%|▏         | 7/540 [01:04<1:27:06,  9.81s/it]\u001b[A\n",
            "  1%|▏         | 8/540 [01:15<1:30:57, 10.26s/it]\u001b[A\n",
            "  2%|▏         | 9/540 [01:23<1:23:49,  9.47s/it]\u001b[A\n",
            "  2%|▏         | 10/540 [01:32<1:24:10,  9.53s/it]\u001b[A\n",
            "  2%|▏         | 11/540 [01:41<1:22:07,  9.31s/it]\u001b[A\n",
            "  2%|▏         | 12/540 [01:50<1:21:46,  9.29s/it]\u001b[A\n",
            "  2%|▏         | 13/540 [02:00<1:21:29,  9.28s/it]\u001b[A\n",
            "  3%|▎         | 14/540 [02:08<1:18:43,  8.98s/it]\u001b[A\n",
            "  3%|▎         | 15/540 [02:18<1:20:32,  9.20s/it]\u001b[A\n",
            "  3%|▎         | 16/540 [02:27<1:21:20,  9.31s/it]\u001b[A\n",
            "  3%|▎         | 17/540 [02:35<1:17:26,  8.88s/it]\u001b[A\n",
            "  3%|▎         | 18/540 [02:45<1:20:37,  9.27s/it]\u001b[A\n",
            "  4%|▎         | 19/540 [02:55<1:20:48,  9.31s/it]\u001b[A\n",
            "  4%|▎         | 20/540 [03:03<1:17:23,  8.93s/it]\u001b[A\n",
            "  4%|▍         | 21/540 [03:13<1:20:28,  9.30s/it]\u001b[A\n",
            "  4%|▍         | 22/540 [03:22<1:19:13,  9.18s/it]\u001b[A\n",
            "  4%|▍         | 23/540 [03:30<1:16:12,  8.84s/it]\u001b[A\n",
            "  4%|▍         | 24/540 [03:39<1:16:45,  8.93s/it]\u001b[A\n",
            "  5%|▍         | 25/540 [03:48<1:15:58,  8.85s/it]\u001b[A\n",
            "  5%|▍         | 26/540 [03:57<1:16:38,  8.95s/it]\u001b[A\n",
            "  5%|▌         | 27/540 [04:06<1:17:34,  9.07s/it]\u001b[A\n",
            "  5%|▌         | 28/540 [04:14<1:14:07,  8.69s/it]\u001b[A\n",
            "  5%|▌         | 29/540 [04:26<1:21:45,  9.60s/it]\u001b[A\n",
            "  6%|▌         | 30/540 [04:35<1:20:49,  9.51s/it]\u001b[A\n",
            "  6%|▌         | 31/540 [04:43<1:17:36,  9.15s/it]\u001b[A\n",
            "  6%|▌         | 32/540 [04:53<1:19:01,  9.33s/it]\u001b[A\n",
            "  6%|▌         | 33/540 [05:02<1:18:22,  9.27s/it]\u001b[A\n",
            "  6%|▋         | 34/540 [05:10<1:15:54,  9.00s/it]\u001b[A\n",
            "  6%|▋         | 35/540 [05:20<1:17:47,  9.24s/it]\u001b[A\n",
            "  7%|▋         | 36/540 [05:30<1:18:21,  9.33s/it]\u001b[A\n",
            "  7%|▋         | 37/540 [05:39<1:17:07,  9.20s/it]\u001b[A\n",
            "  7%|▋         | 38/540 [05:49<1:18:32,  9.39s/it]\u001b[A\n",
            "  7%|▋         | 39/540 [05:57<1:16:56,  9.21s/it]\u001b[A\n",
            "  7%|▋         | 40/540 [06:07<1:17:14,  9.27s/it]\u001b[A\n",
            "  8%|▊         | 41/540 [06:16<1:18:16,  9.41s/it]\u001b[A\n",
            "  8%|▊         | 42/540 [06:25<1:16:15,  9.19s/it]\u001b[A\n",
            "  8%|▊         | 43/540 [06:35<1:17:37,  9.37s/it]\u001b[A\n",
            "  8%|▊         | 44/540 [06:45<1:18:02,  9.44s/it]\u001b[A\n",
            "  8%|▊         | 45/540 [06:53<1:15:29,  9.15s/it]\u001b[A\n",
            "  9%|▊         | 46/540 [07:03<1:17:17,  9.39s/it]\u001b[A\n",
            "  9%|▊         | 47/540 [07:13<1:17:43,  9.46s/it]\u001b[A\n",
            "  9%|▉         | 48/540 [07:21<1:15:34,  9.22s/it]\u001b[A\n",
            "  9%|▉         | 49/540 [07:33<1:21:24,  9.95s/it]\u001b[A\n",
            "  9%|▉         | 50/540 [07:43<1:21:59, 10.04s/it]\u001b[A\n",
            "  9%|▉         | 51/540 [07:53<1:21:25,  9.99s/it]\u001b[A\n",
            " 10%|▉         | 52/540 [08:01<1:17:01,  9.47s/it]\u001b[A\n",
            " 10%|▉         | 53/540 [08:11<1:17:11,  9.51s/it]\u001b[A\n",
            " 10%|█         | 54/540 [08:20<1:16:21,  9.43s/it]\u001b[A\n",
            " 10%|█         | 55/540 [08:29<1:13:52,  9.14s/it]\u001b[A\n",
            " 10%|█         | 56/540 [08:39<1:15:42,  9.39s/it]\u001b[A\n",
            " 11%|█         | 57/540 [08:49<1:17:08,  9.58s/it]\u001b[A\n",
            " 11%|█         | 58/540 [08:57<1:13:48,  9.19s/it]\u001b[A\n",
            " 11%|█         | 59/540 [09:07<1:14:58,  9.35s/it]\u001b[A\n",
            " 11%|█         | 60/540 [09:16<1:15:02,  9.38s/it]\u001b[A\n",
            " 11%|█▏        | 61/540 [09:25<1:13:39,  9.23s/it]\u001b[A\n",
            " 11%|█▏        | 62/540 [09:35<1:15:14,  9.44s/it]\u001b[A\n",
            " 12%|█▏        | 63/540 [09:45<1:15:44,  9.53s/it]\u001b[A\n",
            " 12%|█▏        | 64/540 [09:53<1:12:31,  9.14s/it]\u001b[A\n",
            " 12%|█▏        | 65/540 [10:03<1:14:10,  9.37s/it]\u001b[A\n",
            " 12%|█▏        | 66/540 [10:13<1:15:52,  9.61s/it]\u001b[A\n",
            " 12%|█▏        | 67/540 [10:21<1:12:33,  9.20s/it]\u001b[A\n",
            " 13%|█▎        | 68/540 [10:33<1:18:39, 10.00s/it]\u001b[A\n",
            " 13%|█▎        | 69/540 [10:43<1:17:38,  9.89s/it]\u001b[A\n",
            " 13%|█▎        | 70/540 [10:51<1:13:55,  9.44s/it]\u001b[A\n",
            " 13%|█▎        | 71/540 [11:01<1:15:30,  9.66s/it]\u001b[A\n",
            " 13%|█▎        | 72/540 [11:11<1:16:24,  9.80s/it]\u001b[A\n",
            " 14%|█▎        | 73/540 [11:20<1:13:20,  9.42s/it]\u001b[A\n",
            " 14%|█▎        | 74/540 [11:30<1:15:58,  9.78s/it]\u001b[A\n",
            " 14%|█▍        | 75/540 [11:40<1:16:04,  9.82s/it]\u001b[A\n",
            " 14%|█▍        | 76/540 [11:50<1:14:30,  9.63s/it]\u001b[A\n",
            " 14%|█▍        | 77/540 [12:00<1:15:28,  9.78s/it]\u001b[A\n",
            " 14%|█▍        | 78/540 [12:10<1:17:13, 10.03s/it]\u001b[A\n",
            " 15%|█▍        | 79/540 [12:19<1:14:55,  9.75s/it]\u001b[A\n",
            " 15%|█▍        | 80/540 [12:29<1:14:45,  9.75s/it]\u001b[A\n",
            " 15%|█▌        | 81/540 [12:39<1:14:56,  9.80s/it]\u001b[A\n",
            " 15%|█▌        | 82/540 [12:48<1:13:10,  9.59s/it]\u001b[A\n",
            " 15%|█▌        | 83/540 [12:57<1:11:59,  9.45s/it]\u001b[A\n",
            " 16%|█▌        | 84/540 [13:07<1:12:25,  9.53s/it]\u001b[A\n",
            " 16%|█▌        | 85/540 [13:16<1:10:32,  9.30s/it]\u001b[A\n",
            " 16%|█▌        | 86/540 [13:25<1:11:11,  9.41s/it]\u001b[A\n",
            " 16%|█▌        | 87/540 [13:36<1:13:49,  9.78s/it]\u001b[A\n",
            " 16%|█▋        | 88/540 [13:46<1:14:49,  9.93s/it]\u001b[A\n",
            " 16%|█▋        | 89/540 [13:55<1:11:01,  9.45s/it]\u001b[A\n",
            " 17%|█▋        | 90/540 [14:05<1:13:16,  9.77s/it]\u001b[A\n",
            " 17%|█▋        | 91/540 [14:16<1:14:21,  9.94s/it]\u001b[A\n",
            " 17%|█▋        | 92/540 [14:24<1:11:35,  9.59s/it]\u001b[A\n",
            " 17%|█▋        | 93/540 [14:35<1:13:17,  9.84s/it]\u001b[A\n",
            " 17%|█▋        | 94/540 [14:45<1:14:56, 10.08s/it]\u001b[A\n",
            " 18%|█▊        | 95/540 [14:55<1:14:10, 10.00s/it]\u001b[A\n",
            " 18%|█▊        | 96/540 [15:06<1:14:49, 10.11s/it]\u001b[A\n",
            " 18%|█▊        | 97/540 [15:16<1:15:38, 10.24s/it]\u001b[A\n",
            " 18%|█▊        | 98/540 [15:27<1:16:04, 10.33s/it]\u001b[A\n",
            " 18%|█▊        | 99/540 [15:35<1:12:24,  9.85s/it]\u001b[A\n",
            " 19%|█▊        | 100/540 [15:47<1:15:07, 10.24s/it]\u001b[A\n",
            " 19%|█▊        | 101/540 [15:58<1:16:52, 10.51s/it]\u001b[A\n",
            " 19%|█▉        | 102/540 [16:08<1:16:10, 10.44s/it]\u001b[A\n",
            " 19%|█▉        | 103/540 [16:17<1:12:53, 10.01s/it]\u001b[A\n",
            " 19%|█▉        | 104/540 [16:28<1:15:11, 10.35s/it]\u001b[A\n",
            " 19%|█▉        | 105/540 [16:41<1:19:46, 11.00s/it]\u001b[A\n",
            " 20%|█▉        | 106/540 [16:52<1:19:25, 10.98s/it]\u001b[A\n",
            " 20%|█▉        | 107/540 [17:02<1:17:10, 10.69s/it]\u001b[A\n",
            " 20%|██        | 108/540 [17:12<1:16:55, 10.68s/it]\u001b[A\n",
            " 20%|██        | 109/540 [17:23<1:16:44, 10.68s/it]\u001b[A\n",
            " 20%|██        | 110/540 [17:32<1:13:59, 10.32s/it]\u001b[A\n",
            " 21%|██        | 111/540 [17:45<1:18:28, 10.98s/it]\u001b[A\n",
            " 21%|██        | 112/540 [17:56<1:17:56, 10.93s/it]\u001b[A\n",
            " 21%|██        | 113/540 [18:06<1:17:10, 10.84s/it]\u001b[A\n",
            " 21%|██        | 114/540 [18:15<1:13:10, 10.31s/it]\u001b[A\n",
            " 21%|██▏       | 115/540 [18:27<1:16:07, 10.75s/it]\u001b[A\n",
            " 21%|██▏       | 116/540 [18:38<1:16:36, 10.84s/it]\u001b[A\n",
            " 22%|██▏       | 117/540 [18:49<1:15:37, 10.73s/it]\u001b[A\n",
            " 22%|██▏       | 118/540 [18:58<1:12:12, 10.27s/it]\u001b[A\n",
            " 22%|██▏       | 119/540 [19:09<1:13:25, 10.47s/it]\u001b[A\n",
            " 22%|██▏       | 120/540 [19:20<1:14:07, 10.59s/it]\u001b[A\n",
            " 22%|██▏       | 121/540 [19:31<1:15:56, 10.87s/it]\u001b[A\n",
            " 23%|██▎       | 122/540 [19:44<1:19:51, 11.46s/it]\u001b[A\n",
            " 23%|██▎       | 123/540 [19:55<1:18:13, 11.25s/it]\u001b[A\n",
            " 23%|██▎       | 124/540 [20:07<1:19:47, 11.51s/it]\u001b[A\n",
            " 23%|██▎       | 125/540 [20:18<1:18:24, 11.34s/it]\u001b[A\n",
            " 23%|██▎       | 126/540 [20:30<1:19:11, 11.48s/it]\u001b[A\n",
            " 24%|██▎       | 127/540 [20:39<1:14:50, 10.87s/it]\u001b[A\n",
            " 24%|██▎       | 128/540 [20:51<1:17:20, 11.26s/it]\u001b[A\n",
            " 24%|██▍       | 129/540 [21:03<1:17:30, 11.31s/it]\u001b[A\n",
            " 24%|██▍       | 130/540 [21:14<1:17:35, 11.35s/it]\u001b[A\n",
            " 24%|██▍       | 131/540 [21:27<1:19:29, 11.66s/it]\u001b[A\n",
            " 24%|██▍       | 132/540 [21:37<1:17:06, 11.34s/it]\u001b[A\n",
            " 25%|██▍       | 133/540 [21:49<1:17:05, 11.37s/it]\u001b[A\n",
            " 25%|██▍       | 134/540 [22:00<1:16:22, 11.29s/it]\u001b[A\n",
            " 25%|██▌       | 135/540 [22:11<1:16:05, 11.27s/it]\u001b[A\n",
            " 25%|██▌       | 136/540 [22:22<1:15:28, 11.21s/it]\u001b[A\n",
            " 25%|██▌       | 137/540 [22:32<1:13:16, 10.91s/it]\u001b[A\n",
            " 26%|██▌       | 138/540 [22:46<1:18:53, 11.77s/it]\u001b[A\n",
            " 26%|██▌       | 139/540 [22:58<1:18:22, 11.73s/it]\u001b[A\n",
            " 26%|██▌       | 140/540 [23:09<1:17:18, 11.60s/it]\u001b[A\n",
            " 26%|██▌       | 141/540 [23:20<1:15:18, 11.32s/it]\u001b[A\n",
            " 26%|██▋       | 142/540 [23:30<1:14:08, 11.18s/it]\u001b[A\n",
            " 26%|██▋       | 143/540 [23:42<1:13:55, 11.17s/it]\u001b[A\n",
            " 27%|██▋       | 144/540 [23:53<1:13:15, 11.10s/it]\u001b[A\n",
            " 27%|██▋       | 145/540 [24:03<1:11:02, 10.79s/it]\u001b[A\n",
            " 27%|██▋       | 146/540 [24:14<1:11:28, 10.88s/it]\u001b[A\n",
            " 27%|██▋       | 147/540 [24:25<1:11:13, 10.87s/it]\u001b[A\n",
            " 27%|██▋       | 148/540 [24:36<1:11:13, 10.90s/it]\u001b[A\n",
            " 28%|██▊       | 149/540 [24:45<1:08:51, 10.57s/it]\u001b[A\n",
            " 28%|██▊       | 150/540 [24:56<1:09:40, 10.72s/it]\u001b[A\n",
            " 28%|██▊       | 151/540 [25:08<1:11:39, 11.05s/it]\u001b[A\n",
            " 28%|██▊       | 152/540 [25:19<1:11:31, 11.06s/it]\u001b[A\n",
            " 28%|██▊       | 153/540 [25:31<1:12:29, 11.24s/it]\u001b[A\n",
            " 29%|██▊       | 154/540 [25:42<1:11:49, 11.17s/it]\u001b[A\n",
            " 29%|██▊       | 155/540 [25:55<1:15:17, 11.73s/it]\u001b[A\n",
            " 29%|██▉       | 156/540 [26:07<1:15:38, 11.82s/it]\u001b[A\n",
            " 29%|██▉       | 157/540 [26:18<1:14:03, 11.60s/it]\u001b[A\n",
            " 29%|██▉       | 158/540 [26:29<1:12:33, 11.40s/it]\u001b[A\n",
            " 29%|██▉       | 159/540 [26:40<1:10:55, 11.17s/it]\u001b[A\n",
            " 30%|██▉       | 160/540 [26:52<1:12:01, 11.37s/it]\u001b[A\n",
            " 30%|██▉       | 161/540 [27:04<1:13:58, 11.71s/it]\u001b[A\n",
            " 30%|███       | 162/540 [27:15<1:13:14, 11.62s/it]\u001b[A\n",
            " 30%|███       | 163/540 [27:27<1:12:55, 11.61s/it]\u001b[A\n",
            " 30%|███       | 164/540 [27:37<1:10:09, 11.20s/it]\u001b[A\n",
            " 31%|███       | 165/540 [27:50<1:12:21, 11.58s/it]\u001b[A\n",
            " 31%|███       | 166/540 [28:02<1:13:33, 11.80s/it]\u001b[A\n",
            " 31%|███       | 167/540 [28:13<1:12:41, 11.69s/it]\u001b[A\n",
            " 31%|███       | 168/540 [28:25<1:12:36, 11.71s/it]\u001b[A\n",
            " 31%|███▏      | 169/540 [28:35<1:09:29, 11.24s/it]\u001b[A\n",
            " 31%|███▏      | 170/540 [28:49<1:14:01, 12.00s/it]\u001b[A\n",
            " 32%|███▏      | 171/540 [29:02<1:15:17, 12.24s/it]\u001b[A\n",
            " 32%|███▏      | 172/540 [29:13<1:13:05, 11.92s/it]\u001b[A\n",
            " 32%|███▏      | 173/540 [29:24<1:11:47, 11.74s/it]\u001b[A\n",
            " 32%|███▏      | 174/540 [29:34<1:08:29, 11.23s/it]\u001b[A\n",
            " 32%|███▏      | 175/540 [29:46<1:08:43, 11.30s/it]\u001b[A\n",
            " 33%|███▎      | 176/540 [29:58<1:09:33, 11.47s/it]\u001b[A\n",
            " 33%|███▎      | 177/540 [30:09<1:08:27, 11.32s/it]\u001b[A\n",
            " 33%|███▎      | 178/540 [30:20<1:07:44, 11.23s/it]\u001b[A\n",
            " 33%|███▎      | 179/540 [30:30<1:04:59, 10.80s/it]\u001b[A\n",
            " 33%|███▎      | 180/540 [30:41<1:06:27, 11.08s/it]\u001b[A\n",
            " 34%|███▎      | 181/540 [30:53<1:07:06, 11.22s/it]\u001b[A\n",
            " 34%|███▎      | 182/540 [31:05<1:07:59, 11.39s/it]\u001b[A\n",
            " 34%|███▍      | 183/540 [31:15<1:06:37, 11.20s/it]\u001b[A\n",
            " 34%|███▍      | 184/540 [31:26<1:04:30, 10.87s/it]\u001b[A\n",
            " 34%|███▍      | 185/540 [31:39<1:08:16, 11.54s/it]\u001b[A\n",
            " 34%|███▍      | 186/540 [31:51<1:08:49, 11.66s/it]\u001b[A\n",
            " 35%|███▍      | 187/540 [32:02<1:08:11, 11.59s/it]\u001b[A\n",
            " 35%|███▍      | 188/540 [32:13<1:06:57, 11.41s/it]\u001b[A\n",
            " 35%|███▌      | 189/540 [32:23<1:03:28, 10.85s/it]\u001b[A\n",
            " 35%|███▌      | 190/540 [32:34<1:04:29, 11.06s/it]\u001b[A\n",
            " 35%|███▌      | 191/540 [32:47<1:06:47, 11.48s/it]\u001b[A\n",
            " 36%|███▌      | 192/540 [32:58<1:06:53, 11.53s/it]\u001b[A\n",
            " 36%|███▌      | 193/540 [33:10<1:06:43, 11.54s/it]\u001b[A\n",
            " 36%|███▌      | 194/540 [33:20<1:03:57, 11.09s/it]\u001b[A\n",
            " 36%|███▌      | 195/540 [33:32<1:05:12, 11.34s/it]\u001b[A\n",
            " 36%|███▋      | 196/540 [33:44<1:06:52, 11.66s/it]\u001b[A\n",
            " 36%|███▋      | 197/540 [33:56<1:07:48, 11.86s/it]\u001b[A\n",
            " 37%|███▋      | 198/540 [34:08<1:06:20, 11.64s/it]\u001b[A\n",
            " 37%|███▋      | 199/540 [34:18<1:04:29, 11.35s/it]\u001b[A\n",
            " 37%|███▋      | 200/540 [34:29<1:02:54, 11.10s/it]\u001b[A\n",
            " 37%|███▋      | 201/540 [34:43<1:07:17, 11.91s/it]\u001b[A\n",
            " 37%|███▋      | 202/540 [34:54<1:06:17, 11.77s/it]\u001b[A\n",
            " 38%|███▊      | 203/540 [35:05<1:05:27, 11.66s/it]\u001b[A\n",
            " 38%|███▊      | 204/540 [35:15<1:02:40, 11.19s/it]\u001b[A\n",
            " 38%|███▊      | 205/540 [35:26<1:01:51, 11.08s/it]\u001b[A\n",
            " 38%|███▊      | 206/540 [35:39<1:03:37, 11.43s/it]\u001b[A\n",
            " 38%|███▊      | 207/540 [35:50<1:03:48, 11.50s/it]\u001b[A\n",
            " 39%|███▊      | 208/540 [36:02<1:03:23, 11.46s/it]\u001b[A\n",
            " 39%|███▊      | 209/540 [36:12<1:01:18, 11.11s/it]\u001b[A\n",
            " 39%|███▉      | 210/540 [36:24<1:02:00, 11.28s/it]\u001b[A\n",
            " 39%|███▉      | 211/540 [36:37<1:05:03, 11.87s/it]\u001b[A\n",
            " 39%|███▉      | 212/540 [36:49<1:04:54, 11.87s/it]\u001b[A\n",
            " 39%|███▉      | 213/540 [37:00<1:04:32, 11.84s/it]\u001b[A\n",
            " 40%|███▉      | 214/540 [37:12<1:03:25, 11.67s/it]\u001b[A\n",
            " 40%|███▉      | 215/540 [37:22<1:01:41, 11.39s/it]\u001b[A\n",
            " 40%|████      | 216/540 [37:38<1:07:30, 12.50s/it]\u001b[A\n",
            " 40%|████      | 217/540 [37:49<1:05:32, 12.18s/it]\u001b[A\n",
            " 40%|████      | 218/540 [38:01<1:05:27, 12.20s/it]\u001b[A\n",
            " 41%|████      | 219/540 [38:13<1:04:11, 12.00s/it]\u001b[A\n",
            " 41%|████      | 220/540 [38:24<1:03:27, 11.90s/it]\u001b[A\n",
            " 41%|████      | 221/540 [38:37<1:04:43, 12.17s/it]\u001b[A\n",
            " 41%|████      | 222/540 [38:48<1:03:04, 11.90s/it]\u001b[A\n",
            " 41%|████▏     | 223/540 [39:00<1:01:53, 11.72s/it]\u001b[A\n",
            " 41%|████▏     | 224/540 [39:11<1:01:08, 11.61s/it]\u001b[A\n",
            " 42%|████▏     | 225/540 [39:23<1:01:01, 11.62s/it]\u001b[A\n",
            " 42%|████▏     | 226/540 [39:36<1:03:27, 12.13s/it]\u001b[A\n",
            " 42%|████▏     | 227/540 [39:48<1:03:36, 12.19s/it]\u001b[A\n",
            " 42%|████▏     | 228/540 [40:00<1:02:24, 12.00s/it]\u001b[A\n",
            " 42%|████▏     | 229/540 [40:10<59:37, 11.50s/it]  \u001b[A\n",
            " 43%|████▎     | 230/540 [40:23<1:01:04, 11.82s/it]\u001b[A\n",
            " 43%|████▎     | 231/540 [40:38<1:05:19, 12.68s/it]\u001b[A\n",
            " 43%|████▎     | 232/540 [40:49<1:03:49, 12.43s/it]\u001b[A\n",
            " 43%|████▎     | 233/540 [41:01<1:02:44, 12.26s/it]\u001b[A\n",
            " 43%|████▎     | 234/540 [41:12<59:45, 11.72s/it]  \u001b[A\n",
            " 44%|████▎     | 235/540 [41:23<59:06, 11.63s/it]\u001b[A\n",
            " 44%|████▎     | 236/540 [41:36<1:01:26, 12.13s/it]\u001b[A\n",
            " 44%|████▍     | 237/540 [41:48<1:00:20, 11.95s/it]\u001b[A\n",
            " 44%|████▍     | 238/540 [42:00<59:41, 11.86s/it]  \u001b[A\n",
            " 44%|████▍     | 239/540 [42:11<58:55, 11.74s/it]\u001b[A\n",
            " 44%|████▍     | 240/540 [42:21<56:13, 11.24s/it]\u001b[A\n",
            " 45%|████▍     | 241/540 [42:34<58:57, 11.83s/it]\u001b[A\n",
            " 45%|████▍     | 242/540 [42:47<1:00:09, 12.11s/it]\u001b[A\n",
            " 45%|████▌     | 243/540 [43:00<1:00:31, 12.23s/it]\u001b[A\n",
            " 45%|████▌     | 244/540 [43:12<1:00:00, 12.16s/it]\u001b[A\n",
            " 45%|████▌     | 245/540 [43:26<1:02:53, 12.79s/it]\u001b[A\n",
            " 46%|████▌     | 246/540 [43:40<1:04:42, 13.21s/it]\u001b[A\n",
            " 46%|████▌     | 247/540 [43:53<1:04:27, 13.20s/it]\u001b[A\n",
            " 46%|████▌     | 248/540 [44:06<1:03:53, 13.13s/it]\u001b[A\n",
            " 46%|████▌     | 249/540 [44:19<1:02:24, 12.87s/it]\u001b[A\n",
            " 46%|████▋     | 250/540 [44:32<1:02:23, 12.91s/it]\u001b[A\n",
            " 46%|████▋     | 251/540 [44:46<1:04:33, 13.40s/it]\u001b[A\n",
            " 47%|████▋     | 252/540 [44:59<1:03:22, 13.20s/it]\u001b[A\n",
            " 47%|████▋     | 253/540 [45:12<1:02:29, 13.07s/it]\u001b[A\n",
            " 47%|████▋     | 254/540 [45:23<59:32, 12.49s/it]  \u001b[A\n",
            " 47%|████▋     | 255/540 [45:35<59:43, 12.57s/it]\u001b[A\n",
            " 47%|████▋     | 256/540 [45:50<1:01:53, 13.08s/it]\u001b[A\n",
            " 48%|████▊     | 257/540 [46:03<1:01:42, 13.08s/it]\u001b[A\n",
            " 48%|████▊     | 258/540 [46:17<1:03:27, 13.50s/it]\u001b[A\n",
            " 48%|████▊     | 259/540 [46:29<1:01:22, 13.11s/it]\u001b[A\n",
            " 48%|████▊     | 260/540 [46:42<1:00:56, 13.06s/it]\u001b[A\n",
            " 48%|████▊     | 261/540 [46:56<1:01:38, 13.26s/it]\u001b[A\n",
            " 49%|████▊     | 262/540 [47:09<1:01:12, 13.21s/it]\u001b[A\n",
            " 49%|████▊     | 263/540 [47:23<1:01:08, 13.24s/it]\u001b[A\n",
            " 49%|████▉     | 264/540 [47:35<59:23, 12.91s/it]  \u001b[A\n",
            " 49%|████▉     | 265/540 [47:49<1:00:52, 13.28s/it]\u001b[A\n",
            " 49%|████▉     | 266/540 [48:03<1:01:32, 13.48s/it]\u001b[A\n",
            " 49%|████▉     | 267/540 [48:16<1:00:21, 13.26s/it]\u001b[A\n",
            " 50%|████▉     | 268/540 [48:28<58:21, 12.87s/it]  \u001b[A\n",
            " 50%|████▉     | 269/540 [48:39<56:21, 12.48s/it]\u001b[A\n",
            " 50%|█████     | 270/540 [48:52<56:34, 12.57s/it]\u001b[A\n",
            " 50%|█████     | 271/540 [49:03<54:06, 12.07s/it]\u001b[A\n",
            " 50%|█████     | 272/540 [49:15<54:24, 12.18s/it]\u001b[A\n",
            " 51%|█████     | 273/540 [49:24<50:06, 11.26s/it]\u001b[A\n",
            " 51%|█████     | 274/540 [49:35<49:04, 11.07s/it]\u001b[A\n",
            " 51%|█████     | 275/540 [49:46<48:14, 10.92s/it]\u001b[A\n",
            " 51%|█████     | 276/540 [49:56<47:47, 10.86s/it]\u001b[A\n",
            " 51%|█████▏    | 277/540 [50:05<45:17, 10.33s/it]\u001b[A\n",
            " 51%|█████▏    | 278/540 [50:17<46:27, 10.64s/it]\u001b[A\n",
            " 52%|█████▏    | 279/540 [50:27<46:25, 10.67s/it]\u001b[A\n",
            " 52%|█████▏    | 280/540 [50:37<44:57, 10.37s/it]\u001b[A\n",
            " 52%|█████▏    | 281/540 [50:48<45:48, 10.61s/it]\u001b[A\n",
            " 52%|█████▏    | 282/540 [50:59<46:10, 10.74s/it]\u001b[A\n",
            " 52%|█████▏    | 283/540 [51:10<46:02, 10.75s/it]\u001b[A\n",
            " 53%|█████▎    | 284/540 [51:19<43:48, 10.27s/it]\u001b[A\n",
            " 53%|█████▎    | 285/540 [51:30<44:54, 10.57s/it]\u001b[A\n",
            " 53%|█████▎    | 286/540 [51:41<45:11, 10.67s/it]\u001b[A\n",
            " 53%|█████▎    | 287/540 [51:52<45:25, 10.77s/it]\u001b[A\n",
            " 53%|█████▎    | 288/540 [52:04<46:39, 11.11s/it]\u001b[A\n",
            " 54%|█████▎    | 289/540 [52:14<45:09, 10.80s/it]\u001b[A\n",
            " 54%|█████▎    | 290/540 [52:26<45:33, 10.93s/it]\u001b[A\n",
            " 54%|█████▍    | 291/540 [52:36<45:14, 10.90s/it]\u001b[A\n",
            " 54%|█████▍    | 292/540 [52:46<43:27, 10.51s/it]\u001b[A\n",
            " 54%|█████▍    | 293/540 [52:57<43:52, 10.66s/it]\u001b[A\n",
            " 54%|█████▍    | 294/540 [53:08<43:53, 10.71s/it]\u001b[A\n",
            " 55%|█████▍    | 295/540 [53:19<43:43, 10.71s/it]\u001b[A\n",
            " 55%|█████▍    | 296/540 [53:28<41:43, 10.26s/it]\u001b[A\n",
            " 55%|█████▌    | 297/540 [53:39<42:57, 10.61s/it]\u001b[A\n",
            " 55%|█████▌    | 298/540 [53:50<42:44, 10.60s/it]\u001b[A\n",
            " 55%|█████▌    | 299/540 [54:00<42:06, 10.48s/it]\u001b[A\n",
            " 56%|█████▌    | 300/540 [54:10<41:13, 10.31s/it]\u001b[A\n",
            " 56%|█████▌    | 301/540 [54:22<43:00, 10.80s/it]\u001b[A\n",
            " 56%|█████▌    | 302/540 [54:33<43:28, 10.96s/it]\u001b[A\n",
            " 56%|█████▌    | 303/540 [54:44<42:52, 10.85s/it]\u001b[A\n",
            " 56%|█████▋    | 304/540 [54:56<44:00, 11.19s/it]\u001b[A\n",
            " 56%|█████▋    | 305/540 [55:07<44:21, 11.33s/it]\u001b[A\n",
            " 57%|█████▋    | 306/540 [55:19<44:21, 11.37s/it]\u001b[A\n",
            " 57%|█████▋    | 307/540 [55:30<44:06, 11.36s/it]\u001b[A\n",
            " 57%|█████▋    | 308/540 [55:40<42:33, 11.01s/it]\u001b[A\n",
            " 57%|█████▋    | 309/540 [55:52<43:30, 11.30s/it]\u001b[A\n",
            " 57%|█████▋    | 310/540 [56:05<44:28, 11.60s/it]\u001b[A\n",
            " 58%|█████▊    | 311/540 [56:17<44:49, 11.74s/it]\u001b[A\n",
            " 58%|█████▊    | 312/540 [56:29<44:36, 11.74s/it]\u001b[A\n",
            " 58%|█████▊    | 313/540 [56:40<44:01, 11.64s/it]\u001b[A\n",
            " 58%|█████▊    | 314/540 [56:50<42:14, 11.21s/it]\u001b[A\n",
            " 58%|█████▊    | 315/540 [57:02<42:33, 11.35s/it]\u001b[A\n",
            " 59%|█████▊    | 316/540 [57:14<42:55, 11.50s/it]\u001b[A\n",
            " 59%|█████▊    | 317/540 [57:25<42:24, 11.41s/it]\u001b[A\n",
            " 59%|█████▉    | 318/540 [57:35<41:18, 11.16s/it]\u001b[A\n",
            " 59%|█████▉    | 319/540 [57:46<40:52, 11.10s/it]\u001b[A\n",
            " 59%|█████▉    | 320/540 [57:59<41:52, 11.42s/it]\u001b[A\n",
            " 59%|█████▉    | 321/540 [58:10<41:36, 11.40s/it]\u001b[A\n",
            " 60%|█████▉    | 322/540 [58:21<40:49, 11.24s/it]\u001b[A\n",
            " 60%|█████▉    | 323/540 [58:31<39:05, 10.81s/it]\u001b[A\n",
            " 60%|██████    | 324/540 [58:43<40:20, 11.21s/it]\u001b[A\n",
            " 60%|██████    | 325/540 [58:55<41:00, 11.44s/it]\u001b[A\n",
            " 60%|██████    | 326/540 [59:07<41:28, 11.63s/it]\u001b[A\n",
            " 61%|██████    | 327/540 [59:18<41:20, 11.65s/it]\u001b[A\n",
            " 61%|██████    | 328/540 [59:29<39:53, 11.29s/it]\u001b[A\n",
            " 61%|██████    | 329/540 [59:41<40:36, 11.55s/it]\u001b[A\n",
            " 61%|██████    | 330/540 [59:54<41:33, 11.87s/it]\u001b[A\n",
            " 61%|██████▏   | 331/540 [1:00:06<41:30, 11.92s/it]\u001b[A\n",
            " 61%|██████▏   | 332/540 [1:00:17<41:05, 11.85s/it]\u001b[A\n",
            " 62%|██████▏   | 333/540 [1:00:28<39:22, 11.41s/it]\u001b[A\n",
            " 62%|██████▏   | 334/540 [1:00:39<39:04, 11.38s/it]\u001b[A\n",
            " 62%|██████▏   | 335/540 [1:00:53<41:52, 12.26s/it]\u001b[A\n",
            " 62%|██████▏   | 336/540 [1:01:05<41:11, 12.12s/it]\u001b[A\n",
            " 62%|██████▏   | 337/540 [1:01:17<40:25, 11.95s/it]\u001b[A\n",
            " 63%|██████▎   | 338/540 [1:01:28<39:55, 11.86s/it]\u001b[A\n",
            " 63%|██████▎   | 339/540 [1:01:38<37:47, 11.28s/it]\u001b[A\n",
            " 63%|██████▎   | 340/540 [1:01:50<38:05, 11.43s/it]\u001b[A\n",
            " 63%|██████▎   | 341/540 [1:02:02<38:40, 11.66s/it]\u001b[A\n",
            " 63%|██████▎   | 342/540 [1:02:14<38:30, 11.67s/it]\u001b[A\n",
            " 64%|██████▎   | 343/540 [1:02:25<38:08, 11.62s/it]\u001b[A\n",
            " 64%|██████▎   | 344/540 [1:02:36<36:35, 11.20s/it]\u001b[A\n",
            " 64%|██████▍   | 345/540 [1:02:48<37:08, 11.43s/it]\u001b[A\n",
            " 64%|██████▍   | 346/540 [1:03:00<37:47, 11.69s/it]\u001b[A\n",
            " 64%|██████▍   | 347/540 [1:03:12<37:28, 11.65s/it]\u001b[A\n",
            " 64%|██████▍   | 348/540 [1:03:23<37:17, 11.65s/it]\u001b[A\n",
            " 65%|██████▍   | 349/540 [1:03:37<38:42, 12.16s/it]\u001b[A\n",
            " 65%|██████▍   | 350/540 [1:03:48<37:37, 11.88s/it]\u001b[A\n",
            " 65%|██████▌   | 351/540 [1:03:59<36:43, 11.66s/it]\u001b[A\n",
            " 65%|██████▌   | 352/540 [1:04:11<36:38, 11.70s/it]\u001b[A\n",
            " 65%|██████▌   | 353/540 [1:04:23<36:38, 11.76s/it]\u001b[A\n",
            " 66%|██████▌   | 354/540 [1:04:34<35:58, 11.60s/it]\u001b[A\n",
            " 66%|██████▌   | 355/540 [1:04:45<35:29, 11.51s/it]\u001b[A\n",
            " 66%|██████▌   | 356/540 [1:04:56<34:27, 11.24s/it]\u001b[A\n",
            " 66%|██████▌   | 357/540 [1:05:08<34:58, 11.47s/it]\u001b[A\n",
            " 66%|██████▋   | 358/540 [1:05:20<35:31, 11.71s/it]\u001b[A\n",
            " 66%|██████▋   | 359/540 [1:05:32<35:21, 11.72s/it]\u001b[A\n",
            " 67%|██████▋   | 360/540 [1:05:43<34:55, 11.64s/it]\u001b[A\n",
            " 67%|██████▋   | 361/540 [1:05:54<34:15, 11.49s/it]\u001b[A\n",
            " 67%|██████▋   | 362/540 [1:06:06<34:32, 11.64s/it]\u001b[A\n",
            " 67%|██████▋   | 363/540 [1:06:20<36:04, 12.23s/it]\u001b[A\n",
            " 67%|██████▋   | 364/540 [1:06:32<36:06, 12.31s/it]\u001b[A\n",
            " 68%|██████▊   | 365/540 [1:06:45<35:48, 12.28s/it]\u001b[A\n",
            " 68%|██████▊   | 366/540 [1:06:57<35:38, 12.29s/it]\u001b[A\n",
            " 68%|██████▊   | 367/540 [1:07:08<34:19, 11.91s/it]\u001b[A\n",
            " 68%|██████▊   | 368/540 [1:07:19<33:39, 11.74s/it]\u001b[A\n",
            " 68%|██████▊   | 369/540 [1:07:32<33:58, 11.92s/it]\u001b[A\n",
            " 69%|██████▊   | 370/540 [1:07:45<35:04, 12.38s/it]\u001b[A\n",
            " 69%|██████▊   | 371/540 [1:07:59<35:48, 12.71s/it]\u001b[A\n",
            " 69%|██████▉   | 372/540 [1:08:11<35:06, 12.54s/it]\u001b[A\n",
            " 69%|██████▉   | 373/540 [1:08:23<34:23, 12.35s/it]\u001b[A\n",
            " 69%|██████▉   | 374/540 [1:08:34<33:29, 12.10s/it]\u001b[A\n",
            " 69%|██████▉   | 375/540 [1:08:45<32:20, 11.76s/it]\u001b[A\n",
            " 70%|██████▉   | 376/540 [1:08:58<33:09, 12.13s/it]\u001b[A\n",
            " 70%|██████▉   | 377/540 [1:09:12<34:44, 12.79s/it]\u001b[A\n",
            " 70%|███████   | 378/540 [1:09:24<33:53, 12.56s/it]\u001b[A\n",
            " 70%|███████   | 379/540 [1:09:36<33:14, 12.39s/it]\u001b[A\n",
            " 70%|███████   | 380/540 [1:09:48<32:38, 12.24s/it]\u001b[A\n",
            " 71%|███████   | 381/540 [1:09:59<31:27, 11.87s/it]\u001b[A\n",
            " 71%|███████   | 382/540 [1:10:11<31:13, 11.86s/it]\u001b[A\n",
            " 71%|███████   | 383/540 [1:10:23<31:08, 11.90s/it]\u001b[A\n",
            " 71%|███████   | 384/540 [1:10:35<30:44, 11.82s/it]\u001b[A\n",
            " 71%|███████▏  | 385/540 [1:10:46<30:21, 11.75s/it]\u001b[A\n",
            " 71%|███████▏  | 386/540 [1:10:58<30:07, 11.74s/it]\u001b[A\n",
            " 72%|███████▏  | 387/540 [1:11:09<29:06, 11.42s/it]\u001b[A\n",
            " 72%|███████▏  | 388/540 [1:11:21<29:18, 11.57s/it]\u001b[A\n",
            " 72%|███████▏  | 389/540 [1:11:33<29:36, 11.77s/it]\u001b[A\n",
            " 72%|███████▏  | 390/540 [1:11:45<29:30, 11.80s/it]\u001b[A\n",
            " 72%|███████▏  | 391/540 [1:11:59<31:24, 12.64s/it]\u001b[A\n",
            " 73%|███████▎  | 392/540 [1:12:12<31:08, 12.63s/it]\u001b[A\n",
            " 73%|███████▎  | 393/540 [1:12:24<30:26, 12.42s/it]\u001b[A\n",
            " 73%|███████▎  | 394/540 [1:12:35<29:28, 12.12s/it]\u001b[A\n",
            " 73%|███████▎  | 395/540 [1:12:49<30:09, 12.48s/it]\u001b[A\n",
            " 73%|███████▎  | 396/540 [1:13:02<30:45, 12.82s/it]\u001b[A\n",
            " 74%|███████▎  | 397/540 [1:13:16<30:58, 13.00s/it]\u001b[A\n",
            " 74%|███████▎  | 398/540 [1:13:28<30:24, 12.85s/it]\u001b[A\n",
            " 74%|███████▍  | 399/540 [1:13:41<29:57, 12.75s/it]\u001b[A\n",
            " 74%|███████▍  | 400/540 [1:13:53<29:33, 12.67s/it]\u001b[A\n",
            " 74%|███████▍  | 401/540 [1:14:07<29:46, 12.85s/it]\u001b[A\n",
            " 74%|███████▍  | 402/540 [1:14:18<28:32, 12.41s/it]\u001b[A\n",
            " 75%|███████▍  | 403/540 [1:14:30<28:02, 12.28s/it]\u001b[A\n",
            " 75%|███████▍  | 404/540 [1:14:45<29:30, 13.02s/it]\u001b[A\n",
            " 75%|███████▌  | 405/540 [1:14:57<29:01, 12.90s/it]\u001b[A\n",
            " 75%|███████▌  | 406/540 [1:15:11<29:16, 13.11s/it]\u001b[A\n",
            " 75%|███████▌  | 407/540 [1:15:23<28:31, 12.87s/it]\u001b[A\n",
            " 76%|███████▌  | 408/540 [1:15:35<27:49, 12.65s/it]\u001b[A\n",
            " 76%|███████▌  | 409/540 [1:15:48<27:37, 12.66s/it]\u001b[A\n",
            " 76%|███████▌  | 410/540 [1:16:00<27:05, 12.50s/it]\u001b[A\n",
            " 76%|███████▌  | 411/540 [1:16:12<26:41, 12.41s/it]\u001b[A\n",
            " 76%|███████▋  | 412/540 [1:16:25<26:21, 12.35s/it]\u001b[A\n",
            " 76%|███████▋  | 413/540 [1:16:37<26:07, 12.35s/it]\u001b[A\n",
            " 77%|███████▋  | 414/540 [1:16:49<25:55, 12.35s/it]\u001b[A\n",
            " 77%|███████▋  | 415/540 [1:17:01<25:27, 12.22s/it]\u001b[A\n",
            " 77%|███████▋  | 416/540 [1:17:14<25:38, 12.40s/it]\u001b[A\n",
            " 77%|███████▋  | 417/540 [1:17:28<26:26, 12.90s/it]\u001b[A\n",
            " 77%|███████▋  | 418/540 [1:17:40<25:38, 12.61s/it]\u001b[A\n",
            " 78%|███████▊  | 419/540 [1:17:51<24:34, 12.19s/it]\u001b[A\n",
            " 78%|███████▊  | 420/540 [1:18:03<24:17, 12.14s/it]\u001b[A\n",
            " 78%|███████▊  | 421/540 [1:18:17<24:58, 12.59s/it]\u001b[A\n",
            " 78%|███████▊  | 422/540 [1:18:30<25:11, 12.81s/it]\u001b[A\n",
            " 78%|███████▊  | 423/540 [1:18:43<24:50, 12.74s/it]\u001b[A\n",
            " 79%|███████▊  | 424/540 [1:18:55<24:31, 12.68s/it]\u001b[A\n",
            " 79%|███████▊  | 425/540 [1:19:08<24:23, 12.73s/it]\u001b[A\n",
            " 79%|███████▉  | 426/540 [1:19:22<24:37, 12.96s/it]\u001b[A\n",
            " 79%|███████▉  | 427/540 [1:19:34<24:12, 12.85s/it]\u001b[A\n",
            " 79%|███████▉  | 428/540 [1:19:47<24:04, 12.90s/it]\u001b[A\n",
            " 79%|███████▉  | 429/540 [1:20:00<23:42, 12.82s/it]\u001b[A\n",
            " 80%|███████▉  | 430/540 [1:20:15<24:41, 13.47s/it]\u001b[A\n",
            " 80%|███████▉  | 431/540 [1:20:29<24:55, 13.72s/it]\u001b[A\n",
            " 80%|████████  | 432/540 [1:20:43<24:32, 13.63s/it]\u001b[A\n",
            " 80%|████████  | 433/540 [1:20:56<24:04, 13.50s/it]\u001b[A\n",
            " 80%|████████  | 434/540 [1:21:09<23:42, 13.42s/it]\u001b[A\n",
            " 81%|████████  | 435/540 [1:21:22<23:17, 13.31s/it]\u001b[A\n",
            " 81%|████████  | 436/540 [1:21:37<23:40, 13.66s/it]\u001b[A\n",
            " 81%|████████  | 437/540 [1:21:50<23:19, 13.59s/it]\u001b[A\n",
            " 81%|████████  | 438/540 [1:22:02<22:22, 13.16s/it]\u001b[A\n",
            " 81%|████████▏ | 439/540 [1:22:15<21:51, 12.98s/it]\u001b[A\n",
            " 81%|████████▏ | 440/540 [1:22:28<21:53, 13.14s/it]\u001b[A\n",
            " 82%|████████▏ | 441/540 [1:22:43<22:18, 13.53s/it]\u001b[A\n",
            " 82%|████████▏ | 442/540 [1:22:59<23:25, 14.34s/it]\u001b[A\n",
            " 82%|████████▏ | 443/540 [1:23:11<22:22, 13.84s/it]\u001b[A\n",
            " 82%|████████▏ | 444/540 [1:23:24<21:38, 13.53s/it]\u001b[A\n",
            " 82%|████████▏ | 445/540 [1:23:37<20:59, 13.26s/it]\u001b[A\n",
            " 83%|████████▎ | 446/540 [1:23:51<20:55, 13.36s/it]\u001b[A\n",
            " 83%|████████▎ | 447/540 [1:24:03<20:19, 13.11s/it]\u001b[A\n",
            " 83%|████████▎ | 448/540 [1:24:15<19:39, 12.82s/it]\u001b[A\n",
            " 83%|████████▎ | 449/540 [1:24:27<18:57, 12.50s/it]\u001b[A\n",
            " 83%|████████▎ | 450/540 [1:24:40<18:59, 12.66s/it]\u001b[A\n",
            " 84%|████████▎ | 451/540 [1:24:54<19:17, 13.00s/it]\u001b[A\n",
            " 84%|████████▎ | 452/540 [1:25:07<19:05, 13.02s/it]\u001b[A\n",
            " 84%|████████▍ | 453/540 [1:25:20<18:54, 13.05s/it]\u001b[A\n",
            " 84%|████████▍ | 454/540 [1:25:33<18:36, 12.98s/it]\u001b[A\n",
            " 84%|████████▍ | 455/540 [1:25:46<18:17, 12.92s/it]\u001b[A\n",
            " 84%|████████▍ | 456/540 [1:26:01<19:04, 13.63s/it]\u001b[A\n",
            " 85%|████████▍ | 457/540 [1:26:14<18:51, 13.63s/it]\u001b[A\n",
            " 85%|████████▍ | 458/540 [1:26:28<18:24, 13.47s/it]\u001b[A\n",
            " 85%|████████▌ | 459/540 [1:26:40<17:36, 13.04s/it]\u001b[A\n",
            " 85%|████████▌ | 460/540 [1:26:52<16:56, 12.71s/it]\u001b[A\n",
            " 85%|████████▌ | 461/540 [1:27:06<17:22, 13.19s/it]\u001b[A\n",
            " 86%|████████▌ | 462/540 [1:27:20<17:34, 13.52s/it]\u001b[A\n",
            " 86%|████████▌ | 463/540 [1:27:33<17:12, 13.40s/it]\u001b[A\n",
            " 86%|████████▌ | 464/540 [1:27:47<16:55, 13.37s/it]\u001b[A\n",
            " 86%|████████▌ | 465/540 [1:27:59<16:27, 13.16s/it]\u001b[A\n",
            " 86%|████████▋ | 466/540 [1:28:12<16:15, 13.19s/it]\u001b[A\n",
            " 86%|████████▋ | 467/540 [1:28:25<15:50, 13.02s/it]\u001b[A\n",
            " 87%|████████▋ | 468/540 [1:28:40<16:10, 13.47s/it]\u001b[A\n",
            " 87%|████████▋ | 469/540 [1:28:52<15:33, 13.14s/it]\u001b[A\n",
            " 87%|████████▋ | 470/540 [1:29:05<15:09, 13.00s/it]\u001b[A\n",
            " 87%|████████▋ | 471/540 [1:29:18<14:58, 13.03s/it]\u001b[A\n",
            " 87%|████████▋ | 472/540 [1:29:30<14:27, 12.76s/it]\u001b[A\n",
            " 88%|████████▊ | 473/540 [1:29:43<14:23, 12.89s/it]\u001b[A\n",
            " 88%|████████▊ | 474/540 [1:29:56<14:09, 12.87s/it]\u001b[A\n",
            " 88%|████████▊ | 475/540 [1:30:09<13:52, 12.81s/it]\u001b[A\n",
            " 88%|████████▊ | 476/540 [1:30:22<13:47, 12.94s/it]\u001b[A\n",
            " 88%|████████▊ | 477/540 [1:30:34<13:23, 12.76s/it]\u001b[A\n",
            " 89%|████████▊ | 478/540 [1:30:47<13:13, 12.80s/it]\u001b[A\n",
            " 89%|████████▊ | 479/540 [1:31:00<12:54, 12.70s/it]\u001b[A\n",
            " 89%|████████▉ | 480/540 [1:31:12<12:39, 12.66s/it]\u001b[A\n",
            " 89%|████████▉ | 481/540 [1:31:28<13:27, 13.68s/it]\u001b[A\n",
            " 89%|████████▉ | 482/540 [1:31:41<13:04, 13.53s/it]\u001b[A\n",
            " 89%|████████▉ | 483/540 [1:31:55<12:51, 13.53s/it]\u001b[A\n",
            " 90%|████████▉ | 484/540 [1:32:08<12:32, 13.43s/it]\u001b[A\n",
            " 90%|████████▉ | 485/540 [1:32:21<12:04, 13.18s/it]\u001b[A\n",
            " 90%|█████████ | 486/540 [1:32:35<12:15, 13.61s/it]\u001b[A\n",
            " 90%|█████████ | 487/540 [1:32:49<12:02, 13.64s/it]\u001b[A\n",
            " 90%|█████████ | 488/540 [1:33:02<11:39, 13.44s/it]\u001b[A\n",
            " 91%|█████████ | 489/540 [1:33:14<11:07, 13.09s/it]\u001b[A\n",
            " 91%|█████████ | 490/540 [1:33:27<10:53, 13.07s/it]\u001b[A\n",
            " 91%|█████████ | 491/540 [1:33:42<11:11, 13.71s/it]\u001b[A\n",
            " 91%|█████████ | 492/540 [1:33:58<11:29, 14.36s/it]\u001b[A\n",
            " 91%|█████████▏| 493/540 [1:34:12<11:09, 14.24s/it]\u001b[A\n",
            " 91%|█████████▏| 494/540 [1:34:26<10:46, 14.05s/it]\u001b[A\n",
            " 92%|█████████▏| 495/540 [1:34:39<10:25, 13.91s/it]\u001b[A\n",
            " 92%|█████████▏| 496/540 [1:34:55<10:30, 14.34s/it]\u001b[A\n",
            " 92%|█████████▏| 497/540 [1:35:08<10:02, 14.02s/it]\u001b[A\n",
            " 92%|█████████▏| 498/540 [1:35:22<09:43, 13.90s/it]\u001b[A\n",
            " 92%|█████████▏| 499/540 [1:35:35<09:18, 13.63s/it]\u001b[A\n",
            " 93%|█████████▎| 500/540 [1:35:48<09:03, 13.59s/it]\u001b[A\n",
            " 93%|█████████▎| 501/540 [1:36:02<08:51, 13.63s/it]\u001b[A\n",
            " 93%|█████████▎| 502/540 [1:36:15<08:35, 13.57s/it]\u001b[A\n",
            " 93%|█████████▎| 503/540 [1:36:29<08:19, 13.50s/it]\u001b[A\n",
            " 93%|█████████▎| 504/540 [1:36:43<08:14, 13.75s/it]\u001b[A\n",
            " 94%|█████████▎| 505/540 [1:36:56<07:51, 13.48s/it]\u001b[A\n",
            " 94%|█████████▎| 506/540 [1:37:11<07:50, 13.83s/it]\u001b[A\n",
            " 94%|█████████▍| 507/540 [1:37:25<07:41, 13.98s/it]\u001b[A\n",
            " 94%|█████████▍| 508/540 [1:37:38<07:21, 13.81s/it]\u001b[A\n",
            " 94%|█████████▍| 509/540 [1:37:52<07:03, 13.66s/it]\u001b[A\n",
            " 94%|█████████▍| 510/540 [1:38:05<06:46, 13.55s/it]\u001b[A\n",
            " 95%|█████████▍| 511/540 [1:38:20<06:45, 13.98s/it]\u001b[A\n",
            " 95%|█████████▍| 512/540 [1:38:34<06:30, 13.95s/it]\u001b[A\n",
            " 95%|█████████▌| 513/540 [1:38:48<06:16, 13.94s/it]\u001b[A\n",
            " 95%|█████████▌| 514/540 [1:39:02<06:03, 13.98s/it]\u001b[A\n",
            " 95%|█████████▌| 515/540 [1:39:18<06:03, 14.53s/it]\u001b[A\n",
            " 96%|█████████▌| 516/540 [1:39:33<05:53, 14.74s/it]\u001b[A\n",
            " 96%|█████████▌| 517/540 [1:39:47<05:33, 14.51s/it]\u001b[A\n",
            " 96%|█████████▌| 518/540 [1:40:00<05:14, 14.28s/it]\u001b[A\n",
            " 96%|█████████▌| 519/540 [1:40:14<04:57, 14.16s/it]\u001b[A\n",
            " 96%|█████████▋| 520/540 [1:40:28<04:41, 14.06s/it]\u001b[A\n",
            " 96%|█████████▋| 521/540 [1:40:44<04:38, 14.64s/it]\u001b[A\n",
            " 97%|█████████▋| 522/540 [1:40:59<04:26, 14.83s/it]\u001b[A\n",
            " 97%|█████████▋| 523/540 [1:41:14<04:09, 14.69s/it]\u001b[A\n",
            " 97%|█████████▋| 524/540 [1:41:28<03:53, 14.58s/it]\u001b[A\n",
            " 97%|█████████▋| 525/540 [1:41:43<03:39, 14.67s/it]\u001b[A\n",
            " 97%|█████████▋| 526/540 [1:42:01<03:38, 15.62s/it]\u001b[A\n",
            " 98%|█████████▊| 527/540 [1:42:15<03:17, 15.16s/it]\u001b[A\n",
            " 98%|█████████▊| 528/540 [1:42:29<02:59, 14.98s/it]\u001b[A\n",
            " 98%|█████████▊| 529/540 [1:42:44<02:41, 14.71s/it]\u001b[A\n",
            " 98%|█████████▊| 530/540 [1:42:58<02:25, 14.55s/it]\u001b[A\n",
            " 98%|█████████▊| 531/540 [1:43:13<02:11, 14.67s/it]\u001b[A\n",
            " 99%|█████████▊| 532/540 [1:43:26<01:54, 14.36s/it]\u001b[A\n",
            " 99%|█████████▊| 533/540 [1:43:40<01:38, 14.05s/it]\u001b[A\n",
            " 99%|█████████▉| 534/540 [1:43:53<01:23, 13.90s/it]\u001b[A\n",
            " 99%|█████████▉| 535/540 [1:44:07<01:08, 13.74s/it]\u001b[A\n",
            " 99%|█████████▉| 536/540 [1:44:23<00:58, 14.56s/it]\u001b[A\n",
            " 99%|█████████▉| 537/540 [1:44:38<00:44, 14.78s/it]\u001b[A\n",
            "100%|█████████▉| 538/540 [1:44:53<00:29, 14.66s/it]\u001b[A\n",
            "100%|█████████▉| 539/540 [1:45:08<00:14, 14.83s/it]\u001b[A\n",
            "100%|██████████| 540/540 [1:45:24<00:00, 11.71s/it]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "os.makedirs('./results', exist_ok=True)\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 2\n",
        "max_topics = 11\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),\n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "\n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word,\n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nowyy-Md7aH"
      },
      "source": [
        "** **\n",
        "#### Step 7: Final Model\n",
        "** **\n",
        "\n",
        "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iE9BE-RJd7aH"
      },
      "outputs": [],
      "source": [
        "num_topics = 8\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=num_topics,\n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.01,\n",
        "                                           eta=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYvqV39td7aI",
        "outputId": "e4e060f1-f581-473a-a0f2-1a19d0773fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.013*\"model\" + 0.007*\"image\" + 0.006*\"use\" + 0.006*\"learn\" + '\n",
            "  '0.005*\"training\" + 0.005*\"distribution\" + 0.004*\"parameter\" + 0.004*\"show\" '\n",
            "  '+ 0.004*\"object\" + 0.004*\"edge\"'),\n",
            " (1,\n",
            "  '0.005*\"embed\" + 0.002*\"row\" + 0.002*\"polynomial\" + 0.002*\"compressive\" + '\n",
            "  '0.001*\"eigenvector\" + 0.001*\"embedding\" + 0.001*\"percentile_percentile\" + '\n",
            "  '0.001*\"spectral\" + 0.001*\"singular_vector\" + 0.001*\"fel\"'),\n",
            " (2,\n",
            "  '0.016*\"feature\" + 0.007*\"learn\" + 0.006*\"function\" + 0.006*\"use\" + '\n",
            "  '0.005*\"set\" + 0.005*\"result\" + 0.005*\"method\" + 0.004*\"lattice\" + '\n",
            "  '0.004*\"random\" + 0.004*\"show\"'),\n",
            " (3,\n",
            "  '0.011*\"network\" + 0.006*\"chip\" + 0.006*\"input\" + 0.006*\"function\" + '\n",
            "  '0.005*\"use\" + 0.005*\"figure\" + 0.005*\"model\" + 0.004*\"output\" + '\n",
            "  '0.004*\"neural\" + 0.004*\"system\"'),\n",
            " (4,\n",
            "  '0.006*\"training\" + 0.005*\"cascade\" + 0.005*\"image\" + 0.004*\"use\" + '\n",
            "  '0.004*\"model\" + 0.004*\"feature\" + 0.004*\"learn\" + 0.004*\"layer\" + '\n",
            "  '0.004*\"input\" + 0.003*\"dataset\"'),\n",
            " (5,\n",
            "  '0.008*\"object\" + 0.006*\"use\" + 0.006*\"learn\" + 0.006*\"point\" + '\n",
            "  '0.006*\"function\" + 0.006*\"set\" + 0.006*\"feature\" + 0.005*\"space\" + '\n",
            "  '0.004*\"model\" + 0.004*\"cue\"'),\n",
            " (6,\n",
            "  '0.009*\"neuron\" + 0.009*\"input\" + 0.007*\"output\" + 0.007*\"learn\" + '\n",
            "  '0.006*\"unit\" + 0.006*\"rule\" + 0.006*\"layer\" + 0.006*\"spike\" + 0.005*\"pool\" '\n",
            "  '+ 0.005*\"response\"'),\n",
            " (7,\n",
            "  '0.012*\"model\" + 0.009*\"use\" + 0.008*\"set\" + 0.008*\"datum\" + 0.007*\"learn\" + '\n",
            "  '0.006*\"distribution\" + 0.006*\"problem\" + 0.006*\"time\" + 0.005*\"sample\" + '\n",
            "  '0.005*\"matrix\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVRsYH_Ud7aI"
      },
      "source": [
        "** **\n",
        "#### Step 8: Visualize Results\n",
        "** **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o_l_n7B4d7aJ",
        "outputId": "647af4df-df54-4c69-ea6d-58d6d083f61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "7      0.133791  0.003815       1        1  51.334954\n",
              "0      0.039898 -0.001082       2        1  11.583583\n",
              "5      0.031692  0.004926       3        1  10.506923\n",
              "3     -0.012553 -0.061529       4        1   8.581922\n",
              "2      0.024301  0.055022       5        1   7.870014\n",
              "6     -0.034307 -0.058808       6        1   6.264402\n",
              "4     -0.052042  0.035430       7        1   3.587436\n",
              "1     -0.130780  0.022224       8        1   0.270764, topic_info=               Term         Freq        Total Category  logprob  loglift\n",
              "513         feature   733.000000   733.000000  Default  30.0000  30.0000\n",
              "143           input   493.000000   493.000000  Default  29.0000  29.0000\n",
              "189         network   495.000000   495.000000  Default  28.0000  28.0000\n",
              "156           learn  1116.000000  1116.000000  Default  27.0000  27.0000\n",
              "1837         neuron   191.000000   191.000000  Default  26.0000  26.0000\n",
              "...             ...          ...          ...      ...      ...      ...\n",
              "379         capture     0.253683    69.401286   Topic8  -7.5862   0.3001\n",
              "888       symmetric     0.222653    34.426288   Topic8  -7.7167   0.8707\n",
              "350   approximation     0.259179   253.347362   Topic8  -7.5648  -0.9733\n",
              "412         compute     0.243965   335.966480   Topic8  -7.6253  -1.3161\n",
              "300             use     0.244559  1293.935119   Topic8  -7.6229  -2.6621\n",
              "\n",
              "[561 rows x 6 columns], token_table=      Topic      Freq    Term\n",
              "term                         \n",
              "4183      1  0.051446  accent\n",
              "4183      2  0.154339  accent\n",
              "4183      3  0.051446  accent\n",
              "4183      4  0.720251  accent\n",
              "4183      5  0.051446  accent\n",
              "...     ...       ...     ...\n",
              "6068      3  0.105097    zorn\n",
              "6068      4  0.105097    zorn\n",
              "6068      5  0.105097    zorn\n",
              "6068      6  0.105097    zorn\n",
              "6068      7  0.525486    zorn\n",
              "\n",
              "[2748 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 1, 6, 4, 3, 7, 5, 2])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el4641333902402475684647474299\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el4641333902402475684647474299_data = {\"mdsDat\": {\"x\": [0.13379095406481198, 0.03989837707853002, 0.03169220500155209, -0.012553325944735398, 0.02430103095035403, -0.03430671162331947, -0.05204215254011498, -0.13078037698707798], \"y\": [0.0038152459440364706, -0.0010815179094130496, 0.004926344062522079, -0.06152860240349011, 0.055021518858287384, -0.05880770950371323, 0.03543028185558862, 0.02222443909618186], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [51.33495422653625, 11.583583488100196, 10.506923389344525, 8.581922361291058, 7.870014196903329, 6.264402386470964, 3.5874363704336303, 0.2707635809200654]}, \"tinfo\": {\"Term\": [\"feature\", \"input\", \"network\", \"learn\", \"neuron\", \"model\", \"image\", \"embed\", \"function\", \"object\", \"output\", \"training\", \"use\", \"layer\", \"rule\", \"row\", \"unit\", \"dataset\", \"chip\", \"figure\", \"set\", \"point\", \"cascade\", \"distribution\", \"system\", \"neural\", \"spike\", \"result\", \"space\", \"classification\", \"topic\", \"document\", \"coreset\", \"change_point\", \"tensor\", \"intervention\", \"prototype\", \"legislation\", \"posterior\", \"vote\", \"text\", \"variational\", \"bcm\", \"tempered_transition\", \"coalescent\", \"word\", \"cluster\", \"clustering\", \"sparse_representation\", \"sentence\", \"arm\", \"factorisation\", \"latent\", \"particle\", \"influence_estimation\", \"glm\", \"virtual_point\", \"stick_breake\", \"language\", \"message\", \"matrix\", \"sparse\", \"graph\", \"metric\", \"log\", \"observation\", \"datum\", \"distribution\", \"estimate\", \"gaussian\", \"problem\", \"mixture\", \"likelihood\", \"draw\", \"case\", \"sample\", \"probability\", \"model\", \"give\", \"set\", \"parameter\", \"vector\", \"time\", \"method\", \"use\", \"consider\", \"example\", \"assume\", \"define\", \"number\", \"result\", \"learn\", \"approach\", \"show\", \"class\", \"follow\", \"function\", \"large\", \"point\", \"also\", \"base\", \"first\", \"copula\", \"repulsive\", \"doublet\", \"rhythm\", \"singularity\", \"combo\", \"margin_maximize\", \"trace_norm\", \"eeg\", \"crf\", \"segmentation\", \"weighted_trace\", \"complex_cell\", \"pretraine\", \"co_occurrence\", \"deepmask\", \"triple\", \"percept\", \"east_african\", \"multi_category\", \"labeling\", \"coco\", \"segmentation_proposal\", \"hop_size\", \"mixture_doublet\", \"transparency\", \"texture\", \"triplet\", \"decorrelation\", \"mnist_movie\", \"bar\", \"movie\", \"subject\", \"edge\", \"spike_count\", \"pixel\", \"proposal\", \"image\", \"logistic_regression\", \"filter\", \"scene\", \"margin\", \"generalization\", \"model\", \"orientation\", \"object\", \"training\", \"shape\", \"pairwise\", \"visual\", \"distribution\", \"parameter\", \"test\", \"learn\", \"use\", \"error\", \"show\", \"layer\", \"figure\", \"number\", \"result\", \"class\", \"function\", \"sample\", \"set\", \"datum\", \"base\", \"feature\", \"approach\", \"also\", \"experiment\", \"probability\", \"large\", \"affine\", \"cue\", \"validity\", \"symnet\", \"rigid\", \"private\", \"convex_upper\", \"privacy\", \"ipsilateral\", \"structured_prediction\", \"tally\", \"differentially_private\", \"agent\", \"whole_image\", \"cem\", \"affine_transformation\", \"iearne\", \"symmetry\", \"ear\", \"resnet\", \"owl\", \"frugal\", \"belief\", \"minimalist\", \"ttl\", \"frugality\", \"baep\", \"differential_privacy\", \"front_rule\", \"contralateral\", \"alignment\", \"chunk\", \"robot\", \"localization\", \"wave\", \"action\", \"search\", \"object\", \"correspondence\", \"policy\", \"decision\", \"heuristic\", \"window\", \"translation\", \"rule\", \"point\", \"loss\", \"space\", \"function\", \"feature\", \"map\", \"view\", \"learn\", \"use\", \"set\", \"network\", \"order\", \"state\", \"result\", \"image\", \"show\", \"model\", \"figure\", \"follow\", \"algorithm\", \"number\", \"value\", \"datum\", \"time\", \"chip\", \"synapse\", \"neurotropin\", \"tv_denoise\", \"synapsis\", \"transistor\", \"capacitor\", \"lgmd\", \"tv\", \"laplacian_smoothe\", \"charge\", \"distributed_neuron\", \"axon_terminal\", \"voltage\", \"programmable\", \"clbt\", \"circuit\", \"collective_excitation\", \"axon\", \"lateral\", \"uran\", \"sigmoidal\", \"growth_cone\", \"active_noise\", \"afferent\", \"accent\", \"drain\", \"modularity\", \"current_source\", \"linear_smoother\", \"presynaptic\", \"analog\", \"modular\", \"software\", \"hardware\", \"network\", \"synaptic\", \"cell\", \"module\", \"current\", \"development\", \"unit\", \"input\", \"pattern\", \"output\", \"figure\", \"neural\", \"function\", \"weight\", \"system\", \"neural_network\", \"signal\", \"connection\", \"value\", \"use\", \"scale\", \"model\", \"show\", \"neuron\", \"time\", \"layer\", \"result\", \"order\", \"number\", \"error\", \"learn\", \"size\", \"also\", \"dualsgd\", \"lattice\", \"descriptor\", \"greedy_coordinate\", \"sift\", \"bgm\", \"fogd\", \"channel_selection\", \"merging\", \"budget_maintenance\", \"cochlear_implant\", \"asynchronous\", \"infomax\", \"monotonic\", \"feature_imputation\", \"monotonicity\", \"random_forest\", \"rff\", \"crystal\", \"calibrator\", \"ensemble\", \"marginal_diversity\", \"coordinate_descent\", \"multi_core\", \"separate_calibrator\", \"nogd\", \"phoneme\", \"mistake_rate\", \"airline\", \"mc\", \"weak_learner\", \"descent\", \"feature\", \"compact\", \"rd\", \"selection\", \"budget\", \"kernel\", \"boost\", \"speech\", \"convergence\", \"hypothesis\", \"function\", \"random\", \"label\", \"learn\", \"method\", \"propose\", \"result\", \"dataset\", \"gradient\", \"use\", \"set\", \"space\", \"base\", \"select\", \"rate\", \"linear\", \"error\", \"show\", \"subset\", \"machine\", \"follow\", \"approach\", \"problem\", \"time\", \"model\", \"classification\", \"number\", \"large\", \"lds\", \"density_separator\", \"quasi_classe\", \"tier\", \"spike\", \"production\", \"spike_train\", \"latch\", \"activation_space\", \"oscillation\", \"ld\", \"production_system\", \"dynamic_route\", \"oscillation_re\", \"quasi_classes\", \"shelf\", \"ring\", \"visible_neuron\", \"auxiliary_action\", \"ws\", \"spike_time\", \"novel_categorie\", \"circular_shift\", \"projection_re\", \"weights_toward\", \"neuronal\", \"modulate\", \"msec_tone\", \"raster\", \"tonic\", \"neuron\", \"fire\", \"tone\", \"pool\", \"net\", \"unlabeled\", \"phase\", \"unit\", \"code\", \"response\", \"rule\", \"input\", \"output\", \"population\", \"reward\", \"layer\", \"variability\", \"activity\", \"activation\", \"trial\", \"learn\", \"system\", \"neural\", \"sequence\", \"figure\", \"feature\", \"binary\", \"time\", \"single\", \"learning\", \"low\", \"use\", \"model\", \"weight\", \"training\", \"show\", \"optical_flow\", \"ucf\", \"fcboost\", \"sect\", \"short_live\", \"displacement\", \"temporal_convnet\", \"chainboost\", \"zorn\", \"permanent\", \"request\", \"person\", \"allocator\", \"wider_search\", \"inexact_matche\", \"normalized_correlation\", \"fuse\", \"action_recognition\", \"lagrangian\", \"cuhk_dataset\", \"lifetime\", \"spatio_temporal\", \"stream_convnet\", \"hmdb\", \"partial_occlusion\", \"cuhk\", \"stack\", \"video_frame\", \"trajectory_stacke\", \"fk\", \"video\", \"cascade\", \"identification\", \"stage\", \"convnet\", \"allocation\", \"motion\", \"detector\", \"frame\", \"spatial\", \"deep\", \"architecture\", \"temporal\", \"training\", \"boost\", \"image\", \"layer\", \"computer_vision\", \"dataset\", \"train\", \"feature\", \"input\", \"use\", \"learn\", \"model\", \"page\", \"classification\", \"set\", \"task\", \"network\", \"base\", \"large\", \"performance\", \"compressive\", \"percentile_percentile\", \"fel\", \"ip_iq\", \"fel_dx\", \"sidestep\", \"percentile\", \"commute\", \"singular_vector\", \"adjacency\", \"pairwise_distance\", \"eigensolver\", \"ecol\", \"legendre\", \"spielman\", \"recursion\", \"fastembe\", \"weighing\", \"eig\", \"ksk\", \"madhow\", \"kfel\", \"maxl\", \"ksn\", \"sparsification\", \"vlt\", \"embedding\", \"downstream\", \"suppress\", \"collaboration\", \"embed\", \"polynomial\", \"row\", \"eigenvector\", \"normalized_correlation\", \"zero\", \"modularity\", \"spectral\", \"pairwise\", \"vol\", \"ser\", \"eigenvalue\", \"graph\", \"matrix\", \"approximate\", \"capture\", \"symmetric\", \"approximation\", \"compute\", \"use\"], \"Freq\": [733.0, 493.0, 495.0, 1116.0, 191.0, 1712.0, 453.0, 103.0, 825.0, 324.0, 346.0, 447.0, 1293.0, 289.0, 228.0, 100.0, 206.0, 294.0, 106.0, 544.0, 1090.0, 496.0, 91.0, 694.0, 384.0, 349.0, 92.0, 761.0, 462.0, 310.0, 203.6772987742501, 157.24453332081748, 104.88663412150395, 69.91238703754607, 72.41803097647464, 64.30034912186491, 101.05072603274684, 58.44392596499583, 188.3580321111093, 53.62483241357008, 74.44836736061016, 46.4419908693828, 47.84887231938147, 41.78916999119824, 38.944798242526424, 157.75966686384533, 281.9937969683238, 53.24468127233992, 31.95678567555775, 65.28919563523027, 64.82414528632923, 30.976899733665554, 93.91410536739617, 42.38660119666936, 28.081059874955784, 27.105055559930197, 34.58485121520106, 28.649948570432652, 45.3030768542151, 31.803829713677697, 498.6498434806268, 93.23942509732532, 126.39874815606616, 173.358090640972, 362.8944047952422, 180.28608006953502, 751.6803587918075, 564.84043729346, 271.4809058041176, 165.47239047787554, 546.0662744273427, 171.27567369394535, 157.87958623601136, 108.2408894232056, 349.4272617711559, 519.3933457372616, 351.5767174764331, 1166.5660297486486, 454.0867586480391, 753.7348456816931, 451.49578512916446, 291.8129280249258, 533.1099060475996, 435.3333860733693, 814.5885142969629, 289.54314464558047, 326.42752734425096, 242.3088604879483, 298.96300058267275, 432.34951577280697, 479.10463006035593, 635.3882965832886, 364.1289552545874, 453.59450104671555, 308.0795433824359, 325.3667590025896, 443.4661861335434, 324.4149520928569, 320.8296420553116, 318.73650994215143, 317.79156691844855, 298.94845620680866, 28.802264833129897, 30.56266101591878, 20.709349051251458, 29.731599235647828, 19.28411340710678, 16.68112274121572, 16.631325319738853, 19.530835806896402, 24.4679880054903, 17.041156467505623, 33.23521020221115, 14.305383467926832, 13.964352617952692, 22.789639228139972, 11.306792334757057, 11.293007374620249, 12.660217201194836, 10.570814657684096, 10.525593962495421, 10.986552222619313, 14.43509556031242, 8.624119195849762, 8.619319609385943, 8.54074645713216, 7.97357976046288, 11.903858400798217, 16.70386568438941, 13.436730529652749, 7.289856569428748, 7.281241591857699, 17.90706536205305, 20.732715753911105, 23.63024540006634, 80.21644101691336, 26.16216110033872, 38.41050655137659, 43.91631823630777, 153.03934256900484, 30.65912834624033, 52.11369816922296, 30.213671941125778, 49.68945256944111, 53.35279078545091, 273.30789064121683, 28.881266897844586, 84.07934242750488, 99.40682366725177, 29.494493439190638, 27.955814495480787, 37.37593732492409, 98.55305255958781, 92.57398546394828, 63.39437726753588, 117.9362901887188, 126.48648143519326, 64.68316645789866, 85.2013724451144, 55.63913698270811, 67.84915048353896, 71.30276108891242, 74.56113622020955, 59.8379920153621, 71.56298209218862, 64.56547636470754, 73.78598619841256, 69.76442328312503, 58.33637758242472, 62.14372082275386, 56.15694948819394, 53.250484269522275, 49.137008339680946, 51.581702294347814, 50.865516841922144, 40.13297345756387, 78.28214277031006, 25.05427945857643, 19.8775988183076, 20.36858090314237, 19.766682789280054, 17.11539601905638, 15.279149402388882, 13.420300832832268, 13.283320395256668, 11.527261360623969, 13.355686396112793, 28.861036016533536, 12.119943199002721, 11.893032646786425, 10.194235130153254, 9.587597314268573, 29.540716288244894, 12.06899676647116, 8.912314564519354, 13.264690215541883, 8.307026835307948, 45.15508808963707, 7.663512624417713, 7.66378196416809, 7.662433571455509, 7.6432850397682985, 8.856955929032974, 7.0213682630755185, 8.287826355492783, 12.53300196259727, 10.09111019101189, 27.435678764464885, 15.908879290456866, 31.398593653670165, 68.76449135171917, 74.05362839594014, 160.25175338649183, 21.76516340955225, 63.02876302730845, 62.83606257596504, 28.305825718017566, 36.224495689227325, 42.2889788707109, 74.16781941564314, 117.41680872542678, 68.19850520457385, 88.9335869262843, 114.9810649018542, 107.03712103708702, 42.89983464872413, 41.26125969338395, 117.99365880715756, 123.15583010942701, 112.08182632897804, 74.39655379887353, 62.38450686163416, 51.87789408639956, 73.18871916885392, 57.961408351628876, 64.79924557259868, 79.91207626749558, 58.35844111774918, 56.288768976035506, 49.403405993829296, 55.66546823345556, 49.161309648643076, 50.17277132053425, 48.87882024605016, 95.95544662585957, 28.32821626918918, 18.05836601553216, 21.150769045245973, 19.87386516589824, 19.21880350091811, 13.82994337394311, 13.778189141284493, 21.76802506789798, 15.797668668960876, 15.780421562625758, 12.624829260289886, 12.01744849276804, 26.969740531801023, 11.39716698563752, 11.378744736855566, 31.3040680437912, 10.72517828844241, 10.205272763309301, 14.76830568658737, 10.184523289728784, 23.98774370305393, 8.999336787866335, 8.998243739358093, 8.95021417326174, 13.766884160257472, 8.384977113160218, 11.35548079050419, 7.770125797226342, 8.717293376587634, 16.238963252265762, 16.5134042113884, 11.953522219506244, 17.474141273116572, 13.199310651319344, 166.84488717340776, 17.646473975897095, 37.797528864255156, 29.057610767152614, 47.55461299822531, 24.284150786375122, 54.53067548916386, 92.74128260434658, 38.98168504106806, 67.22541650100246, 78.74203385690906, 62.279015030933515, 89.77458845471487, 56.64352626434686, 59.14231079130083, 27.126495679132432, 35.26299565545693, 27.732989094194423, 54.53802520267751, 81.66638598230249, 43.5828049398346, 74.24865072249924, 56.865194657010804, 34.300622249379714, 47.891792174509646, 37.27403518832284, 43.423243840130134, 37.252583765447156, 38.74961193349022, 34.99373122648472, 38.29351645873955, 32.794964071342456, 31.304237766091216, 25.08332019698642, 59.94183508544783, 37.552776281993694, 15.19207497274758, 14.17618237657913, 12.487985225641646, 11.937100044339795, 12.313522856431042, 13.084795268998798, 9.654987473156357, 9.64016162298979, 10.67842791328192, 11.694851726082117, 25.650138493695213, 8.431715686108916, 7.942252850795824, 7.939927509202403, 7.922651126261684, 11.363029118012234, 7.3690066636656395, 32.51468052751909, 7.2628741484292, 15.172108671193882, 6.726020107062189, 6.226816890356502, 6.2265787733472076, 7.927035355246297, 6.225934630886899, 6.225883877705522, 7.2936644117978915, 11.953425651016051, 18.607050151332604, 229.379368656098, 19.212451016524163, 35.10617180408116, 34.94754104391474, 19.262055484229297, 49.89388264661676, 29.18973472560595, 18.68337786080542, 36.8767405225071, 20.75603289833248, 90.8248549684243, 56.49500044313977, 44.76681318164291, 98.51925381207936, 68.03472760800227, 44.712091101482265, 71.6105531525526, 44.17375549016223, 32.54478393116648, 82.68431535919316, 76.31904759652892, 52.28072356168521, 54.05057835202645, 31.959501627072747, 41.26907648195918, 39.63216119137134, 44.03107476312988, 54.42767787320272, 26.29352473160201, 38.279132129042836, 43.68514107886448, 44.86051708864649, 46.336730889267734, 46.79452796883338, 53.93688445865119, 36.529155668074246, 37.43547891274068, 36.54646927001464, 17.199348357395777, 14.415889976946964, 14.414267295789454, 11.610640537169676, 65.15041954801653, 14.969461257370186, 15.403416023952698, 7.734853126225602, 7.7253809774605235, 12.760903324858113, 7.723025733191243, 7.181359276645994, 7.167371226418919, 7.166155225518312, 7.169350762768743, 8.283529430708953, 15.600677494524376, 9.241496421492363, 6.625819906171568, 8.510986237043973, 6.48841045777968, 6.055903311316938, 5.500893837137516, 5.499780852924502, 6.709457336886697, 21.581674207727605, 7.148572364212424, 4.9645047975684315, 5.32451950495906, 4.9450773989052825, 106.36277733578589, 21.877628358260015, 17.883944744826863, 59.6463173807066, 50.53560232739263, 13.645089494662813, 27.129747667847617, 71.64753255930692, 42.79809837306864, 52.45253146362783, 65.92521276858452, 105.0844762517181, 81.87363958821497, 26.994129228534142, 33.906670162120726, 65.21536988626465, 14.090832602202033, 29.060248074377736, 23.232805477116084, 26.712326742967925, 81.7340109045139, 46.842805545886044, 43.8060163377243, 32.91979184095463, 43.83549392323291, 46.41687934781894, 26.582978182052262, 44.87837292374853, 29.57957068270226, 33.629459662236755, 27.238627553581424, 35.80081712422917, 36.77414572107423, 29.113682089589318, 28.779829917633027, 28.94131699183094, 16.67123549223021, 10.151479519769548, 8.462633470720585, 7.705188513231579, 7.4847068597096476, 9.74257147895729, 5.667822127683028, 5.62329895381317, 5.495403708647218, 5.488018203686551, 9.075990519327275, 20.414522810203167, 5.095890073330504, 4.795379374930407, 4.795565612734313, 5.592399672956533, 8.413651000365912, 5.260806540580017, 3.596570101461595, 3.5845197829364426, 6.595887518949851, 5.257050937660685, 3.2217435787640056, 3.220717921293223, 3.1821524292207575, 3.180156445800642, 12.071835739249968, 3.626309926673629, 2.8127480100397655, 2.7868260598077903, 20.325514249843952, 34.971088576014544, 15.1523971815752, 21.015525698898625, 21.05217637330195, 13.05340690455182, 10.947958841317664, 11.848060414256071, 18.245895789110847, 16.30674458927109, 20.23966749071046, 20.562812830031568, 13.065932264431863, 37.500773246637465, 14.65943535427574, 31.627472983419583, 25.042104237327987, 10.376118556317788, 22.447122973196702, 18.13270484757208, 26.734762955952554, 23.39993871310716, 29.30821620959183, 26.398353474904027, 27.472357199941243, 15.484761099396865, 17.093363718021138, 19.890484332710017, 15.239121533341589, 16.784572956264807, 14.590347683980632, 14.195488065833974, 13.954500203435662, 0.8202481410628616, 0.48912085243121195, 0.4383647896328416, 0.33828632879264636, 0.33819227028031146, 0.2880586757741229, 0.2880008204763129, 0.23770275453371942, 0.4405774928537107, 0.3176031180301067, 0.43811899192393666, 0.18792623023702756, 0.1878165583608722, 0.18775390118975305, 0.18773041566393178, 0.18647992054743448, 0.13841364890989682, 0.1383818255858751, 0.1383773438374532, 0.13835665212883003, 0.13835043879579056, 0.1383446183432946, 0.13831509409800877, 0.13831228572967946, 0.13829702159300877, 0.13827830883823422, 0.6847567958398804, 0.17154901933327113, 0.1873552001937789, 0.13803424271394685, 2.30107724647833, 0.8491168869885878, 1.055960228087302, 0.7313613335887106, 0.22474264398356125, 0.2281406387018415, 0.28436644263104, 0.48723700477636533, 0.38874842963770767, 0.23188375715087048, 0.17857743398912707, 0.39604235878529864, 0.3541685393138828, 0.4047900623528088, 0.3250330129480036, 0.25368311243516917, 0.22265267955470103, 0.2591785217615045, 0.24396458649359323, 0.2445585345685448], \"Total\": [733.0, 493.0, 495.0, 1116.0, 191.0, 1712.0, 453.0, 103.0, 825.0, 324.0, 346.0, 447.0, 1293.0, 289.0, 228.0, 100.0, 206.0, 294.0, 106.0, 544.0, 1090.0, 496.0, 91.0, 694.0, 384.0, 349.0, 92.0, 761.0, 462.0, 310.0, 209.03001111415585, 162.7839866086939, 108.77874254408476, 73.45474967684422, 76.2571776788643, 67.72325686418094, 106.67990548105446, 61.8541389477361, 202.19713361711987, 57.647019807369354, 80.08687533552236, 50.005668324225844, 51.70872702503653, 45.17762759914648, 42.27115206723638, 172.41016897808422, 308.994104355091, 58.547530661529876, 35.354027322572875, 72.24078462151597, 71.7950732126368, 34.348215003615216, 104.57773528267371, 47.32412973910007, 31.442221572484517, 30.45546211211524, 38.87208366180247, 32.286447602768504, 51.053621987420435, 35.85683202982374, 567.4664786968286, 105.31029186507307, 143.37021281630146, 199.96842883673693, 428.2685553661811, 209.65130719949659, 934.5498165321869, 694.2322853827335, 323.7033813974784, 194.46965199246677, 693.8912174238961, 202.89722396407794, 187.71079787517112, 125.09385127496051, 447.65059397229123, 692.2327472576687, 453.70345234545914, 1712.3873455966084, 605.2505308060884, 1090.039680842071, 627.6613163928013, 380.24434281727605, 764.6599573765537, 607.737470826194, 1293.935119051468, 390.4827176801915, 453.6681065373655, 317.58368274185153, 413.9488850419716, 665.0363806114226, 761.212758447624, 1116.4027185299462, 541.6336837409287, 753.387693995446, 442.23957526628993, 492.85608572206024, 825.586897995855, 499.7146958852943, 496.9866163518752, 507.561946769724, 519.1042746261134, 448.1269414469085, 32.430332070224665, 34.55504600379928, 24.31337240513403, 34.97300226569276, 23.018358832555016, 20.26099187672399, 20.267352266610818, 23.80325360966056, 30.976167388741946, 21.584588742367096, 42.223440726741735, 18.36588664260714, 18.129957558460575, 29.753442870444896, 14.870120179000496, 14.856078273207638, 16.788787277053512, 14.180841780831614, 14.188373833842387, 14.963479584985686, 20.169817865922905, 12.15888109433008, 12.159474007808823, 12.161523113593933, 11.48770266245338, 17.48199193322472, 24.575365294064117, 19.785939767107823, 10.814906244553653, 10.813954027180428, 28.553514090247635, 33.51519518259559, 39.00342769903212, 158.64804501653353, 45.488250948614066, 72.72170038591732, 87.82142573330046, 453.4932958848644, 57.58646025266705, 118.0651667574773, 58.309366410692824, 122.67867153733785, 136.1065470729145, 1712.3873455966084, 59.32761405194137, 324.9544422051288, 447.97933372218665, 61.6882622866203, 56.85509954157354, 103.69485806694495, 694.2322853827335, 627.6613163928013, 305.715431408305, 1116.4027185299462, 1293.935119051468, 369.49584249478437, 753.387693995446, 289.0935482167663, 544.8056801600567, 665.0363806114226, 761.212758447624, 442.23957526628993, 825.586897995855, 692.2327472576687, 1090.039680842071, 934.5498165321869, 519.1042746261134, 733.0850070209906, 541.6336837409287, 507.561946769724, 332.2253396245576, 453.70345234545914, 499.7146958852943, 45.0131923547399, 91.28352830036646, 29.36079660743322, 23.50750790888154, 24.120213486865506, 23.577530285504103, 21.0104140369361, 19.025725111788848, 17.030033990376342, 17.110756833796522, 15.07139665102023, 17.656818002436648, 38.41853244912349, 16.134609272283544, 15.851158881065567, 13.790008055348322, 13.131667619215012, 40.62885578704624, 16.876798426864116, 12.482726889785816, 18.8238878512148, 11.83471703488739, 64.74656758748299, 11.184755153264, 11.186126566568225, 11.185236793684046, 11.194662310328555, 13.113339828597947, 10.539064767036885, 12.447651590876383, 18.958527340656467, 15.20833983082694, 43.26661971734748, 24.61640325268657, 52.247185851861445, 125.78008858742427, 137.68984119531765, 324.9544422051288, 35.93702724734844, 124.94959163495113, 139.70118218997922, 52.26154710919291, 76.33584369506262, 97.40562311838235, 228.215393717008, 496.9866163518752, 259.48920379417007, 462.7598165990179, 825.586897995855, 733.0850070209906, 144.25079927660448, 143.02645091339193, 1116.4027185299462, 1293.935119051468, 1090.039680842071, 495.9003625582281, 367.14962692943595, 270.3556612121388, 761.212758447624, 453.4932958848644, 753.387693995446, 1712.3873455966084, 544.8056801600567, 492.85608572206024, 355.74335943667, 665.0363806114226, 465.51179054531343, 934.5498165321869, 764.6599573765537, 106.11988009227794, 32.5784594319712, 21.688027837078753, 25.656106577891517, 24.63610519023458, 23.86954130293282, 17.43253616568672, 17.420635112395487, 27.553699380628128, 20.13742337825387, 20.14124015569834, 16.21789374973983, 15.60924236797761, 35.33816363122558, 15.01020676040294, 14.99901758212201, 41.59909005201018, 14.388393159110242, 13.785569437404522, 19.96989026619096, 13.780537297507149, 32.84654302137766, 12.569517533636022, 12.568687887675967, 12.566054703528666, 19.437679481524125, 11.96110637226757, 16.26757126296797, 11.352769452360388, 12.741884425534833, 23.840948596496382, 24.440091463880076, 17.932176511390306, 28.028098210859845, 20.25337582829719, 495.9003625582281, 29.563281671192716, 83.01277734675062, 59.606049834169966, 126.07520425917087, 49.180830930649456, 206.25217586805232, 493.6405816649861, 129.7120624435893, 346.33146020199695, 544.8056801600567, 349.13775655710776, 825.586897995855, 348.0169398340159, 384.2466059292498, 75.69612349620347, 140.4673664751255, 80.64988021723609, 465.51179054531343, 1293.935119051468, 319.2204434155495, 1712.3873455966084, 753.387693995446, 191.92662965033745, 764.6599573765537, 289.0935482167663, 761.212758447624, 367.14962692943595, 665.0363806114226, 369.49584249478437, 1116.4027185299462, 334.6406009908271, 507.561946769724, 28.79857298983713, 73.46839189441638, 46.657300103261065, 19.099310828071708, 18.31813263093923, 16.147744118648177, 15.568894033875113, 16.18781792334715, 17.396315995660544, 13.266925447145875, 13.266541775830424, 15.076253319084627, 16.66076211846214, 36.62820673460483, 12.15904986431738, 11.538853887662185, 11.537632491551738, 11.546220892813555, 16.567613746823856, 10.963111535049869, 48.54653052239401, 11.010053246731943, 23.264312210547942, 10.421217802402479, 9.813174952314204, 9.816469660330114, 12.498099179557219, 9.816158001798426, 9.816907341480137, 11.714395773992127, 20.568452183982597, 35.883539300080734, 733.0850070209906, 39.31935286398878, 87.26057995825687, 90.22858353844674, 41.59378854055589, 154.98960438146722, 78.91267186071502, 45.84269583319564, 145.96492900937338, 58.764051636562726, 825.586897995855, 366.15402687211287, 239.0821006592023, 1116.4027185299462, 607.737470826194, 266.06323384607515, 761.212758447624, 294.04620746091103, 153.50881499267678, 1293.935119051468, 1090.039680842071, 462.7598165990179, 519.1042746261134, 150.35030094756297, 298.13069262173497, 273.43791202773207, 369.49584249478437, 753.387693995446, 96.91022632715794, 314.3883597615592, 492.85608572206024, 541.6336837409287, 693.8912174238961, 764.6599573765537, 1712.3873455966084, 310.4605370434236, 665.0363806114226, 499.7146958852943, 20.943002900335703, 18.129643787389075, 18.132000987384156, 15.948322307154719, 92.44781475796383, 21.611623024209653, 22.63014706809167, 11.371985298124029, 11.385967426175272, 19.12064649185412, 11.58543979953734, 10.810327001065897, 10.80819067308588, 10.807186605839243, 10.823047208941757, 12.562318361269048, 23.720987933186084, 14.12702404161549, 10.246963491146035, 13.42564989947977, 10.333607724362269, 9.69853744091849, 9.121101471448874, 9.11982885868463, 11.13766367233077, 35.8951176770727, 12.154384354162557, 8.558366995841217, 9.201853928534147, 8.558249631729888, 191.92662965033745, 39.28450729562118, 32.260713687259624, 115.89539724026272, 98.31731977758089, 24.686346575175616, 57.42466600056441, 206.25217586805232, 110.0118731247504, 145.08716083972746, 228.215393717008, 493.6405816649861, 346.33146020199695, 71.83615236765802, 101.69146383711393, 289.0935482167663, 28.491729122752634, 96.29833919524643, 69.6002853046517, 105.65884323068597, 1116.4027185299462, 384.2466059292498, 349.13775655710776, 225.90762220050942, 544.8056801600567, 733.0850070209906, 132.08715923178144, 764.6599573765537, 216.6552986808714, 349.26770896349745, 175.82824003418946, 1293.935119051468, 1712.3873455966084, 348.0169398340159, 447.97933372218665, 753.387693995446, 20.550691079969365, 13.956866287096398, 12.360504276141599, 11.483692469998859, 11.596925014725281, 16.05963643308495, 9.42123075058169, 9.46506679654211, 9.514996465061035, 9.52127396467269, 15.766317170998716, 36.326673752734976, 9.100589518812772, 8.661116664702929, 8.661615288919608, 10.194698073215624, 16.52879640090143, 10.708728486032713, 7.393944673026701, 7.411241169785304, 13.824840538233593, 11.284767850617474, 6.949337532584407, 6.949361098295015, 6.994144155811081, 6.995551781872996, 27.649767398631123, 8.338792322998565, 6.5374905843305475, 6.566025445942184, 48.44645741612914, 91.54414558758197, 39.727752596594804, 66.47270965151455, 71.26305471231547, 42.090723133816546, 34.21020182413243, 38.07930878582131, 69.2379410848148, 66.69878917969005, 96.84367584044709, 101.53602279005759, 57.54844991741698, 447.97933372218665, 78.91267186071502, 453.4932958848644, 289.0935482167663, 42.10662713394581, 294.04620746091103, 206.32901222161377, 733.0850070209906, 493.6405816649861, 1293.935119051468, 1116.4027185299462, 1712.3873455966084, 168.65758765957537, 310.4605370434236, 1090.039680842071, 242.6134104297979, 495.9003625582281, 519.1042746261134, 499.7146958852943, 398.84431689056333, 6.6580683513833225, 4.833764392644489, 4.774543524676675, 4.635199325545516, 4.635875840374169, 4.567111344210767, 4.567070847331243, 4.500811470924906, 9.044032424593452, 7.308666496149458, 10.082223106179228, 4.425382586748005, 4.427126408631974, 4.428228558489528, 4.428331659425132, 5.303122580223493, 4.344870081459314, 4.345495997877561, 4.345451427696294, 4.346040914248122, 4.34585136128258, 4.346076285057965, 4.346632189171051, 4.34675200381008, 4.346834714199938, 4.347111954727014, 21.96916765176214, 5.696157671914368, 6.741649941386885, 4.999290816493838, 103.99337173015924, 59.12378286888146, 100.10070664541438, 60.65705028176219, 10.194698073215624, 10.60194514668909, 16.26757126296797, 72.25981357382804, 56.85509954157354, 20.64044250562419, 9.46919950321437, 120.00268287037024, 143.37021281630146, 567.4664786968286, 159.9252891002764, 69.40128567889514, 34.42628751716915, 253.34736172272116, 335.9664795853904, 1293.935119051468], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.1429, -6.4016, -6.8066, -7.2122, -7.177, -7.2959, -6.8438, -7.3914, -6.2211, -7.4774, -7.1493, -7.6212, -7.5914, -7.7268, -7.7973, -6.3984, -5.8176, -7.4845, -7.9951, -7.2806, -7.2878, -8.0262, -6.9171, -7.7126, -8.1243, -8.1597, -7.916, -8.1043, -7.6461, -7.9998, -5.2475, -6.9243, -6.62, -6.3041, -5.5653, -6.2649, -4.8371, -5.1229, -5.8555, -6.3506, -5.1567, -6.3162, -6.3976, -6.7751, -5.6031, -5.2068, -5.597, -4.3976, -5.3411, -4.8344, -5.3469, -5.7833, -5.1807, -5.3833, -4.7568, -5.7911, -5.6712, -5.9692, -5.7591, -5.3902, -5.2875, -5.0052, -5.5619, -5.3422, -5.7291, -5.6745, -5.3648, -5.6774, -5.6885, -5.6951, -5.698, -5.7592, -6.6102, -6.5509, -6.9401, -6.5784, -7.0114, -7.1564, -7.1594, -6.9987, -6.7733, -7.135, -6.467, -7.31, -7.3341, -6.8443, -7.5452, -7.5465, -7.4322, -7.6126, -7.6168, -7.574, -7.301, -7.8161, -7.8166, -7.8258, -7.8945, -7.4938, -7.155, -7.3727, -7.9842, -7.9854, -7.0855, -6.9389, -6.8081, -5.5859, -6.7063, -6.3223, -6.1884, -4.94, -6.5477, -6.0172, -6.5624, -6.0649, -5.9937, -4.3601, -6.6075, -5.5389, -5.3714, -6.5864, -6.64, -6.3496, -5.3801, -5.4426, -5.8213, -5.2005, -5.1305, -5.8012, -5.5256, -5.9518, -5.7534, -5.7037, -5.659, -5.879, -5.7001, -5.803, -5.6695, -5.7255, -5.9044, -5.8412, -5.9425, -5.9956, -6.076, -6.0275, -6.0415, -6.1809, -5.5128, -6.6521, -6.8835, -6.8591, -6.8891, -7.0331, -7.1466, -7.2763, -7.2866, -7.4284, -7.2812, -6.5106, -7.3782, -7.3971, -7.5513, -7.6126, -6.4873, -7.3825, -7.6857, -7.288, -7.756, -6.063, -7.8366, -7.8366, -7.8368, -7.8393, -7.6919, -7.9241, -7.7583, -7.3447, -7.5614, -6.5613, -7.1062, -6.4263, -5.6424, -5.5683, -4.7964, -6.7928, -5.7295, -5.7326, -6.53, -6.2834, -6.1286, -5.5668, -5.1074, -5.6507, -5.3852, -5.1283, -5.1999, -6.1142, -6.1532, -5.1025, -5.0596, -5.1539, -5.5637, -5.7398, -5.9242, -5.5801, -5.8133, -5.7018, -5.4922, -5.8065, -5.8426, -5.9731, -5.8537, -5.978, -5.9576, -5.9838, -5.1068, -6.3269, -6.7771, -6.619, -6.6813, -6.7148, -7.0439, -7.0476, -6.5903, -6.9109, -6.912, -7.1351, -7.1844, -6.376, -7.2374, -7.239, -6.227, -7.2981, -7.3478, -6.9782, -7.3499, -6.4932, -7.4736, -7.4737, -7.479, -7.0485, -7.5443, -7.241, -7.6204, -7.5054, -6.8833, -6.8665, -7.1897, -6.81, -7.0906, -4.5537, -6.8002, -6.0385, -6.3014, -5.8088, -6.4809, -5.672, -5.1409, -6.0076, -5.4627, -5.3045, -5.5391, -5.1734, -5.6339, -5.5908, -6.3702, -6.1079, -6.3481, -5.6718, -5.2681, -5.8961, -5.3633, -5.63, -6.1356, -5.8018, -6.0524, -5.8997, -6.053, -6.0136, -6.1156, -6.0254, -6.1804, -6.227, -6.3619, -5.4907, -5.9584, -6.8633, -6.9326, -7.0594, -7.1045, -7.0734, -7.0127, -7.3166, -7.3182, -7.2159, -7.125, -6.3396, -7.4521, -7.5119, -7.5122, -7.5144, -7.1538, -7.5868, -6.1024, -7.6013, -6.8647, -7.6781, -7.7553, -7.7553, -7.5138, -7.7554, -7.7554, -7.5971, -7.1031, -6.6606, -4.1487, -6.6286, -6.0257, -6.0303, -6.626, -5.6742, -6.2103, -6.6565, -5.9765, -6.5513, -5.0752, -5.55, -5.7827, -4.9939, -5.3641, -5.7839, -5.3129, -5.796, -6.1015, -5.1691, -5.2492, -5.6275, -5.5942, -6.1197, -5.864, -5.9045, -5.7992, -5.5873, -6.3148, -5.9392, -5.8071, -5.7806, -5.7482, -5.7384, -5.5963, -5.986, -5.9615, -5.9855, -6.5111, -6.6876, -6.6877, -6.904, -5.1792, -6.6499, -6.6214, -7.3102, -7.3114, -6.8096, -7.3117, -7.3845, -7.3864, -7.3866, -7.3861, -7.2417, -6.6086, -7.1322, -7.465, -7.2146, -7.4859, -7.5549, -7.651, -7.6512, -7.4524, -6.2841, -7.389, -7.7536, -7.6836, -7.7576, -4.6891, -6.2705, -6.472, -5.2675, -5.4333, -6.7426, -6.0553, -5.0842, -5.5995, -5.396, -5.1674, -4.7012, -4.9508, -6.0603, -5.8323, -5.1783, -6.7104, -5.9866, -6.2104, -6.0708, -4.9525, -5.5091, -5.5762, -5.8619, -5.5755, -5.5183, -6.0757, -5.552, -5.9689, -5.8405, -6.0513, -5.778, -5.7512, -5.9847, -5.9963, -5.9907, -5.9848, -6.4809, -6.6628, -6.7566, -6.7856, -6.522, -7.0637, -7.0716, -7.0946, -7.0959, -6.5929, -5.7823, -7.1701, -7.2308, -7.2308, -7.0771, -6.6686, -7.1382, -7.5185, -7.5219, -6.9121, -7.1389, -7.6286, -7.6289, -7.6409, -7.6416, -6.3076, -7.5103, -7.7643, -7.7736, -5.7866, -5.244, -6.0803, -5.7532, -5.7515, -6.2295, -6.4053, -6.3263, -5.8946, -6.0069, -5.7909, -5.775, -6.2285, -5.1741, -6.1134, -5.3445, -5.5779, -6.459, -5.6873, -5.9008, -5.5125, -5.6458, -5.4206, -5.5252, -5.4853, -6.0586, -5.9598, -5.8083, -6.0746, -5.978, -6.1181, -6.1456, -6.1627, -6.4127, -6.9297, -7.0393, -7.2984, -7.2987, -7.4591, -7.4593, -7.6513, -7.0342, -7.3615, -7.0398, -7.8863, -7.8868, -7.8872, -7.8873, -7.894, -8.1921, -8.1923, -8.1923, -8.1925, -8.1925, -8.1926, -8.1928, -8.1928, -8.1929, -8.193, -6.5932, -7.9774, -7.8893, -8.1948, -5.3812, -6.3781, -6.1601, -6.5274, -7.7074, -7.6923, -7.472, -6.9336, -7.1594, -7.6761, -7.9373, -7.1408, -7.2525, -7.1189, -7.3384, -7.5862, -7.7167, -7.5648, -7.6253, -7.6229], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6409, 0.6322, 0.6304, 0.6174, 0.6151, 0.6149, 0.6126, 0.6101, 0.5959, 0.5945, 0.5938, 0.5929, 0.5892, 0.5888, 0.5848, 0.578, 0.5754, 0.5719, 0.5658, 0.5656, 0.5647, 0.5635, 0.5592, 0.5566, 0.5537, 0.5503, 0.5499, 0.5473, 0.5473, 0.5469, 0.5375, 0.5451, 0.5408, 0.524, 0.5012, 0.5159, 0.449, 0.4605, 0.4909, 0.5053, 0.4272, 0.4974, 0.4937, 0.5221, 0.4191, 0.3795, 0.4118, 0.283, 0.3794, 0.2979, 0.3374, 0.4021, 0.3061, 0.3332, 0.204, 0.3677, 0.3376, 0.3963, 0.3414, 0.2362, 0.2038, 0.1032, 0.2697, 0.1594, 0.3053, 0.2515, 0.0453, 0.2348, 0.2291, 0.2015, 0.1761, 0.262, 2.0369, 2.0328, 1.9951, 1.9932, 1.9786, 1.9612, 1.9579, 1.9578, 1.9197, 1.9192, 1.9162, 1.9057, 1.8945, 1.8889, 1.8816, 1.8814, 1.8733, 1.8618, 1.857, 1.8466, 1.8211, 1.8121, 1.8115, 1.8022, 1.7904, 1.7713, 1.7695, 1.7686, 1.7611, 1.76, 1.689, 1.6753, 1.6545, 1.4736, 1.6024, 1.5173, 1.4626, 1.0693, 1.5252, 1.3378, 1.4981, 1.2518, 1.2191, 0.3205, 1.4357, 0.8037, 0.6501, 1.4177, 1.4457, 1.1352, 0.2034, 0.2416, 0.5823, -0.0921, -0.1697, 0.4129, -0.024, 0.5077, 0.0724, -0.0773, -0.1677, 0.1554, -0.2899, -0.2167, -0.5372, -0.4394, -0.0303, -0.3122, -0.1109, -0.099, 0.2444, -0.0187, -0.1293, 2.1384, 2.0995, 2.0945, 2.0854, 2.0841, 2.0768, 2.0481, 2.0338, 2.0149, 1.9999, 1.9851, 1.974, 1.9671, 1.967, 1.9658, 1.951, 1.9386, 1.9344, 1.9178, 1.9162, 1.9031, 1.8992, 1.8928, 1.8751, 1.875, 1.8749, 1.8715, 1.8607, 1.847, 1.8464, 1.8392, 1.8429, 1.7976, 1.8166, 1.7439, 1.6493, 1.6329, 1.5462, 1.7517, 1.5688, 1.4542, 1.6399, 1.5077, 1.4188, 1.1292, 0.8103, 0.9168, 0.6038, 0.2818, 0.329, 1.0405, 1.01, 0.0059, -0.0989, -0.0216, 0.3562, 0.4807, 0.6023, -0.0887, 0.1959, -0.2002, -0.8116, 0.0193, 0.0834, 0.2789, -0.2273, 0.0051, -0.6715, -0.497, 2.3548, 2.3157, 2.2724, 2.2624, 2.2407, 2.2388, 2.224, 2.2209, 2.2198, 2.2128, 2.2115, 2.2051, 2.194, 2.1853, 2.1801, 2.1793, 2.1712, 2.1617, 2.1548, 2.1538, 2.1531, 2.1412, 2.1214, 2.1213, 2.1162, 2.1106, 2.1003, 2.096, 2.0763, 2.0759, 2.0715, 2.0635, 2.0499, 1.983, 2.0274, 1.3662, 1.9395, 1.6688, 1.737, 1.4805, 1.7498, 1.1252, 0.7835, 1.2533, 0.8162, 0.5213, 0.7317, 0.2367, 0.64, 0.5842, 1.4293, 1.0734, 1.388, 0.3113, -0.3073, 0.4643, -0.6827, -0.1284, 0.7336, -0.315, 0.4071, -0.4084, 0.1675, -0.3872, 0.0985, -0.9171, 0.1327, -0.3304, 2.404, 2.3386, 2.325, 2.3132, 2.2858, 2.2851, 2.2765, 2.2685, 2.2573, 2.2243, 2.2228, 2.1972, 2.1882, 2.1858, 2.176, 2.1686, 2.1684, 2.1655, 2.165, 2.1449, 2.1413, 2.1261, 2.1146, 2.1043, 2.0872, 2.0869, 2.0868, 2.0868, 2.0867, 2.0683, 1.9994, 1.8854, 1.3802, 1.826, 1.6316, 1.5936, 1.7723, 1.4087, 1.5476, 1.6445, 1.1663, 1.5014, 0.3349, 0.6732, 0.8668, 0.1145, 0.3524, 0.7586, 0.1784, 0.6465, 0.991, -0.2083, -0.1169, 0.3615, 0.2799, 0.9936, 0.5647, 0.6107, 0.4149, -0.0856, 1.2376, 0.4364, 0.1189, 0.0511, -0.1643, -0.2516, -0.9157, 0.4022, -0.3351, -0.0733, 2.5734, 2.5411, 2.5408, 2.4529, 2.4203, 2.4031, 2.3856, 2.3849, 2.3824, 2.3659, 2.3647, 2.3613, 2.3595, 2.3594, 2.3584, 2.3539, 2.3512, 2.3459, 2.3343, 2.3145, 2.3049, 2.2993, 2.2646, 2.2645, 2.2635, 2.2615, 2.2395, 2.2257, 2.2232, 2.2218, 2.18, 2.1849, 2.1803, 2.106, 2.1048, 2.1774, 2.0204, 1.7129, 1.8262, 1.7529, 1.5285, 1.2232, 1.3281, 1.7915, 1.672, 1.2812, 2.0662, 1.5722, 1.6731, 1.3952, 0.1559, 0.6658, 0.6946, 0.8442, 0.2503, 0.0107, 1.1671, -0.0652, 0.7791, 0.4299, 0.9054, -0.8172, -1.0706, 0.2892, 0.0252, -0.489, 3.1185, 3.0094, 2.9489, 2.9287, 2.8899, 2.8279, 2.8196, 2.807, 2.7788, 2.7768, 2.7755, 2.7514, 2.7478, 2.7365, 2.7365, 2.7273, 2.6525, 2.617, 2.6071, 2.6014, 2.5877, 2.5638, 2.559, 2.5587, 2.5402, 2.5394, 2.499, 2.495, 2.4843, 2.4707, 2.4592, 2.3654, 2.3638, 2.1762, 2.1084, 2.157, 2.1884, 2.1602, 1.9941, 1.9191, 1.7623, 1.7308, 1.8451, 0.8473, 1.6445, 0.6648, 0.8815, 1.927, 0.7552, 0.896, 0.0164, 0.2787, -0.4598, -0.4168, -0.8047, 0.9397, 0.4284, -0.676, 0.5601, -0.0582, -0.244, -0.2334, -0.025, 3.8177, 3.6209, 3.5237, 3.2941, 3.2937, 3.1482, 3.148, 2.9707, 2.8899, 2.7757, 2.7756, 2.7526, 2.7516, 2.7511, 2.7509, 2.564, 2.4652, 2.4648, 2.4648, 2.4645, 2.4645, 2.4644, 2.4641, 2.464, 2.4639, 2.4637, 2.4433, 2.409, 2.3286, 2.3221, 2.1007, 1.6685, 1.36, 1.4936, 2.097, 2.0728, 1.865, 0.9124, 0.9264, 1.4229, 1.9409, 0.1979, -0.0917, -1.3339, -0.2869, 0.3001, 0.8707, -0.9733, -1.3161, -2.6621]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7], \"Freq\": [0.05144647029242964, 0.15433941087728892, 0.05144647029242964, 0.7202505840940149, 0.05144647029242964, 0.05144647029242964, 0.1828588313007404, 0.00795038396959741, 0.5485764939022213, 0.039751919847987045, 0.00795038396959741, 0.174908447331143, 0.04770230381758445, 0.09338176808799382, 0.09338176808799382, 0.09338176808799382, 0.09338176808799382, 0.09338176808799382, 0.18676353617598765, 0.4669088404399691, 0.12930981476017728, 0.22988411512920404, 0.04310327158672576, 0.24425187232477927, 0.014367757195575252, 0.3304584154982308, 0.014367757195575252, 0.08782740741916174, 0.08782740741916174, 0.08782740741916174, 0.08782740741916174, 0.08782740741916174, 0.702619259353294, 0.07956279994672591, 0.07956279994672591, 0.07956279994672591, 0.7160651995205332, 0.07956279994672591, 0.07956279994672591, 0.20768790165165443, 0.0934595557432445, 0.0934595557432445, 0.2803786672297335, 0.010384395082582721, 0.30114745739489895, 0.010384395082582721, 0.41047159582128123, 0.13682386527376042, 0.13682386527376042, 0.13682386527376042, 0.13682386527376042, 0.13682386527376042, 0.07957947212494552, 0.07957947212494552, 0.07957947212494552, 0.7162152491245096, 0.07957947212494552, 0.07957947212494552, 0.02221570938846553, 0.02221570938846553, 0.8886283755386212, 0.02221570938846553, 0.02221570938846553, 0.02221570938846553, 0.07251627381117878, 0.07251627381117878, 0.7251627381117879, 0.07251627381117878, 0.07251627381117878, 0.07251627381117878, 0.10411641843157544, 0.02602910460789386, 0.7548440336289219, 0.02602910460789386, 0.02602910460789386, 0.07808731382368159, 0.10186507473434354, 0.10186507473434354, 0.10186507473434354, 0.10186507473434354, 0.6111904484060613, 0.10186507473434354, 0.7336749740411209, 0.030921167488323104, 0.13773974608434836, 0.005622030452422383, 0.08151944156012454, 0.005622030452422383, 0.005622030452422383, 0.1582401389145092, 0.05274671297150307, 0.6857072686295399, 0.05274671297150307, 0.05274671297150307, 0.05274671297150307, 0.5701969035717971, 0.023758204315491544, 0.023758204315491544, 0.023758204315491544, 0.04751640863098309, 0.023758204315491544, 0.30885665610139007, 0.10988299141861045, 0.10988299141861045, 0.10988299141861045, 0.10988299141861045, 0.10988299141861045, 0.10988299141861045, 0.5494149570930522, 0.6284947128724118, 0.10442075166845712, 0.08077831732842909, 0.06107628871173907, 0.06304649157340807, 0.037433854371711046, 0.023642434340028027, 0.12274913146023571, 0.040916377153411905, 0.040916377153411905, 0.6955784116080024, 0.040916377153411905, 0.040916377153411905, 0.672040921616881, 0.10339091101798169, 0.060926786849882064, 0.04800292297263436, 0.083081982068021, 0.014770130145425955, 0.0166163964136042, 0.7190857721563516, 0.056276277820931864, 0.056276277820931864, 0.06252919757881317, 0.08754087661033845, 0.018758759273643955, 0.006252919757881318, 0.7420641711902202, 0.06710154739486034, 0.027630048927295432, 0.055260097854590864, 0.09867874616891226, 0.003947149846756491, 0.003947149846756491, 0.15757954231753424, 0.14773082092268836, 0.2560667562659931, 0.15757954231753424, 0.01969744278969178, 0.06894104976392122, 0.20682314929176368, 0.9053546029194552, 0.013928532352607003, 0.05571412941042801, 0.013928532352607003, 0.013928532352607003, 0.013928532352607003, 0.7620038848050961, 0.07242185682031904, 0.05038042213587412, 0.034636540218413454, 0.056677974902858384, 0.022041434684444926, 0.0031487763834921324, 0.06632947714762305, 0.06632947714762305, 0.06632947714762305, 0.06632947714762305, 0.7296242486238537, 0.06632947714762305, 0.09758988610275204, 0.09758988610275204, 0.09758988610275204, 0.09758988610275204, 0.09758988610275204, 0.6831292027192643, 0.07253962228696119, 0.07253962228696119, 0.07253962228696119, 0.7253962228696118, 0.07253962228696119, 0.07253962228696119, 0.06406460841760661, 0.06406460841760661, 0.06406460841760661, 0.7687753010112792, 0.06406460841760661, 0.06406460841760661, 0.08932828630992896, 0.08932828630992896, 0.7146262904794317, 0.08932828630992896, 0.08932828630992896, 0.08932828630992896, 0.07004391801578966, 0.6303952621421068, 0.07004391801578966, 0.14008783603157932, 0.03502195900789483, 0.07004391801578966, 0.6125936840513221, 0.11173092350621597, 0.08283499501322908, 0.03852790465731585, 0.10402534257475281, 0.021190347561523718, 0.02889592849298689, 0.9282765746052726, 0.019339095304276514, 0.019339095304276514, 0.019339095304276514, 0.019339095304276514, 0.019339095304276514, 0.10811383924780071, 0.1390035076043152, 0.6950175380215761, 0.015444834178257245, 0.015444834178257245, 0.015444834178257245, 0.061928154957889926, 0.061928154957889926, 0.061928154957889926, 0.061928154957889926, 0.7431378594946791, 0.061928154957889926, 0.5526653796218179, 0.05299531037469487, 0.06813682762460768, 0.03785379312478205, 0.0757075862495641, 0.20441048287382307, 0.00757075862495641, 0.19008353977008638, 0.2027557757547588, 0.012672235984672426, 0.012672235984672426, 0.36749484355550033, 0.012672235984672426, 0.19008353977008638, 0.4567989756805661, 0.024042051351608744, 0.024042051351608744, 0.024042051351608744, 0.4567989756805661, 0.024042051351608744, 0.07537541414428697, 0.07537541414428697, 0.07537541414428697, 0.07537541414428697, 0.7537541414428698, 0.07537541414428697, 0.09121498005405919, 0.09121498005405919, 0.09121498005405919, 0.09121498005405919, 0.6385048603784144, 0.09121498005405919, 0.0573639997356407, 0.0573639997356407, 0.0573639997356407, 0.8030959962989699, 0.0573639997356407, 0.0573639997356407, 0.6772208834496082, 0.07204477483506469, 0.10086268476909058, 0.014408954967012938, 0.05763581986805175, 0.028817909934025877, 0.05763581986805175, 0.4478713492472825, 0.010923691445055672, 0.010923691445055672, 0.10923691445055671, 0.021847382890111344, 0.010923691445055672, 0.38232920057694847, 0.7796259062298987, 0.04244381724460767, 0.08935540472548982, 0.033508276772058684, 0.024572736299509703, 0.013403310708823474, 0.015637195826960718, 0.2168340895861446, 0.2168340895861446, 0.04818535324136547, 0.45776085579297193, 0.012046338310341367, 0.04818535324136547, 0.012046338310341367, 0.06308687002024276, 0.06308687002024276, 0.757042440242913, 0.06308687002024276, 0.06308687002024276, 0.06308687002024276, 0.10565165798569237, 0.10565165798569237, 0.10565165798569237, 0.10565165798569237, 0.10565165798569237, 0.10565165798569237, 0.6339099479141542, 0.9529676475375249, 0.013613823536250355, 0.013613823536250355, 0.013613823536250355, 0.013613823536250355, 0.013613823536250355, 0.06177484851480404, 0.06177484851480404, 0.06177484851480404, 0.06177484851480404, 0.7412981821776485, 0.06177484851480404, 0.09929875144426803, 0.049649375722134015, 0.049649375722134015, 0.7943900115541442, 0.049649375722134015, 0.049649375722134015, 0.018846610062703367, 0.009423305031351683, 0.009423305031351683, 0.9046372830097615, 0.009423305031351683, 0.056539830188110096, 0.06575339656554911, 0.13150679313109823, 0.6575339656554912, 0.06575339656554911, 0.06575339656554911, 0.06575339656554911, 0.09615594944502179, 0.024038987361255447, 0.024038987361255447, 0.7452086081989189, 0.024038987361255447, 0.07211696208376635, 0.10963588149196979, 0.10963588149196979, 0.10963588149196979, 0.10963588149196979, 0.10963588149196979, 0.6578152889518187, 0.6964550827784715, 0.1356730680737282, 0.02035096021105923, 0.03617948481966085, 0.07688140524177932, 0.02035096021105923, 0.015828524608601624, 0.6023309815180943, 0.15460902199394932, 0.03543123420694672, 0.012884085166162445, 0.11917778778700261, 0.019326127749243666, 0.05475736195619039, 0.06667103325433421, 0.06667103325433421, 0.06667103325433421, 0.7333813657976763, 0.06667103325433421, 0.06667103325433421, 0.9126387721492906, 0.06796246175579823, 0.0032363077026570587, 0.0064726154053141175, 0.0064726154053141175, 0.0032363077026570587, 0.0032363077026570587, 0.9052474015752978, 0.034160279304728224, 0.017080139652364112, 0.017080139652364112, 0.017080139652364112, 0.017080139652364112, 0.06724895212428711, 0.7397384733671581, 0.06724895212428711, 0.06724895212428711, 0.06724895212428711, 0.06724895212428711, 0.9226150244962973, 0.02365679549990506, 0.02365679549990506, 0.02365679549990506, 0.02365679549990506, 0.02365679549990506, 0.07537759401789579, 0.07537759401789579, 0.07537759401789579, 0.07537759401789579, 0.7537759401789579, 0.07537759401789579, 0.08224440984675137, 0.7401996886207624, 0.08224440984675137, 0.08224440984675137, 0.08224440984675137, 0.08224440984675137, 0.409046757607438, 0.07271942357465565, 0.018179855893663912, 0.036359711787327824, 0.05453956768099174, 0.3908669017137741, 0.009089927946831956, 0.2000283713643472, 0.2000283713643472, 0.2000283713643472, 0.2000283713643472, 0.2000283713643472, 0.2000283713643472, 0.0695004639463048, 0.0695004639463048, 0.0695004639463048, 0.7645051034093528, 0.0695004639463048, 0.0695004639463048, 0.04935592522243736, 0.8390507287814352, 0.04935592522243736, 0.04935592522243736, 0.04935592522243736, 0.04935592522243736, 0.22218215680882594, 0.22218215680882594, 0.22218215680882594, 0.22218215680882594, 0.22218215680882594, 0.22218215680882594, 0.2543276852645927, 0.025432768526459267, 0.05086553705291853, 0.10173107410583707, 0.4832226020027261, 0.0762983055793778, 0.055157327135238514, 0.7722025798933392, 0.055157327135238514, 0.055157327135238514, 0.055157327135238514, 0.055157327135238514, 0.4505811357999503, 0.15019371193331676, 0.15019371193331676, 0.15019371193331676, 0.15019371193331676, 0.15019371193331676, 0.15019371193331676, 0.7351923927197077, 0.07441218549794612, 0.08334164775769966, 0.03571784903901414, 0.04464731129876767, 0.0029764874199178447, 0.023811899359342758, 0.47498461314362234, 0.09499692262872446, 0.11874615328590558, 0.023749230657181115, 0.04749846131436223, 0.023749230657181115, 0.23749230657181117, 0.18598911690378772, 0.12399274460252516, 0.02479854892050503, 0.3471796848870704, 0.06199637230126258, 0.2479854892050503, 0.012399274460252515, 0.7426705123413742, 0.08707171524002318, 0.0537795888247202, 0.04609679042118874, 0.05634052162589735, 0.005121865602354305, 0.01024373120470861, 0.08033643878118817, 0.08033643878118817, 0.6426915102495053, 0.08033643878118817, 0.08033643878118817, 0.08033643878118817, 0.541225899509922, 0.0685096075329015, 0.12331729355922272, 0.00685096075329015, 0.25348554787173555, 0.00685096075329015, 0.04759544472764839, 0.04759544472764839, 0.8091225603700226, 0.04759544472764839, 0.04759544472764839, 0.04759544472764839, 0.3087153657503546, 0.07016258312508059, 0.3087153657503546, 0.014032516625016119, 0.014032516625016119, 0.014032516625016119, 0.2946828491253385, 0.17193716985049817, 0.04298429246262454, 0.04298429246262454, 0.04298429246262454, 0.6447643869393681, 0.08596858492524909, 0.030835330265339227, 0.8942245776948377, 0.030835330265339227, 0.030835330265339227, 0.030835330265339227, 0.030835330265339227, 0.9652621233183188, 0.009192972603031608, 0.009192972603031608, 0.009192972603031608, 0.009192972603031608, 0.009192972603031608, 0.1947851710666047, 0.08347935902854488, 0.6121819662093291, 0.027826453009514956, 0.027826453009514956, 0.027826453009514956, 0.046329351554294844, 0.7875989764230124, 0.046329351554294844, 0.046329351554294844, 0.046329351554294844, 0.046329351554294844, 0.1207174449237396, 0.0603587224618698, 0.0603587224618698, 0.0603587224618698, 0.6639459470805679, 0.0603587224618698, 0.06572927352519867, 0.02190975784173289, 0.8544805558275826, 0.032864636762599334, 0.010954878920866445, 0.010954878920866445, 0.14294798054260974, 0.14294798054260974, 0.14294798054260974, 0.14294798054260974, 0.14294798054260974, 0.14294798054260974, 0.42884394162782924, 0.13493016582389383, 0.13493016582389383, 0.13493016582389383, 0.13493016582389383, 0.13493016582389383, 0.13493016582389383, 0.5397206632955753, 0.3093391775898161, 0.01586354756870852, 0.18243079704014797, 0.38072514164900445, 0.03172709513741704, 0.06345419027483408, 0.01586354756870852, 0.08808423391281738, 0.08808423391281738, 0.08808423391281738, 0.704673871302539, 0.08808423391281738, 0.08808423391281738, 0.6359544699275157, 0.06801652084786265, 0.04421073855111072, 0.0034008260423931324, 0.1496363458652978, 0.020404956254358793, 0.0748181729326489, 0.8046655049277411, 0.07490237412891207, 0.05350169580636577, 0.01819057657416436, 0.028890915735437513, 0.007490237412891207, 0.012840406993527783, 0.3579067779970906, 0.03579067779970906, 0.45096254027633415, 0.014316271119883624, 0.042948813359650874, 0.03579067779970906, 0.07158135559941813, 0.0924649717147198, 0.6472548020030385, 0.0924649717147198, 0.0924649717147198, 0.0924649717147198, 0.0924649717147198, 0.23749614830702215, 0.1961924703405835, 0.2684739067818511, 0.020651838983219317, 0.030977758474828976, 0.05162959745804829, 0.20651838983219317, 0.06731251556499007, 0.7404376712148908, 0.06731251556499007, 0.06731251556499007, 0.06731251556499007, 0.06731251556499007, 0.7223114031812972, 0.070056958836982, 0.06764120163570676, 0.05314665842805531, 0.07488847323953249, 0.004831514402550483, 0.0024157572012752414, 0.05515828174713488, 0.05515828174713488, 0.05515828174713488, 0.05515828174713488, 0.05515828174713488, 0.7722159444598883, 0.1672075864597476, 0.1672075864597476, 0.055735862153249195, 0.027867931076624598, 0.5294906904558674, 0.027867931076624598, 0.055735862153249195, 0.06429861979498291, 0.021432873264994302, 0.021432873264994302, 0.021432873264994302, 0.8144491840697835, 0.021432873264994302, 0.042865746529988605, 0.15756588528818247, 0.18382686616954622, 0.15756588528818247, 0.10504392352545498, 0.05252196176272749, 0.05252196176272749, 0.31513177057636493, 0.26433062951562314, 0.020333125347355624, 0.20333125347355624, 0.48799500833653503, 0.020333125347355624, 0.020333125347355624, 0.07625822353960288, 0.07625822353960288, 0.6863240118564259, 0.07625822353960288, 0.07625822353960288, 0.07625822353960288, 0.056635346179702346, 0.056635346179702346, 0.7362595003361305, 0.056635346179702346, 0.056635346179702346, 0.056635346179702346, 0.062267910246079375, 0.062267910246079375, 0.12453582049215875, 0.12453582049215875, 0.062267910246079375, 0.062267910246079375, 0.6226791024607937, 0.0616602880393172, 0.0616602880393172, 0.0616602880393172, 0.8015837445111236, 0.0616602880393172, 0.0616602880393172, 0.8138486381233522, 0.142603566680021, 0.014404400674749596, 0.004321320202424879, 0.007202200337374798, 0.015844840742224555, 0.0014404400674749595, 0.9644683317493774, 0.006143110393308136, 0.006143110393308136, 0.006143110393308136, 0.006143110393308136, 0.006143110393308136, 0.04112962954447402, 0.8637222204339545, 0.04112962954447402, 0.04112962954447402, 0.04112962954447402, 0.04112962954447402, 0.351113876264566, 0.175556938132283, 0.175556938132283, 0.175556938132283, 0.175556938132283, 0.175556938132283, 0.08360430623027904, 0.08360430623027904, 0.08360430623027904, 0.6688344498422323, 0.08360430623027904, 0.08360430623027904, 0.8633517866726507, 0.007993998024746766, 0.047963988148480595, 0.01598799604949353, 0.047963988148480595, 0.007993998024746766, 0.03472394275761146, 0.03472394275761146, 0.03472394275761146, 0.03472394275761146, 0.8680985689402865, 0.03472394275761146, 0.09252242398814814, 0.09252242398814814, 0.09252242398814814, 0.09252242398814814, 0.09252242398814814, 0.647656967917037, 0.059252944468911946, 0.059252944468911946, 0.7110353336269434, 0.059252944468911946, 0.11850588893782389, 0.059252944468911946, 0.07048024049202738, 0.7752826454123012, 0.07048024049202738, 0.07048024049202738, 0.07048024049202738, 0.07048024049202738, 0.22588015513860377, 0.22588015513860377, 0.22588015513860377, 0.22588015513860377, 0.22588015513860377, 0.22588015513860377, 0.4097119507096732, 0.5042608624119055, 0.03151630390074409, 0.03151630390074409, 0.012606521560297637, 0.012606521560297637, 0.03228288340033449, 0.7747892016080278, 0.09684865020100347, 0.03228288340033449, 0.03228288340033449, 0.03228288340033449, 0.23012568812215256, 0.23012568812215256, 0.23012568812215256, 0.23012568812215256, 0.23012568812215256, 0.23012568812215256, 0.22596916320738963, 0.22596916320738963, 0.22596916320738963, 0.22596916320738963, 0.22596916320738963, 0.22596916320738963, 0.724983791353894, 0.04166573513528126, 0.19999552864935005, 0.008333147027056252, 0.016666294054112504, 0.008333147027056252, 0.6924174486708955, 0.11540290811181593, 0.11540290811181593, 0.016486129730259418, 0.032972259460518835, 0.016486129730259418, 0.016486129730259418, 0.5192638636635281, 0.28847992425751556, 0.05769598485150312, 0.009615997475250519, 0.05769598485150312, 0.02884799242575156, 0.019231994950501038, 0.019231994950501038, 0.36414670445460967, 0.22759169028413104, 0.04551833805682621, 0.04551833805682621, 0.27311002834095727, 0.04551833805682621, 0.04551833805682621, 0.12359276626848262, 0.08239517751232174, 0.020598794378080435, 0.020598794378080435, 0.6797602144766544, 0.04119758875616087, 0.04119758875616087, 0.541278079476153, 0.17591537582974973, 0.04330224635809224, 0.09472366390832677, 0.11908117748475365, 0.013531951986903825, 0.01082556158952306, 0.837186188263003, 0.06796345439773456, 0.04633871890754629, 0.01235699170867901, 0.009267743781509259, 0.02162473549018827, 0.0030892479271697527, 0.7185869919051708, 0.06612763729188688, 0.05510636440990573, 0.03306381864594344, 0.059514873562698196, 0.05290210983350951, 0.011021272881981146, 0.5749109932910174, 0.14749025482335001, 0.1264202184200143, 0.018060031202859185, 0.09331016121477245, 0.003010005200476531, 0.03612006240571837, 0.9025214264187292, 0.029113594400604167, 0.029113594400604167, 0.029113594400604167, 0.029113594400604167, 0.029113594400604167, 0.23015647907799566, 0.23015647907799566, 0.23015647907799566, 0.23015647907799566, 0.23015647907799566, 0.23015647907799566, 0.08090284810873069, 0.08090284810873069, 0.08090284810873069, 0.08090284810873069, 0.08090284810873069, 0.08090284810873069, 0.6472227848698455, 0.34375276753243494, 0.08457409359924986, 0.14595851637289894, 0.013640982838588687, 0.3123785070036809, 0.06274852105750796, 0.03683065366418946, 0.08224326827827685, 0.08224326827827685, 0.08224326827827685, 0.08224326827827685, 0.6579461462262148, 0.08224326827827685, 0.2094441059824077, 0.2094441059824077, 0.2094441059824077, 0.2094441059824077, 0.2094441059824077, 0.2094441059824077, 0.21570896944455017, 0.21570896944455017, 0.21570896944455017, 0.21570896944455017, 0.21570896944455017, 0.21570896944455017, 0.4992605802496222, 0.12481514506240556, 0.10645997667087533, 0.1450058302930888, 0.03487481994390743, 0.08076274092273301, 0.009177584195765115, 0.10163878415256646, 0.440434731327788, 0.1524581762288497, 0.16939797358761077, 0.05081939207628323, 0.05081939207628323, 0.04234949339690269, 0.10182131011341097, 0.1272766376417637, 0.05091065505670549, 0.1272766376417637, 0.025455327528352743, 0.5600172056237603, 0.6672216560660051, 0.08479740110537858, 0.07140833777295037, 0.060250784995926884, 0.05132474277430808, 0.04463021110809399, 0.022315105554046994, 0.15229913563889733, 0.15229913563889733, 0.15229913563889733, 0.15229913563889733, 0.15229913563889733, 0.15229913563889733, 0.45689740691669195, 0.06423063820873723, 0.06423063820873723, 0.06423063820873723, 0.06423063820873723, 0.7707676585048469, 0.06423063820873723, 0.659421704256442, 0.06898565521452008, 0.11362343211803307, 0.03043484788875886, 0.08927555380702598, 0.020289898592505906, 0.018260908733255313, 0.3755166544907889, 0.2599730684936231, 0.014442948249645727, 0.04332884474893718, 0.028885896499291453, 0.014442948249645727, 0.2599730684936231, 0.09488507966358721, 0.09488507966358721, 0.6641955576451104, 0.09488507966358721, 0.09488507966358721, 0.09488507966358721, 0.08449716178697932, 0.08449716178697932, 0.6759772942958345, 0.08449716178697932, 0.08449716178697932, 0.08449716178697932, 0.08940356100146836, 0.08940356100146836, 0.7152284880117469, 0.08940356100146836, 0.08940356100146836, 0.08940356100146836, 0.5365879728413812, 0.08721068633087911, 0.13929484622293192, 0.1090133579135989, 0.11022461744597221, 0.01332385485610653, 0.003633778597119963, 0.1210009459545963, 0.18150141893189445, 0.06050047297729815, 0.1210009459545963, 0.06050047297729815, 0.06050047297729815, 0.4840037838183852, 0.8484614350335324, 0.07713285773032114, 0.020568762061418968, 0.015426571546064225, 0.02571095257677371, 0.010284381030709484, 0.46287266376870073, 0.3894008123768435, 0.06612466625267154, 0.02204155541755718, 0.04408311083511436, 0.007347185139185726, 0.007347185139185726, 0.7501026052721523, 0.07269716879289582, 0.06939275202958238, 0.02643533410650757, 0.031391959251477745, 0.038000792778104635, 0.00991325028994034, 0.8865404800165337, 0.03283483259320495, 0.03283483259320495, 0.03283483259320495, 0.03283483259320495, 0.03283483259320495, 0.2801142071356055, 0.15634281328498909, 0.19542851660623636, 0.06514283886874546, 0.21497136826686, 0.07165712275562, 0.019542851660623636, 0.8788436420990899, 0.03487474770234484, 0.04882464678328278, 0.013949899080937935, 0.006974949540468968, 0.006974949540468968, 0.006974949540468968, 0.0523579101362246, 0.0523579101362246, 0.0523579101362246, 0.0523579101362246, 0.785368652043369, 0.0523579101362246, 0.07955754843604781, 0.07955754843604781, 0.07955754843604781, 0.7160179359244303, 0.07955754843604781, 0.07955754843604781, 0.09874896989793087, 0.04937448494896544, 0.04937448494896544, 0.6418683043365506, 0.04937448494896544, 0.09874896989793087, 0.3061524368302822, 0.07653810920757055, 0.5357667644529939, 0.019134527301892638, 0.019134527301892638, 0.019134527301892638, 0.019134527301892638, 0.14389812039632593, 0.14389812039632593, 0.14389812039632593, 0.14389812039632593, 0.14389812039632593, 0.14389812039632593, 0.4316943611889778, 0.08222654273313988, 0.740038884598259, 0.08222654273313988, 0.08222654273313988, 0.08222654273313988, 0.08222654273313988, 0.4084129553971617, 0.08508603237440869, 0.051051619424645214, 0.06806882589952695, 0.3573613359725165, 0.017017206474881738, 0.017017206474881738, 0.17619924467109155, 0.02517132066729879, 0.20137056533839032, 0.10068528266919516, 0.02517132066729879, 0.07551396200189638, 0.37756981000948187, 0.07615179038926803, 0.07615179038926803, 0.7615179038926804, 0.07615179038926803, 0.07615179038926803, 0.07615179038926803, 0.3528166820808345, 0.33738095223979797, 0.1278960472543025, 0.033076563945078234, 0.057332710838135605, 0.022051042630052156, 0.0705633364161669, 0.11545190667602756, 0.11545190667602756, 0.11545190667602756, 0.11545190667602756, 0.11545190667602756, 0.11545190667602756, 0.5772595333801378, 0.890522316797842, 0.03180436845706579, 0.03180436845706579, 0.03180436845706579, 0.03180436845706579, 0.03180436845706579, 0.12004252781352408, 0.06002126390676204, 0.06002126390676204, 0.06002126390676204, 0.7202551668811444, 0.06002126390676204, 0.3484316451857871, 0.07292755364353684, 0.08508214591745965, 0.18839618024580349, 0.04659260371670409, 0.2127053647936491, 0.04659260371670409, 0.9450224777044032, 0.0147659762141313, 0.0147659762141313, 0.0147659762141313, 0.0147659762141313, 0.0147659762141313, 0.21574045251706844, 0.21574045251706844, 0.21574045251706844, 0.21574045251706844, 0.21574045251706844, 0.21574045251706844, 0.05871978884863643, 0.05871978884863643, 0.7633572550322735, 0.05871978884863643, 0.05871978884863643, 0.05871978884863643, 0.3677666010406033, 0.1419450039104083, 0.13549295827811703, 0.006452045632291286, 0.32260228161456433, 0.006452045632291286, 0.012904091264582572, 0.23009260178843427, 0.23009260178843427, 0.23009260178843427, 0.23009260178843427, 0.23009260178843427, 0.23009260178843427, 0.23009447442650294, 0.23009447442650294, 0.23009447442650294, 0.23009447442650294, 0.23009447442650294, 0.23009447442650294, 0.2300568330384308, 0.2300568330384308, 0.2300568330384308, 0.2300568330384308, 0.2300568330384308, 0.2300568330384308, 0.5855729040944052, 0.12129724441955536, 0.054374626808766195, 0.012547990802022969, 0.18821986203034452, 0.02927864520472026, 0.008365327201348647, 0.1487370892460327, 0.694106416481486, 0.04957902974867757, 0.04957902974867757, 0.04957902974867757, 0.04957902974867757, 0.13524580507723102, 0.13524580507723102, 0.13524580507723102, 0.13524580507723102, 0.13524580507723102, 0.13524580507723102, 0.5409832203089241, 0.8814261995179883, 0.01958724887817752, 0.01958724887817752, 0.01958724887817752, 0.01958724887817752, 0.01958724887817752, 0.01958724887817752, 0.0993175721855147, 0.04965878609275735, 0.04965878609275735, 0.7945405774841175, 0.04965878609275735, 0.04965878609275735, 0.6483699652378679, 0.10205823526892366, 0.08004567472072444, 0.042023979228380325, 0.0740422491166701, 0.02601484428423544, 0.028015986152253553, 0.0879353933182594, 0.0879353933182594, 0.0879353933182594, 0.0879353933182594, 0.0879353933182594, 0.7034831465460752, 0.8988528939350036, 0.06693585380367048, 0.009562264829095782, 0.009562264829095782, 0.009562264829095782, 0.009562264829095782, 0.050075387829897136, 0.050075387829897136, 0.050075387829897136, 0.751130817448457, 0.050075387829897136, 0.050075387829897136, 0.013611295609098533, 0.06805647804549267, 0.013611295609098533, 0.0816677736545912, 0.816677736545912, 0.013611295609098533, 0.22484071471308695, 0.19370892344512108, 0.13490442882785217, 0.12798625299052643, 0.0069181758373257525, 0.22484071471308695, 0.08647719796657191, 0.08631523854967807, 0.08631523854967807, 0.08631523854967807, 0.08631523854967807, 0.08631523854967807, 0.6905219083974246, 0.047748644488034266, 0.047748644488034266, 0.047748644488034266, 0.047748644488034266, 0.047748644488034266, 0.8117269562965825, 0.5687911624186598, 0.10569662545732576, 0.10569662545732576, 0.034037896333715074, 0.08867767729046822, 0.07345019735170095, 0.023289086965173473, 0.5096377232474218, 0.08589399830012728, 0.12884099745019092, 0.05439953225674728, 0.10020966468348183, 0.09734653140681092, 0.025768199490038186, 0.22582393541608445, 0.22582393541608445, 0.22582393541608445, 0.22582393541608445, 0.22582393541608445, 0.22582393541608445, 0.9376898779402189, 0.016167066861038255, 0.016167066861038255, 0.016167066861038255, 0.016167066861038255, 0.016167066861038255, 0.05740318843418398, 0.05740318843418398, 0.05740318843418398, 0.8036446380785758, 0.05740318843418398, 0.05740318843418398, 0.2893342595119062, 0.07233356487797656, 0.07233356487797656, 0.07233356487797656, 0.07233356487797656, 0.07233356487797656, 0.506334954145836, 0.8417203580641696, 0.11720156884437805, 0.0266367201919041, 0.00532734403838082, 0.00532734403838082, 0.00532734403838082, 0.00532734403838082, 0.5997705248106456, 0.07314274692812751, 0.08411415896734664, 0.08045702162094026, 0.14628549385625503, 0.007314274692812751, 0.0036571373464063753, 0.07848132714153272, 0.07848132714153272, 0.07848132714153272, 0.7063319442737945, 0.07848132714153272, 0.07848132714153272, 0.08124663784022652, 0.2031165946005663, 0.6499731027218122, 0.04062331892011326, 0.04062331892011326, 0.04062331892011326, 0.8475990017283087, 0.07471947122673796, 0.028019801710026736, 0.01867986780668449, 0.028019801710026736, 0.002334983475835561, 0.002334983475835561, 0.38203424734690294, 0.5383209848979087, 0.01736519306122286, 0.01736519306122286, 0.03473038612244572, 0.01736519306122286, 0.01736519306122286, 0.41620228672660653, 0.1811250692236158, 0.26205329164267815, 0.007707449754196417, 0.10790429655874983, 0.0038537248770982085, 0.019268624385491043, 0.4265526401527786, 0.1819957931318522, 0.10237263363666686, 0.07393579095981496, 0.04549894828296305, 0.1535589504550003, 0.017062105606111145, 0.6488789857064658, 0.11450805630114104, 0.05725402815057052, 0.01590389670849181, 0.12086961498453776, 0.03816935210038035, 0.006361558683396724, 0.2301045104554318, 0.2301045104554318, 0.2301045104554318, 0.2301045104554318, 0.2301045104554318, 0.2301045104554318, 0.34661853002369686, 0.1455797826099527, 0.2980919358203793, 0.09705318840663513, 0.07625607660521332, 0.02772948240189575, 0.013864741200947875, 0.48093119415662294, 0.40756880860730754, 0.06521100937716921, 0.008151376172146152, 0.024454128516438453, 0.008151376172146152, 0.008151376172146152, 0.049340436128276946, 0.838787414180708, 0.049340436128276946, 0.049340436128276946, 0.049340436128276946, 0.049340436128276946, 0.09082608208973239, 0.09082608208973239, 0.09082608208973239, 0.09082608208973239, 0.6357825746281267, 0.09082608208973239, 0.8793470957895874, 0.04581768434975806, 0.024671060803715877, 0.007048874515347393, 0.04053102846324751, 0.0017622186288368483, 0.0017622186288368483, 0.2300631745403585, 0.2300631745403585, 0.2300631745403585, 0.2300631745403585, 0.2300631745403585, 0.2300631745403585, 0.0853650516247849, 0.0853650516247849, 0.0853650516247849, 0.0853650516247849, 0.5975553613734943, 0.0853650516247849, 0.05748343501287553, 0.05748343501287553, 0.05748343501287553, 0.05748343501287553, 0.747284655167382, 0.05748343501287553, 0.8924380149753375, 0.027888687967979296, 0.027888687967979296, 0.027888687967979296, 0.027888687967979296, 0.027888687967979296, 0.7157695894719729, 0.06581789328477912, 0.06088155128842068, 0.013163578656955824, 0.1118904185841245, 0.01809992065331426, 0.013163578656955824, 0.8651365668389827, 0.03500552582585479, 0.0050007894036935415, 0.030004736422161248, 0.03500552582585479, 0.0050007894036935415, 0.030004736422161248, 0.0894074109175447, 0.0894074109175447, 0.7152592873403576, 0.0894074109175447, 0.0894074109175447, 0.0894074109175447, 0.10187285084620573, 0.10187285084620573, 0.10187285084620573, 0.10187285084620573, 0.6112371050772344, 0.10187285084620573, 0.8427912253263494, 0.12321509142198092, 0.019714414627516948, 0.004928603656879237, 0.004928603656879237, 0.004928603656879237, 0.08704960681724629, 0.6963968545379703, 0.08704960681724629, 0.08704960681724629, 0.08704960681724629, 0.08704960681724629, 0.09247311367207048, 0.6473117957044934, 0.09247311367207048, 0.09247311367207048, 0.09247311367207048, 0.09247311367207048, 0.6815046858416304, 0.1594265460452143, 0.04671840177149137, 0.04321452163862952, 0.031534921195756675, 0.02160726081931476, 0.015767460597878338, 0.11153135809976127, 0.05576567904988063, 0.05576567904988063, 0.6691881485985676, 0.05576567904988063, 0.05576567904988063, 0.061471991352294406, 0.061471991352294406, 0.061471991352294406, 0.6761919048752385, 0.061471991352294406, 0.061471991352294406, 0.08227483769324163, 0.08227483769324163, 0.08227483769324163, 0.08227483769324163, 0.08227483769324163, 0.5759238638526915, 0.20132184624522526, 0.20132184624522526, 0.03355364104087088, 0.48652779509262767, 0.01677682052043544, 0.050330461561306314, 0.13650681935450049, 0.027301363870900097, 0.08190409161270029, 0.027301363870900097, 0.7098354606434025, 0.027301363870900097, 0.08666371978843071, 0.08666371978843071, 0.08666371978843071, 0.08666371978843071, 0.6933097583074457, 0.08666371978843071, 0.3507725578963116, 0.05846209298271861, 0.0876931394740779, 0.0876931394740779, 0.029231046491359304, 0.0876931394740779, 0.32154151140495235, 0.29837212480842096, 0.626581462097684, 0.029837212480842094, 0.029837212480842094, 0.029837212480842094, 0.029837212480842094, 0.029837212480842094, 0.11684472055077001, 0.11684472055077001, 0.11684472055077001, 0.11684472055077001, 0.11684472055077001, 0.58422360275385, 0.06682937576921595, 0.7351231334613755, 0.06682937576921595, 0.06682937576921595, 0.06682937576921595, 0.06682937576921595, 0.09595807505044782, 0.09595807505044782, 0.09595807505044782, 0.09595807505044782, 0.6717065253531348, 0.09595807505044782, 0.21359410577411397, 0.05085573947002713, 0.05085573947002713, 0.08136918315204342, 0.020342295788010854, 0.5187285425942768, 0.08136918315204342, 0.3307115952768502, 0.09679363764200494, 0.14922352469809094, 0.3367611976294755, 0.008066136470167077, 0.042347216468377157, 0.03428107999821008, 0.44681503810511936, 0.12602475433734137, 0.0801975709419445, 0.17758033565716283, 0.03437038754654764, 0.12602475433734137, 0.005728397924424608, 0.2510036065579097, 0.11889644521164144, 0.14531787748089509, 0.35668933563492433, 0.052842864538507305, 0.06605358067313413, 0.026421432269253652, 0.15109940737683877, 0.09378583906148613, 0.01042064878460957, 0.1771510293383627, 0.005210324392304785, 0.5522943855843072, 0.005210324392304785, 0.05571788391927911, 0.08357682587891865, 0.027858941959639553, 0.22287153567711643, 0.027858941959639553, 0.6128967231120701, 0.04610838788625854, 0.04610838788625854, 0.04610838788625854, 0.8299509819526537, 0.04610838788625854, 0.04610838788625854, 0.10186961653242367, 0.10186961653242367, 0.10186961653242367, 0.10186961653242367, 0.611217699194542, 0.10186961653242367, 0.19618040530837982, 0.09809020265418991, 0.09809020265418991, 0.09809020265418991, 0.09809020265418991, 0.09809020265418991, 0.5885412159251394, 0.10310833010562633, 0.10310833010562633, 0.10310833010562633, 0.10310833010562633, 0.10310833010562633, 0.618649980633758, 0.6495885226652216, 0.1067610766417378, 0.08420591960475095, 0.058643408296165836, 0.05563605402456759, 0.0315772198517816, 0.013533094222192116, 0.12001682369795422, 0.25849777411867064, 0.4923767126069917, 0.06154708907587396, 0.018464126722762188, 0.01538677226896849, 0.03077354453793698, 0.8585684601943289, 0.033388773452001676, 0.04769824778857382, 0.009539649557714766, 0.033388773452001676, 0.014309474336572148, 0.004769824778857383, 0.04866016408444259, 0.04866016408444259, 0.04866016408444259, 0.04866016408444259, 0.04866016408444259, 0.04866016408444259, 0.8272227894355241, 0.596487055785816, 0.0898816111458079, 0.16886848154666936, 0.1007763518907543, 0.02723685186236603, 0.010894740744946412, 0.005447370372473206, 0.08427778665803914, 0.48881116261662705, 0.13484445865286263, 0.05056667199482349, 0.08427778665803914, 0.15170001598447047, 0.016855557331607828, 0.052299486862331004, 0.15689846058699303, 0.052299486862331004, 0.052299486862331004, 0.052299486862331004, 0.6798933292103031, 0.09253102000290148, 0.09253102000290148, 0.09253102000290148, 0.09253102000290148, 0.09253102000290148, 0.6477171400203103, 0.3205022146566169, 0.08950962751671283, 0.12704592292694725, 0.19345629172966966, 0.017324444035492804, 0.23676740181840167, 0.014437036696244005, 0.05312398840792419, 0.05312398840792419, 0.6906118493030144, 0.10624797681584838, 0.05312398840792419, 0.05312398840792419, 0.7589341326188056, 0.05336255619975977, 0.017787518733253255, 0.017787518733253255, 0.05336255619975977, 0.005929172911084419, 0.08893759366626627, 0.29900572045554635, 0.4924800101620764, 0.1231200025405191, 0.01758857179150273, 0.052765715374508186, 0.01758857179150273, 0.4959223722132852, 0.1983689488853141, 0.09918447444265704, 0.09918447444265704, 0.09918447444265704, 0.09918447444265704, 0.7185403787378803, 0.148169080316237, 0.033457534264956734, 0.031864318347577844, 0.04461004568660898, 0.02230502284330449, 0.0031864318347577846, 0.14297674993861123, 0.14297674993861123, 0.14297674993861123, 0.14297674993861123, 0.14297674993861123, 0.14297674993861123, 0.42893024981583366, 0.8874965103753154, 0.021130869294650367, 0.04226173858930073, 0.021130869294650367, 0.021130869294650367, 0.021130869294650367, 0.29295656305307677, 0.08480321562062748, 0.05396568266767204, 0.3006659462913156, 0.09251259885886634, 0.15418766476477724, 0.023128149714716585, 0.2189587228725273, 0.2189587228725273, 0.2189587228725273, 0.2189587228725273, 0.2189587228725273, 0.2189587228725273, 0.20687810136582044, 0.20687810136582044, 0.20687810136582044, 0.20687810136582044, 0.20687810136582044, 0.20687810136582044, 0.07051767556928179, 0.7756944312620997, 0.07051767556928179, 0.07051767556928179, 0.07051767556928179, 0.07051767556928179, 0.5816805960999996, 0.10781148979439646, 0.09276802610215511, 0.06268109871767237, 0.08524629425603442, 0.035101415281896524, 0.035101415281896524, 0.10502796198390629, 0.10502796198390629, 0.10502796198390629, 0.10502796198390629, 0.10502796198390629, 0.10502796198390629, 0.5251398099195315, 0.1651678885008917, 0.027527981416815284, 0.13763990708407642, 0.05505596283363057, 0.027527981416815284, 0.027527981416815284, 0.5505596283363057, 0.1915553152697812, 0.12189883698986076, 0.034828239139960214, 0.15672707612982098, 0.017414119569980107, 0.4701812283894629, 0.1600243342020635, 0.08001216710103175, 0.08001216710103175, 0.08001216710103175, 0.640097336808254, 0.08001216710103175, 0.3575273936393675, 0.522540036857537, 0.013751053601514133, 0.013751053601514133, 0.013751053601514133, 0.013751053601514133, 0.05500421440605653, 0.6458926446677719, 0.038230405759151605, 0.23541881441161777, 0.024145519426832594, 0.024145519426832594, 0.02012126618902716, 0.012072759713416297, 0.37615168953343797, 0.00800322743688166, 0.5042033285235445, 0.01600645487376332, 0.00800322743688166, 0.08803550180569826, 0.4904963551522602, 0.06765466967617383, 0.03382733483808691, 0.3044460135427822, 0.06765466967617383, 0.016913667419043456, 0.016913667419043456, 0.016913667419043456, 0.15531246648807118, 0.05177082216269039, 0.18119787756941635, 0.008628470360448398, 0.025885411081345195, 0.5177082216269039, 0.06039929252313879, 0.45937872383679085, 0.05568226955597465, 0.05568226955597465, 0.027841134777987324, 0.013920567388993662, 0.37585531950282886, 0.9297856830947785, 0.049456685270998856, 0.004945668527099886, 0.004945668527099886, 0.004945668527099886, 0.004945668527099886, 0.004945668527099886, 0.0419446397425209, 0.0419446397425209, 0.0419446397425209, 0.6711142358803344, 0.0419446397425209, 0.1677785589700836, 0.13443822341559455, 0.7730197846396687, 0.03360955585389864, 0.03360955585389864, 0.03360955585389864, 0.03360955585389864, 0.05256041460308775, 0.05256041460308775, 0.7884062190463164, 0.05256041460308775, 0.05256041460308775, 0.05256041460308775, 0.04241326330157736, 0.04241326330157736, 0.8482652660315473, 0.04241326330157736, 0.04241326330157736, 0.04241326330157736, 0.7758371645185983, 0.11461230839479293, 0.04628573992866637, 0.008816331414984072, 0.01983674568371416, 0.033061242806190265, 0.002204082853746018, 0.7868668550483328, 0.04035214641273501, 0.06341051579144073, 0.025940665551043934, 0.06629281196377895, 0.01152918468935286, 0.004323444258507322, 0.18508559007896547, 0.04627139751974137, 0.04627139751974137, 0.04627139751974137, 0.04627139751974137, 0.6940709627961206, 0.09250413978239512, 0.09250413978239512, 0.09250413978239512, 0.09250413978239512, 0.09250413978239512, 0.6475289784767658, 0.06662133413365157, 0.06662133413365157, 0.06662133413365157, 0.7328346754701672, 0.06662133413365157, 0.06662133413365157, 0.10965118046570799, 0.10965118046570799, 0.10965118046570799, 0.10965118046570799, 0.10965118046570799, 0.5482559023285399, 0.1366409153552354, 0.5010166896358631, 0.3188288024955493, 0.011386742946269617, 0.011386742946269617, 0.011386742946269617, 0.5712927630126309, 0.09020412047567856, 0.10147963553513838, 0.018792525099099698, 0.1691327258918973, 0.018792525099099698, 0.03006804015855952, 0.9467574942493442, 0.00937383657672618, 0.00937383657672618, 0.01874767315345236, 0.00937383657672618, 0.00937383657672618, 0.055151111049231566, 0.055151111049231566, 0.055151111049231566, 0.055151111049231566, 0.055151111049231566, 0.7721155546892419, 0.09239542068834576, 0.09239542068834576, 0.09239542068834576, 0.09239542068834576, 0.09239542068834576, 0.6467679448184203, 0.7073525920567338, 0.05462182178044277, 0.04915963960239849, 0.01911763762315497, 0.15294110098523975, 0.010924364356088553, 0.0027310910890221383, 0.08667289417757372, 0.08667289417757372, 0.08667289417757372, 0.08667289417757372, 0.6933831534205898, 0.08667289417757372, 0.10867375289441263, 0.10867375289441263, 0.10867375289441263, 0.10867375289441263, 0.10867375289441263, 0.5433687644720632, 0.5568027851819304, 0.07714737385050842, 0.08385584114185698, 0.09056430843320554, 0.13752357947264546, 0.04025080374809135, 0.013416934582697116, 0.4698570651216541, 0.05729964208800659, 0.04583971367040528, 0.01145992841760132, 0.40109749461604616, 0.01145992841760132, 0.18856814732686347, 0.18856814732686347, 0.18856814732686347, 0.18856814732686347, 0.18856814732686347, 0.18856814732686347, 0.18856814732686347, 0.02893933348808308, 0.8971193381305754, 0.02893933348808308, 0.02893933348808308, 0.02893933348808308, 0.02893933348808308, 0.0634263531016264, 0.0634263531016264, 0.0634263531016264, 0.1268527062032528, 0.0634263531016264, 0.0634263531016264, 0.5708371779146376, 0.08011070087724706, 0.08011070087724706, 0.7209963078952235, 0.08011070087724706, 0.08011070087724706, 0.08011070087724706, 0.1998798503751497, 0.11027853813801361, 0.0689240863362585, 0.18609503310789796, 0.055139269069006805, 0.35840524894854425, 0.013784817267251701, 0.6292590273668647, 0.09852698758353833, 0.09589960124797729, 0.05648880621456197, 0.09458590808019679, 0.01313693167780511, 0.011823238510024599, 0.41301401725590486, 0.009833667077521546, 0.22617434278299553, 0.009833667077521546, 0.009833667077521546, 0.3343446806357325, 0.0866084244605442, 0.0866084244605442, 0.0866084244605442, 0.0866084244605442, 0.6928673956843536, 0.0866084244605442, 0.028593484551395335, 0.85780453654186, 0.028593484551395335, 0.05718696910279067, 0.028593484551395335, 0.028593484551395335, 0.04145900286266301, 0.04145900286266301, 0.8291800572532603, 0.04145900286266301, 0.04145900286266301, 0.04145900286266301, 0.2107838009986469, 0.04215676019972938, 0.04215676019972938, 0.04215676019972938, 0.04215676019972938, 0.6745081631956701, 0.023112505819331577, 0.023112505819331577, 0.6240376571219526, 0.2080125523739842, 0.023112505819331577, 0.09245002327732631, 0.8191750363008694, 0.06992957626958642, 0.009989939467083772, 0.02996981840125132, 0.03995975786833509, 0.009989939467083772, 0.009989939467083772, 0.26729134703174895, 0.030672777528233483, 0.3242550767270397, 0.048200078972938334, 0.030672777528233483, 0.28920047383763, 0.008763650722352424, 0.7497478298391067, 0.09389905383341414, 0.050561028987223, 0.03178121822054017, 0.04189342401798477, 0.023113613251301943, 0.008667604969238228, 0.4886905059427065, 0.10337683779557254, 0.10024420634722185, 0.13783578372743005, 0.07831578620876707, 0.08144841765711776, 0.012530525793402731, 0.27439845405464575, 0.5144971013524609, 0.03429980675683072, 0.03429980675683072, 0.01714990337841536, 0.12004932364890752, 0.01714990337841536, 0.3050333970566615, 0.0581015994393641, 0.537439794814118, 0.014525399859841024, 0.0581015994393641, 0.007262699929920512, 0.02905079971968205, 0.08708000519976475, 0.08708000519976475, 0.08708000519976475, 0.08708000519976475, 0.08708000519976475, 0.08708000519976475, 0.696640041598118, 0.11841763517943972, 0.7815563921843022, 0.04736705407177589, 0.023683527035887945, 0.023683527035887945, 0.023683527035887945, 0.08224039949078384, 0.7401635954170545, 0.08224039949078384, 0.08224039949078384, 0.08224039949078384, 0.08224039949078384, 0.498835050727018, 0.07316247410662931, 0.08646474212601646, 0.059860206087242164, 0.21283628831019435, 0.04655793806785501, 0.019953402029080722, 0.3768207220665552, 0.11082962413722212, 0.05541481206861106, 0.022165924827444426, 0.38790368448027746, 0.03324888724116664, 0.011082962413722213, 0.8997687433843375, 0.013842596052066732, 0.013842596052066732, 0.013842596052066732, 0.041527788156200195, 0.013842596052066732, 0.10190381857649178, 0.10190381857649178, 0.10190381857649178, 0.10190381857649178, 0.6114229114589507, 0.10190381857649178, 0.6374286914152438, 0.08410517456173355, 0.057545645752765065, 0.0486924694831089, 0.026559528808968492, 0.1460774084493267, 0.004426588134828082, 0.6336332863156245, 0.10560554771927075, 0.10560554771927075, 0.10560554771927075, 0.10560554771927075, 0.10560554771927075, 0.6917179376603285, 0.06788743685260518, 0.10274855307421325, 0.028439331654469737, 0.06972223244321613, 0.021100149292025933, 0.01834795590610951, 0.24315808946450712, 0.47010563963138047, 0.11347377508343666, 0.08105269648816904, 0.01621053929763381, 0.04863161789290143, 0.0796031410160011, 0.0796031410160011, 0.0796031410160011, 0.0796031410160011, 0.0796031410160011, 0.6368251281280088, 0.0796031410160011, 0.08622975476087348, 0.08622975476087348, 0.08622975476087348, 0.08622975476087348, 0.08622975476087348, 0.08622975476087348, 0.6036082833261143, 0.6026113827162463, 0.1128237170283721, 0.08627696008051984, 0.07565825730137894, 0.0716762437592011, 0.03849279757438578, 0.011946040626533518, 0.21895678135099372, 0.21895678135099372, 0.21895678135099372, 0.21895678135099372, 0.21895678135099372, 0.21895678135099372, 0.05459071730439408, 0.05459071730439408, 0.05459071730439408, 0.05459071730439408, 0.7642700422615172, 0.05459071730439408, 0.05459071730439408, 0.030444604150554458, 0.1522230207527723, 0.030444604150554458, 0.730670499613307, 0.030444604150554458, 0.030444604150554458, 0.030444604150554458, 0.3773118364071092, 0.17085819007114378, 0.05695273002371459, 0.24916819385375133, 0.042714547517785945, 0.09254818628853621, 0.007119091252964324, 0.39694390362765214, 0.193856325027458, 0.11077504287283314, 0.09231253572736095, 0.036925014290944386, 0.13846880359104144, 0.032309387504576334, 0.5528507379521873, 0.11057014759043747, 0.11057014759043747, 0.11057014759043747, 0.11057014759043747, 0.11057014759043747, 0.04344358376174471, 0.8254280914731494, 0.04344358376174471, 0.04344358376174471, 0.04344358376174471, 0.04344358376174471, 0.6454685993285101, 0.07171873325872334, 0.04482420828670209, 0.0986132582307446, 0.10160153878319139, 0.014941402762234029, 0.020917963867127642, 0.21407089253295192, 0.03567848208882532, 0.03567848208882532, 0.6065341955100304, 0.03567848208882532, 0.03567848208882532, 0.03567848208882532, 0.5640074843104991, 0.04970180896222789, 0.19232439120166442, 0.0388970678834827, 0.11236930721895, 0.03025327502048654, 0.012965689294494232, 0.8831045698663013, 0.03798299225231403, 0.018991496126157015, 0.009495748063078508, 0.028487244189235525, 0.009495748063078508, 0.009495748063078508, 0.9051302616256284, 0.028285320675800887, 0.028285320675800887, 0.028285320675800887, 0.028285320675800887, 0.028285320675800887, 0.23005245557952075, 0.23005245557952075, 0.23005245557952075, 0.23005245557952075, 0.23005245557952075, 0.23005245557952075, 0.08995665549243606, 0.2998555183081202, 0.13493498323865408, 0.1499277591540601, 0.02998555183081202, 0.05997110366162404, 0.23988441464649615, 0.1772300526227099, 0.08861502631135496, 0.08861502631135496, 0.08861502631135496, 0.08861502631135496, 0.08861502631135496, 0.4430751315567748, 0.6919475366334134, 0.11071160586134614, 0.013838950732668267, 0.013838950732668267, 0.15222845805935095, 0.013838950732668267, 0.4144607915104702, 0.04362745173794423, 0.021813725868972116, 0.06544117760691635, 0.4144607915104702, 0.06544117760691635, 0.22581867775680917, 0.22581867775680917, 0.22581867775680917, 0.22581867775680917, 0.22581867775680917, 0.22581867775680917, 0.0865353066586233, 0.07571839332629539, 0.010816913332327913, 0.10816913332327911, 0.010816913332327913, 0.7030993666013142, 0.021983698628677828, 0.5715761643456235, 0.021983698628677828, 0.021983698628677828, 0.021983698628677828, 0.35173917805884525, 0.09677162387753734, 0.09677162387753734, 0.09677162387753734, 0.09677162387753734, 0.09677162387753734, 0.580629743265224, 0.13256652689765222, 0.08837768459843481, 0.04418884229921741, 0.04418884229921741, 0.04418884229921741, 0.6628326344882611, 0.3255000257414668, 0.07233333905365928, 0.03616666952682964, 0.03616666952682964, 0.07233333905365928, 0.03616666952682964, 0.4340000343219557, 0.2858315856177392, 0.04513130299227461, 0.04513130299227461, 0.21061274729728152, 0.04513130299227461, 0.060175070656366146, 0.3159191209459223, 0.5918130187569977, 0.0517836391412373, 0.19233923109602424, 0.02589181957061865, 0.03328948230508112, 0.07767545871185595, 0.03328948230508112, 0.8982096871355182, 0.03097274783225925, 0.03097274783225925, 0.03097274783225925, 0.03097274783225925, 0.03097274783225925, 0.14389860836535126, 0.14389860836535126, 0.14389860836535126, 0.14389860836535126, 0.14389860836535126, 0.14389860836535126, 0.4316958250960538, 0.05844276847093272, 0.05844276847093272, 0.7597559901221254, 0.05844276847093272, 0.05844276847093272, 0.05844276847093272, 0.3076652670785081, 0.6153305341570162, 0.025638772256542344, 0.025638772256542344, 0.025638772256542344, 0.025638772256542344, 0.546897907565276, 0.041275313778511395, 0.09286945600165064, 0.010318828444627849, 0.2682895395603241, 0.030956485333883545, 0.010318828444627849, 0.1483316411700666, 0.1483316411700666, 0.1483316411700666, 0.1483316411700666, 0.1483316411700666, 0.2966632823401332, 0.668094112341605, 0.17428542061085348, 0.029047570101808914, 0.029047570101808914, 0.08714271030542674, 0.029047570101808914, 0.029047570101808914, 0.024613048549568837, 0.07383914564870651, 0.7383914564870652, 0.1230652427478442, 0.024613048549568837, 0.024613048549568837, 0.024613048549568837, 0.04253960070442783, 0.04253960070442783, 0.8507920140885566, 0.04253960070442783, 0.04253960070442783, 0.04253960070442783, 0.0306951285430839, 0.0306951285430839, 0.0306951285430839, 0.8594635992063492, 0.0306951285430839, 0.0306951285430839, 0.04059083171947108, 0.04059083171947108, 0.04059083171947108, 0.8118166343894216, 0.04059083171947108, 0.08118166343894216, 0.03382574408085513, 0.03382574408085513, 0.03382574408085513, 0.6088633934553924, 0.03382574408085513, 0.3044316967276962, 0.5231015626381603, 0.08327985076826434, 0.062459888076198254, 0.15354722485398736, 0.04684491605714869, 0.12231728081588825, 0.010409981346033043, 0.06635085142771469, 0.06635085142771469, 0.7962102171325762, 0.06635085142771469, 0.06635085142771469, 0.06635085142771469, 0.5193447459346403, 0.08655745765577338, 0.14838421312418293, 0.016487134791575882, 0.09067924135366735, 0.07419210656209146, 0.06182675546840956, 0.9296636904588917, 0.022134849772830756, 0.022134849772830756, 0.022134849772830756, 0.022134849772830756, 0.022134849772830756, 0.26064993968604294, 0.08688331322868098, 0.017376662645736198, 0.19114328910309816, 0.017376662645736198, 0.20851995174883436, 0.22589661439457057, 0.10614324460084554, 0.10614324460084554, 0.10614324460084554, 0.10614324460084554, 0.10614324460084554, 0.10614324460084554, 0.6368594676050733, 0.9441734167399664, 0.013113519676943977, 0.013113519676943977, 0.013113519676943977, 0.013113519676943977, 0.013113519676943977, 0.5364465877450791, 0.20607399407280477, 0.09485945758906887, 0.04906523668400114, 0.071962347136535, 0.009813047336800228, 0.032710157789334095, 0.9239965935738969, 0.012486440453701309, 0.012486440453701309, 0.012486440453701309, 0.012486440453701309, 0.012486440453701309, 0.0406911550666365, 0.6917496361328206, 0.12207346519990951, 0.0406911550666365, 0.12207346519990951, 0.0406911550666365, 0.06270252009839186, 0.06270252009839186, 0.06270252009839186, 0.06270252009839186, 0.06270252009839186, 0.7524302411807023, 0.697041861363647, 0.049695292179772206, 0.06408077149496942, 0.06277300064813332, 0.061465229801297205, 0.05884968810762498, 0.006538854234180554, 0.030997454355602777, 0.15498727177801389, 0.030997454355602777, 0.21698218048921944, 0.030997454355602777, 0.55795417840085, 0.11684632290842269, 0.11684632290842269, 0.11684632290842269, 0.11684632290842269, 0.11684632290842269, 0.5842316145421134, 0.975936416558822, 0.00478400204195501, 0.00478400204195501, 0.00478400204195501, 0.00478400204195501, 0.00478400204195501, 0.00478400204195501, 0.04201106354612588, 0.8402212709225176, 0.04201106354612588, 0.04201106354612588, 0.04201106354612588, 0.04201106354612588, 0.35865048353218554, 0.19386512623361382, 0.13085896020768933, 0.048466281558403455, 0.1066258194284876, 0.07269942233760518, 0.08723930680512622, 0.40850098704215454, 0.22099233725231313, 0.08928983323325783, 0.05580614577078614, 0.07589635824826915, 0.06473512909411193, 0.08482534157159494, 0.15296389143517244, 0.15296389143517244, 0.15296389143517244, 0.15296389143517244, 0.15296389143517244, 0.15296389143517244, 0.45889167430551736, 0.0837887906859049, 0.04189439534295245, 0.04189439534295245, 0.7959935115160967, 0.04189439534295245, 0.04189439534295245, 0.36958851909655405, 0.05133173876341028, 0.4311866056126464, 0.11292982527950261, 0.010266347752682056, 0.010266347752682056, 0.010266347752682056, 0.17160515869467177, 0.6864206347786871, 0.0572017195648906, 0.0572017195648906, 0.0572017195648906, 0.0572017195648906, 0.2933971170999611, 0.2271461551741634, 0.15143077011610892, 0.03785769252902723, 0.028393269396770424, 0.2555394245709338, 0.009464423132256808, 0.05956356367483282, 0.7743263277728267, 0.05956356367483282, 0.05956356367483282, 0.05956356367483282, 0.05956356367483282, 0.15162282081679054, 0.6570322235394257, 0.05054094027226352, 0.05054094027226352, 0.05054094027226352, 0.05054094027226352, 0.08939644961542649, 0.08939644961542649, 0.7151715969234119, 0.08939644961542649, 0.08939644961542649, 0.08939644961542649, 0.07258553460905208, 0.03629276730452604, 0.07258553460905208, 0.798440880699573, 0.03629276730452604, 0.03629276730452604, 0.07795415075658627, 0.03897707537829313, 0.03897707537829313, 0.8185185829441558, 0.03897707537829313, 0.03897707537829313, 0.07164932151886662, 0.07164932151886662, 0.07164932151886662, 0.07164932151886662, 0.07164932151886662, 0.07164932151886662, 0.7164932151886662, 0.2133310827622422, 0.06787807178798615, 0.08727180658455362, 0.26666385345280275, 0.009696867398283735, 0.3490872263382145, 0.009696867398283735, 0.2835575518914225, 0.040508221698774646, 0.040508221698774646, 0.040508221698774646, 0.040508221698774646, 0.567115103782845, 0.07256611106019041, 0.07256611106019041, 0.07256611106019041, 0.7256611106019041, 0.07256611106019041, 0.07256611106019041, 0.6298615657000204, 0.09737737089350008, 0.09505886206270246, 0.06337257470846831, 0.06414541098540086, 0.027822105969571454, 0.02241225203104367, 0.034059021400898636, 0.034059021400898636, 0.8514755350224659, 0.034059021400898636, 0.034059021400898636, 0.034059021400898636, 0.6143775642395044, 0.09237145196607932, 0.1052604917752997, 0.11814953158452007, 0.04296346603073457, 0.023629906316904013, 0.004296346603073457, 0.28078323942829575, 0.03509790492853697, 0.1052937147856109, 0.03509790492853697, 0.03509790492853697, 0.4913706689995176, 0.9198957146567072, 0.01999773292731972, 0.01999773292731972, 0.01999773292731972, 0.01999773292731972, 0.01999773292731972, 0.7679272697038354, 0.023668991189501776, 0.08941618893811783, 0.021039103279557134, 0.07363686147844997, 0.002629887909944642, 0.018409215369612493, 0.206413441422669, 0.26833747384946965, 0.0206413441422669, 0.0412826882845338, 0.0206413441422669, 0.0206413441422669, 0.412826882845338, 0.23984288402098203, 0.11992144201049101, 0.11992144201049101, 0.11992144201049101, 0.11992144201049101, 0.11992144201049101, 0.47968576804196406, 0.5313702431588753, 0.05593370980619739, 0.28666026275676165, 0.03495856862887337, 0.020975141177324023, 0.03495856862887337, 0.027966854903098696, 0.9003890891085069, 0.05145080509191468, 0.02572540254595734, 0.02572540254595734, 0.02572540254595734, 0.02572540254595734, 0.14157263370603643, 0.07078631685301821, 0.07078631685301821, 0.07078631685301821, 0.07078631685301821, 0.6370768516771639, 0.09643679721847002, 0.3568161497083391, 0.15429887554955204, 0.15429887554955204, 0.08679311749662302, 0.12536783638401103, 0.019287359443694006, 0.23003778380094586, 0.23003778380094586, 0.23003778380094586, 0.23003778380094586, 0.23003778380094586, 0.23003778380094586, 0.24224286851590415, 0.048448573703180835, 0.09689714740636167, 0.43603716332862746, 0.09689714740636167, 0.048448573703180835, 0.11319207307267977, 0.028298018268169942, 0.028298018268169942, 0.7640464932405884, 0.028298018268169942, 0.056596036536339885, 0.9367353278702686, 0.017346950516116084, 0.017346950516116084, 0.017346950516116084, 0.017346950516116084, 0.017346950516116084, 0.07655914734510987, 0.0574193605088324, 0.5933333919246014, 0.1148387210176648, 0.019139786836277466, 0.15311829469021973, 0.048618145451836015, 0.048618145451836015, 0.048618145451836015, 0.048618145451836015, 0.5834177454220322, 0.048618145451836015, 0.24309072725918007, 0.23012332780617512, 0.23012332780617512, 0.23012332780617512, 0.23012332780617512, 0.23012332780617512, 0.23012332780617512, 0.5718112460126555, 0.08620270040391792, 0.05459504358914802, 0.16378513076744405, 0.03448108016156717, 0.08332927705712066, 0.008620270040391792, 0.05444877339491432, 0.7622828275288005, 0.05444877339491432, 0.05444877339491432, 0.05444877339491432, 0.05444877339491432, 0.17957087400372734, 0.08978543700186367, 0.08978543700186367, 0.08978543700186367, 0.08978543700186367, 0.6284980590130457, 0.061978569367516466, 0.061978569367516466, 0.7437428324101976, 0.061978569367516466, 0.061978569367516466, 0.061978569367516466, 0.061978569367516466, 0.11545855329201934, 0.11545855329201934, 0.11545855329201934, 0.11545855329201934, 0.11545855329201934, 0.11545855329201934, 0.5772927664600968, 0.30130013486033735, 0.06550002931746464, 0.4716002110857454, 0.06550002931746464, 0.039300017590478784, 0.039300017590478784, 0.013100005863492928, 0.9164192630661133, 0.011600243836279915, 0.02320048767255983, 0.02320048767255983, 0.011600243836279915, 0.005800121918139957, 0.011600243836279915, 0.14896857991787033, 0.07448428995893516, 0.07448428995893516, 0.07448428995893516, 0.07448428995893516, 0.6703586096304165, 0.565933884488523, 0.0943223140814205, 0.0943223140814205, 0.0943223140814205, 0.0943223140814205, 0.0943223140814205, 0.10509725396872077, 0.10509725396872077, 0.10509725396872077, 0.10509725396872077, 0.10509725396872077, 0.10509725396872077, 0.5254862698436039], \"Term\": [\"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action_recognition\", \"action_recognition\", \"action_recognition\", \"action_recognition\", \"action_recognition\", \"action_recognition\", \"action_recognition\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation_space\", \"activation_space\", \"activation_space\", \"activation_space\", \"activation_space\", \"activation_space\", \"active_noise\", \"active_noise\", \"active_noise\", \"active_noise\", \"active_noise\", \"active_noise\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"adjacency\", \"adjacency\", \"adjacency\", \"adjacency\", \"adjacency\", \"adjacency\", \"afferent\", \"afferent\", \"afferent\", \"afferent\", \"afferent\", \"afferent\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine\", \"affine_transformation\", \"affine_transformation\", \"affine_transformation\", \"affine_transformation\", \"affine_transformation\", \"affine_transformation\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"airline\", \"airline\", \"airline\", \"airline\", \"airline\", \"airline\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"allocation\", \"allocation\", \"allocation\", \"allocation\", \"allocation\", \"allocation\", \"allocation\", \"allocator\", \"allocator\", \"allocator\", \"allocator\", \"allocator\", \"allocator\", \"allocator\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"assume\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"auxiliary_action\", \"auxiliary_action\", \"auxiliary_action\", \"auxiliary_action\", \"auxiliary_action\", \"auxiliary_action\", \"axon\", \"axon\", \"axon\", \"axon\", \"axon\", \"axon\", \"axon_terminal\", \"axon_terminal\", \"axon_terminal\", \"axon_terminal\", \"axon_terminal\", \"axon_terminal\", \"baep\", \"baep\", \"baep\", \"baep\", \"baep\", \"baep\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"bcm\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"belief\", \"bgm\", \"bgm\", \"bgm\", \"bgm\", \"bgm\", \"bgm\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"boost\", \"boost\", \"boost\", \"boost\", \"boost\", \"boost\", \"boost\", \"budget\", \"budget\", \"budget\", \"budget\", \"budget\", \"budget\", \"budget_maintenance\", \"budget_maintenance\", \"budget_maintenance\", \"budget_maintenance\", \"budget_maintenance\", \"budget_maintenance\", \"calibrator\", \"calibrator\", \"calibrator\", \"calibrator\", \"calibrator\", \"calibrator\", \"capacitor\", \"capacitor\", \"capacitor\", \"capacitor\", \"capacitor\", \"capacitor\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"cascade\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cem\", \"cem\", \"cem\", \"cem\", \"cem\", \"cem\", \"chainboost\", \"chainboost\", \"chainboost\", \"chainboost\", \"chainboost\", \"chainboost\", \"chainboost\", \"change_point\", \"change_point\", \"change_point\", \"change_point\", \"change_point\", \"change_point\", \"channel_selection\", \"channel_selection\", \"channel_selection\", \"channel_selection\", \"channel_selection\", \"channel_selection\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"chunk\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circular_shift\", \"circular_shift\", \"circular_shift\", \"circular_shift\", \"circular_shift\", \"circular_shift\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"clbt\", \"clbt\", \"clbt\", \"clbt\", \"clbt\", \"clbt\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"co_occurrence\", \"co_occurrence\", \"co_occurrence\", \"co_occurrence\", \"co_occurrence\", \"co_occurrence\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"cochlear_implant\", \"cochlear_implant\", \"cochlear_implant\", \"cochlear_implant\", \"cochlear_implant\", \"cochlear_implant\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"coco\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"collaboration\", \"collaboration\", \"collaboration\", \"collaboration\", \"collaboration\", \"collaboration\", \"collective_excitation\", \"collective_excitation\", \"collective_excitation\", \"collective_excitation\", \"collective_excitation\", \"collective_excitation\", \"combo\", \"combo\", \"combo\", \"combo\", \"combo\", \"combo\", \"commute\", \"commute\", \"commute\", \"commute\", \"commute\", \"commute\", \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"complex_cell\", \"complex_cell\", \"complex_cell\", \"complex_cell\", \"complex_cell\", \"complex_cell\", \"compressive\", \"compressive\", \"compressive\", \"compressive\", \"compressive\", \"compressive\", \"compressive\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"compute\", \"computer_vision\", \"computer_vision\", \"computer_vision\", \"computer_vision\", \"computer_vision\", \"computer_vision\", \"computer_vision\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"contralateral\", \"contralateral\", \"contralateral\", \"contralateral\", \"contralateral\", \"contralateral\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convex_upper\", \"convex_upper\", \"convex_upper\", \"convex_upper\", \"convex_upper\", \"convex_upper\", \"convnet\", \"convnet\", \"convnet\", \"convnet\", \"convnet\", \"convnet\", \"convnet\", \"coordinate_descent\", \"coordinate_descent\", \"coordinate_descent\", \"coordinate_descent\", \"coordinate_descent\", \"coordinate_descent\", \"copula\", \"copula\", \"copula\", \"copula\", \"copula\", \"copula\", \"coreset\", \"coreset\", \"coreset\", \"coreset\", \"coreset\", \"coreset\", \"correspondence\", \"correspondence\", \"correspondence\", \"correspondence\", \"correspondence\", \"correspondence\", \"crf\", \"crf\", \"crf\", \"crf\", \"crf\", \"crf\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cuhk\", \"cuhk\", \"cuhk\", \"cuhk\", \"cuhk\", \"cuhk\", \"cuhk\", \"cuhk_dataset\", \"cuhk_dataset\", \"cuhk_dataset\", \"cuhk_dataset\", \"cuhk_dataset\", \"cuhk_dataset\", \"cuhk_dataset\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current_source\", \"current_source\", \"current_source\", \"current_source\", \"current_source\", \"current_source\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decorrelation\", \"decorrelation\", \"decorrelation\", \"decorrelation\", \"decorrelation\", \"decorrelation\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deepmask\", \"deepmask\", \"deepmask\", \"deepmask\", \"deepmask\", \"deepmask\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"density_separator\", \"density_separator\", \"density_separator\", \"density_separator\", \"density_separator\", \"density_separator\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"descriptor\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"detector\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"differential_privacy\", \"differential_privacy\", \"differential_privacy\", \"differential_privacy\", \"differential_privacy\", \"differential_privacy\", \"differentially_private\", \"differentially_private\", \"differentially_private\", \"differentially_private\", \"differentially_private\", \"differentially_private\", \"displacement\", \"displacement\", \"displacement\", \"displacement\", \"displacement\", \"displacement\", \"displacement\", \"distributed_neuron\", \"distributed_neuron\", \"distributed_neuron\", \"distributed_neuron\", \"distributed_neuron\", \"distributed_neuron\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"doublet\", \"doublet\", \"doublet\", \"doublet\", \"doublet\", \"doublet\", \"downstream\", \"downstream\", \"downstream\", \"downstream\", \"downstream\", \"downstream\", \"drain\", \"drain\", \"drain\", \"drain\", \"drain\", \"drain\", \"draw\", \"draw\", \"draw\", \"draw\", \"draw\", \"draw\", \"dualsgd\", \"dualsgd\", \"dualsgd\", \"dualsgd\", \"dualsgd\", \"dualsgd\", \"dynamic_route\", \"dynamic_route\", \"dynamic_route\", \"dynamic_route\", \"dynamic_route\", \"dynamic_route\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"ear\", \"east_african\", \"east_african\", \"east_african\", \"east_african\", \"east_african\", \"east_african\", \"ecol\", \"ecol\", \"ecol\", \"ecol\", \"ecol\", \"ecol\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eig\", \"eig\", \"eig\", \"eig\", \"eig\", \"eig\", \"eigensolver\", \"eigensolver\", \"eigensolver\", \"eigensolver\", \"eigensolver\", \"eigensolver\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"factorisation\", \"factorisation\", \"factorisation\", \"factorisation\", \"factorisation\", \"factorisation\", \"fastembe\", \"fastembe\", \"fastembe\", \"fastembe\", \"fastembe\", \"fastembe\", \"fcboost\", \"fcboost\", \"fcboost\", \"fcboost\", \"fcboost\", \"fcboost\", \"fcboost\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature_imputation\", \"feature_imputation\", \"feature_imputation\", \"feature_imputation\", \"feature_imputation\", \"feature_imputation\", \"fel\", \"fel\", \"fel\", \"fel\", \"fel\", \"fel\", \"fel_dx\", \"fel_dx\", \"fel_dx\", \"fel_dx\", \"fel_dx\", \"fel_dx\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fk\", \"fk\", \"fk\", \"fk\", \"fk\", \"fk\", \"fk\", \"fogd\", \"fogd\", \"fogd\", \"fogd\", \"fogd\", \"fogd\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"front_rule\", \"front_rule\", \"front_rule\", \"front_rule\", \"front_rule\", \"front_rule\", \"frugal\", \"frugal\", \"frugal\", \"frugal\", \"frugal\", \"frugal\", \"frugality\", \"frugality\", \"frugality\", \"frugality\", \"frugality\", \"frugality\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"fuse\", \"fuse\", \"fuse\", \"fuse\", \"fuse\", \"fuse\", \"fuse\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"glm\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"greedy_coordinate\", \"greedy_coordinate\", \"greedy_coordinate\", \"greedy_coordinate\", \"greedy_coordinate\", \"greedy_coordinate\", \"growth_cone\", \"growth_cone\", \"growth_cone\", \"growth_cone\", \"growth_cone\", \"growth_cone\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"heuristic\", \"hmdb\", \"hmdb\", \"hmdb\", \"hmdb\", \"hmdb\", \"hmdb\", \"hmdb\", \"hop_size\", \"hop_size\", \"hop_size\", \"hop_size\", \"hop_size\", \"hop_size\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"identification\", \"identification\", \"identification\", \"identification\", \"identification\", \"identification\", \"identification\", \"iearne\", \"iearne\", \"iearne\", \"iearne\", \"iearne\", \"iearne\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"inexact_matche\", \"inexact_matche\", \"inexact_matche\", \"inexact_matche\", \"inexact_matche\", \"inexact_matche\", \"inexact_matche\", \"influence_estimation\", \"influence_estimation\", \"influence_estimation\", \"influence_estimation\", \"influence_estimation\", \"influence_estimation\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"intervention\", \"intervention\", \"intervention\", \"intervention\", \"intervention\", \"intervention\", \"ip_iq\", \"ip_iq\", \"ip_iq\", \"ip_iq\", \"ip_iq\", \"ip_iq\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"ipsilateral\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kfel\", \"kfel\", \"kfel\", \"kfel\", \"kfel\", \"kfel\", \"ksk\", \"ksk\", \"ksk\", \"ksk\", \"ksk\", \"ksk\", \"ksn\", \"ksn\", \"ksn\", \"ksn\", \"ksn\", \"ksn\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"labeling\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"laplacian_smoothe\", \"laplacian_smoothe\", \"laplacian_smoothe\", \"laplacian_smoothe\", \"laplacian_smoothe\", \"laplacian_smoothe\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latch\", \"latch\", \"latch\", \"latch\", \"latch\", \"latch\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lateral\", \"lattice\", \"lattice\", \"lattice\", \"lattice\", \"lattice\", \"lattice\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"ld\", \"ld\", \"ld\", \"ld\", \"ld\", \"ld\", \"lds\", \"lds\", \"lds\", \"lds\", \"lds\", \"lds\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"legendre\", \"legendre\", \"legendre\", \"legendre\", \"legendre\", \"legendre\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"legislation\", \"lgmd\", \"lgmd\", \"lgmd\", \"lgmd\", \"lgmd\", \"lgmd\", \"lifetime\", \"lifetime\", \"lifetime\", \"lifetime\", \"lifetime\", \"lifetime\", \"lifetime\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_smoother\", \"linear_smoother\", \"linear_smoother\", \"linear_smoother\", \"linear_smoother\", \"linear_smoother\", \"localization\", \"localization\", \"localization\", \"localization\", \"localization\", \"localization\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logistic_regression\", \"logistic_regression\", \"logistic_regression\", \"logistic_regression\", \"logistic_regression\", \"logistic_regression\", \"logistic_regression\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"madhow\", \"madhow\", \"madhow\", \"madhow\", \"madhow\", \"madhow\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin_maximize\", \"margin_maximize\", \"margin_maximize\", \"margin_maximize\", \"margin_maximize\", \"margin_maximize\", \"marginal_diversity\", \"marginal_diversity\", \"marginal_diversity\", \"marginal_diversity\", \"marginal_diversity\", \"marginal_diversity\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maxl\", \"maxl\", \"maxl\", \"maxl\", \"maxl\", \"maxl\", \"mc\", \"mc\", \"mc\", \"mc\", \"mc\", \"mc\", \"merging\", \"merging\", \"merging\", \"merging\", \"merging\", \"merging\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"minimalist\", \"minimalist\", \"minimalist\", \"minimalist\", \"minimalist\", \"minimalist\", \"mistake_rate\", \"mistake_rate\", \"mistake_rate\", \"mistake_rate\", \"mistake_rate\", \"mistake_rate\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture_doublet\", \"mixture_doublet\", \"mixture_doublet\", \"mixture_doublet\", \"mixture_doublet\", \"mixture_doublet\", \"mnist_movie\", \"mnist_movie\", \"mnist_movie\", \"mnist_movie\", \"mnist_movie\", \"mnist_movie\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modular\", \"modularity\", \"modularity\", \"modularity\", \"modularity\", \"modularity\", \"modularity\", \"modulate\", \"modulate\", \"modulate\", \"modulate\", \"modulate\", \"modulate\", \"module\", \"module\", \"module\", \"module\", \"module\", \"module\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonic\", \"monotonicity\", \"monotonicity\", \"monotonicity\", \"monotonicity\", \"monotonicity\", \"monotonicity\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"movie\", \"movie\", \"movie\", \"movie\", \"movie\", \"movie\", \"movie\", \"msec_tone\", \"msec_tone\", \"msec_tone\", \"msec_tone\", \"msec_tone\", \"msec_tone\", \"multi_category\", \"multi_category\", \"multi_category\", \"multi_category\", \"multi_category\", \"multi_category\", \"multi_core\", \"multi_core\", \"multi_core\", \"multi_core\", \"multi_core\", \"multi_core\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuronal\", \"neuronal\", \"neuronal\", \"neuronal\", \"neuronal\", \"neuronal\", \"neurotropin\", \"neurotropin\", \"neurotropin\", \"neurotropin\", \"neurotropin\", \"neurotropin\", \"nogd\", \"nogd\", \"nogd\", \"nogd\", \"nogd\", \"nogd\", \"normalized_correlation\", \"normalized_correlation\", \"normalized_correlation\", \"normalized_correlation\", \"normalized_correlation\", \"normalized_correlation\", \"normalized_correlation\", \"novel_categorie\", \"novel_categorie\", \"novel_categorie\", \"novel_categorie\", \"novel_categorie\", \"novel_categorie\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"optical_flow\", \"optical_flow\", \"optical_flow\", \"optical_flow\", \"optical_flow\", \"optical_flow\", \"optical_flow\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"oscillation\", \"oscillation\", \"oscillation\", \"oscillation\", \"oscillation\", \"oscillation\", \"oscillation_re\", \"oscillation_re\", \"oscillation_re\", \"oscillation_re\", \"oscillation_re\", \"oscillation_re\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"owl\", \"owl\", \"owl\", \"owl\", \"owl\", \"owl\", \"page\", \"page\", \"page\", \"page\", \"page\", \"page\", \"page\", \"pairwise\", \"pairwise\", \"pairwise\", \"pairwise\", \"pairwise\", \"pairwise\", \"pairwise_distance\", \"pairwise_distance\", \"pairwise_distance\", \"pairwise_distance\", \"pairwise_distance\", \"pairwise_distance\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"partial_occlusion\", \"partial_occlusion\", \"partial_occlusion\", \"partial_occlusion\", \"partial_occlusion\", \"partial_occlusion\", \"partial_occlusion\", \"particle\", \"particle\", \"particle\", \"particle\", \"particle\", \"particle\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile\", \"percentile_percentile\", \"percentile_percentile\", \"percentile_percentile\", \"percentile_percentile\", \"percentile_percentile\", \"percentile_percentile\", \"percept\", \"percept\", \"percept\", \"percept\", \"percept\", \"percept\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"permanent\", \"permanent\", \"permanent\", \"permanent\", \"permanent\", \"permanent\", \"permanent\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"pixel\", \"pixel\", \"pixel\", \"pixel\", \"pixel\", \"pixel\", \"pixel\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"presynaptic\", \"pretraine\", \"pretraine\", \"pretraine\", \"pretraine\", \"pretraine\", \"pretraine\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"private\", \"private\", \"private\", \"private\", \"private\", \"private\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production_system\", \"production_system\", \"production_system\", \"production_system\", \"production_system\", \"production_system\", \"programmable\", \"programmable\", \"programmable\", \"programmable\", \"programmable\", \"programmable\", \"projection_re\", \"projection_re\", \"projection_re\", \"projection_re\", \"projection_re\", \"projection_re\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"prototype\", \"quasi_classe\", \"quasi_classe\", \"quasi_classe\", \"quasi_classe\", \"quasi_classe\", \"quasi_classe\", \"quasi_classes\", \"quasi_classes\", \"quasi_classes\", \"quasi_classes\", \"quasi_classes\", \"quasi_classes\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"raster\", \"raster\", \"raster\", \"raster\", \"raster\", \"raster\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rd\", \"rd\", \"rd\", \"rd\", \"rd\", \"rd\", \"recursion\", \"recursion\", \"recursion\", \"recursion\", \"recursion\", \"recursion\", \"recursion\", \"repulsive\", \"repulsive\", \"repulsive\", \"repulsive\", \"repulsive\", \"repulsive\", \"request\", \"request\", \"request\", \"request\", \"request\", \"request\", \"request\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"resnet\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"rff\", \"rff\", \"rff\", \"rff\", \"rff\", \"rff\", \"rhythm\", \"rhythm\", \"rhythm\", \"rhythm\", \"rhythm\", \"rhythm\", \"rigid\", \"rigid\", \"rigid\", \"rigid\", \"rigid\", \"rigid\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"robot\", \"robot\", \"robot\", \"robot\", \"robot\", \"robot\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"sect\", \"sect\", \"sect\", \"sect\", \"sect\", \"sect\", \"sect\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation\", \"segmentation_proposal\", \"segmentation_proposal\", \"segmentation_proposal\", \"segmentation_proposal\", \"segmentation_proposal\", \"segmentation_proposal\", \"select\", \"select\", \"select\", \"select\", \"select\", \"select\", \"select\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"separate_calibrator\", \"separate_calibrator\", \"separate_calibrator\", \"separate_calibrator\", \"separate_calibrator\", \"separate_calibrator\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"ser\", \"ser\", \"ser\", \"ser\", \"ser\", \"ser\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shelf\", \"shelf\", \"shelf\", \"shelf\", \"shelf\", \"shelf\", \"shelf\", \"short_live\", \"short_live\", \"short_live\", \"short_live\", \"short_live\", \"short_live\", \"short_live\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"sidestep\", \"sidestep\", \"sidestep\", \"sidestep\", \"sidestep\", \"sidestep\", \"sift\", \"sift\", \"sift\", \"sift\", \"sift\", \"sift\", \"sift\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"sigmoidal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"singular_vector\", \"singular_vector\", \"singular_vector\", \"singular_vector\", \"singular_vector\", \"singular_vector\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"software\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse_representation\", \"sparse_representation\", \"sparse_representation\", \"sparse_representation\", \"sparse_representation\", \"sparse_representation\", \"sparsification\", \"sparsification\", \"sparsification\", \"sparsification\", \"sparsification\", \"sparsification\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"spielman\", \"spielman\", \"spielman\", \"spielman\", \"spielman\", \"spielman\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike_count\", \"spike_count\", \"spike_count\", \"spike_count\", \"spike_count\", \"spike_count\", \"spike_time\", \"spike_time\", \"spike_time\", \"spike_time\", \"spike_time\", \"spike_time\", \"spike_train\", \"spike_train\", \"spike_train\", \"spike_train\", \"spike_train\", \"spike_train\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stack\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stick_breake\", \"stick_breake\", \"stick_breake\", \"stick_breake\", \"stick_breake\", \"stick_breake\", \"stream_convnet\", \"stream_convnet\", \"stream_convnet\", \"stream_convnet\", \"stream_convnet\", \"stream_convnet\", \"stream_convnet\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"structured_prediction\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"subset\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"suppress\", \"symmetric\", \"symmetric\", \"symmetric\", \"symmetric\", \"symmetric\", \"symmetric\", \"symmetric\", \"symmetry\", \"symmetry\", \"symmetry\", \"symmetry\", \"symmetry\", \"symmetry\", \"symmetry\", \"symnet\", \"symnet\", \"symnet\", \"symnet\", \"symnet\", \"symnet\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tally\", \"tally\", \"tally\", \"tally\", \"tally\", \"tally\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"tempered_transition\", \"tempered_transition\", \"tempered_transition\", \"tempered_transition\", \"tempered_transition\", \"tempered_transition\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal_convnet\", \"temporal_convnet\", \"temporal_convnet\", \"temporal_convnet\", \"temporal_convnet\", \"temporal_convnet\", \"temporal_convnet\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"tier\", \"tier\", \"tier\", \"tier\", \"tier\", \"tier\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"tonic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"trajectory_stacke\", \"trajectory_stacke\", \"trajectory_stacke\", \"trajectory_stacke\", \"trajectory_stacke\", \"trajectory_stacke\", \"trajectory_stacke\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"translation\", \"transparency\", \"transparency\", \"transparency\", \"transparency\", \"transparency\", \"transparency\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"triple\", \"triple\", \"triple\", \"triple\", \"triple\", \"triple\", \"triplet\", \"triplet\", \"triplet\", \"triplet\", \"triplet\", \"triplet\", \"ttl\", \"ttl\", \"ttl\", \"ttl\", \"ttl\", \"ttl\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv_denoise\", \"tv_denoise\", \"tv_denoise\", \"tv_denoise\", \"tv_denoise\", \"tv_denoise\", \"ucf\", \"ucf\", \"ucf\", \"ucf\", \"ucf\", \"ucf\", \"ucf\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unit\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"uran\", \"uran\", \"uran\", \"uran\", \"uran\", \"uran\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"validity\", \"validity\", \"validity\", \"validity\", \"validity\", \"validity\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variability\", \"variability\", \"variability\", \"variability\", \"variability\", \"variability\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video\", \"video_frame\", \"video_frame\", \"video_frame\", \"video_frame\", \"video_frame\", \"video_frame\", \"video_frame\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"virtual_point\", \"virtual_point\", \"virtual_point\", \"virtual_point\", \"virtual_point\", \"virtual_point\", \"visible_neuron\", \"visible_neuron\", \"visible_neuron\", \"visible_neuron\", \"visible_neuron\", \"visible_neuron\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vlt\", \"vlt\", \"vlt\", \"vlt\", \"vlt\", \"vlt\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"vote\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"weak_learner\", \"weak_learner\", \"weak_learner\", \"weak_learner\", \"weak_learner\", \"weak_learner\", \"weak_learner\", \"weighing\", \"weighing\", \"weighing\", \"weighing\", \"weighing\", \"weighing\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"weighted_trace\", \"weights_toward\", \"weights_toward\", \"weights_toward\", \"weights_toward\", \"weights_toward\", \"weights_toward\", \"whole_image\", \"whole_image\", \"whole_image\", \"whole_image\", \"whole_image\", \"whole_image\", \"whole_image\", \"wider_search\", \"wider_search\", \"wider_search\", \"wider_search\", \"wider_search\", \"wider_search\", \"wider_search\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"window\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"ws\", \"ws\", \"ws\", \"ws\", \"ws\", \"ws\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zero\", \"zorn\", \"zorn\", \"zorn\", \"zorn\", \"zorn\", \"zorn\", \"zorn\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 1, 6, 4, 3, 7, 5, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el4641333902402475684647474299\", ldavis_el4641333902402475684647474299_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el4641333902402475684647474299\", ldavis_el4641333902402475684647474299_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el4641333902402475684647474299\", ldavis_el4641333902402475684647474299_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "!pip install pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7EfkmvNd7aJ"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "\n",
        "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
        "\n",
        "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
        "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
        "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
        "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
        "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
        "7. http://palmetto.aksw.org/palmetto-webapp/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

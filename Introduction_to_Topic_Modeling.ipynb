{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13-1550/13-1550/blob/main/Introduction_to_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dTAjyCS8JIp"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dke0Nzeh8JIr"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KXnjskB8JIr"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAFQicS48JIr"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "ZgMwjmWa8JIs",
        "outputId": "ff417353-a1cc-4492-fb64-8eeba00d6ce7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82eff103-f870-471f-8a00-f18e221673c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82eff103-f870-471f-8a00-f18e221673c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82eff103-f870-471f-8a00-f18e221673c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82eff103-f870-471f-8a00-f18e221673c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2ca0ee58-0fd2-40b9-ac57-429afe5dd8cc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ca0ee58-0fd2-40b9-ac57-429afe5dd8cc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2ca0ee58-0fd2-40b9-ac57-429afe5dd8cc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"temp/NIPS Papers/papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLVOEWhb8JIs"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WU9qF_cr8JIt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "040d1448-c357-4d20-fd5f-523e71875645"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "2734  1990  Generalization Dynamics in LMS Trained Linear ...   \n",
              "5914  2016         Adaptive Skills Adaptive Partitions (ASAP)   \n",
              "4894  2014                              Non-convex Robust PCA   \n",
              "1080  2001  Gaussian Process Regression with Mismatched Mo...   \n",
              "4807  2014  Semi-supervised Learning with Deep Generative ...   \n",
              "\n",
              "                                               abstract  \\\n",
              "2734                                   Abstract Missing   \n",
              "5914  We introduce the Adaptive Skills, Adaptive Par...   \n",
              "4894  We propose a new provable method for robust PC...   \n",
              "1080                                   Abstract Missing   \n",
              "4807  The ever-increasing size of modern data sets c...   \n",
              "\n",
              "                                             paper_text  \n",
              "2734  Generalization Dynamics in\\nLMS Trained Linear...  \n",
              "5914  Adaptive Skills Adaptive Partitions (ASAP)\\n\\n...  \n",
              "4894  Provable Non-convex Robust PCA\\nPraneeth Netra...  \n",
              "1080  Gaussian Process Regression with\\nMismatched M...  \n",
              "4807  Semi-supervised Learning with\\nDeep Generative...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad43668b-3f66-4d91-968b-c8542c0218d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2734</th>\n",
              "      <td>1990</td>\n",
              "      <td>Generalization Dynamics in LMS Trained Linear ...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Generalization Dynamics in\\nLMS Trained Linear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5914</th>\n",
              "      <td>2016</td>\n",
              "      <td>Adaptive Skills Adaptive Partitions (ASAP)</td>\n",
              "      <td>We introduce the Adaptive Skills, Adaptive Par...</td>\n",
              "      <td>Adaptive Skills Adaptive Partitions (ASAP)\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4894</th>\n",
              "      <td>2014</td>\n",
              "      <td>Non-convex Robust PCA</td>\n",
              "      <td>We propose a new provable method for robust PC...</td>\n",
              "      <td>Provable Non-convex Robust PCA\\nPraneeth Netra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>2001</td>\n",
              "      <td>Gaussian Process Regression with Mismatched Mo...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Gaussian Process Regression with\\nMismatched M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>2014</td>\n",
              "      <td>Semi-supervised Learning with Deep Generative ...</td>\n",
              "      <td>The ever-increasing size of modern data sets c...</td>\n",
              "      <td>Semi-supervised Learning with\\nDeep Generative...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad43668b-3f66-4d91-968b-c8542c0218d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ad43668b-3f66-4d91-968b-c8542c0218d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ad43668b-3f66-4d91-968b-c8542c0218d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e98dfa93-0cb0-4107-aafe-264725211b4f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e98dfa93-0cb0-4107-aafe-264725211b4f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e98dfa93-0cb0-4107-aafe-264725211b4f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          2011,\n          1996,\n          2004\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Measuring the reliability of MCMC inference with bidirectional Monte Carlo\",\n          \"Learning and Forecasting Opinion Dynamics in Social Networks\",\n          \"Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"We propose to train an ensemble with the help of a reservoir in which the learning algorithm can store a limited number of samples. This novel approach lies in the area between offline and online ensemble approaches and can be seen either as a restriction of the former or an enhancement of the latter.  We identify some basic strategies that can be used to populate this reservoir and present our main contribution, dubbed Greedy Edge Expectation Maximization (GEEM), that maintains the reservoir content in the case of Boosting by viewing the samples through their projections into the weak classifier response space.  We propose an efficient algorithmic implementation which makes it tractable in practice, and demonstrate its efficiency experimentally on several compute-vision data-sets, on which it outperforms both online and offline methods in a memory constrained setting.\",\n          \"We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements.  With noisy measurements we show all local minima are very close to a global optimum.  Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\\\\em  from random initialization}.\",\n          \"Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Measuring the reliability of MCMC inference with\\nbidirectional Monte Carlo\\nRoger B. Grosse\\n\\nDepartment of Computer Science\\nUniversity of Toronto\\n\\nSiddharth Ancha\\n\\nDepartment of Computer Science\\nUniversity of Toronto\\n\\nDaniel M. Roy\\n\\nDepartment of Statistics\\nUniversity of Toronto\\n\\nAbstract\\nMarkov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic\\ninference, but it is notoriously hard to measure the quality of approximate posterior\\nsamples. This challenge is particularly salient in black box inference methods,\\nwhich can hide details and obscure inference failures. In this work, we extend\\nthe recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate\\nMCMC-based posterior inference algorithms. By running annealed importance\\nsampling (AIS) chains both from prior to posterior and vice versa on simulated data,\\nwe upper bound in expectation the symmetrized KL divergence between the true\\nposterior distribution and the distribution of approximate samples. We integrate\\nour method into two probabilistic programming languages, WebPPL [GS] and Stan\\n[CGHL+ p], and validate it on several models and datasets. As an example of how\\nour method be used to guide the design of inference algorithms, we apply it to\\nstudy the effectiveness of different model representations in WebPPL and Stan.\\n\\n1\\n\\nIntroduction\\n\\nMarkov chain Monte Carlo (MCMC) is one of the most important classes of probabilistic inference\\nmethods and underlies a variety of approaches to automatic inference [e.g. LTBS00; GMRB+08;\\nGS; CGHL+ p]. Despite its widespread use, it is still difficult to rigorously validate the effectiveness\\nof an MCMC inference algorithm. There are various heuristics for diagnosing convergence, but\\nreliable quantitative measures are hard to find. This creates difficulties both for end users of automatic\\ninference systems and for experienced researchers who develop models and algorithms.\\nIn this paper, we extend the recently proposed bidirectional Monte Carlo (BDMC) [GGA15] method\\nto evaluate certain kinds of MCMC-based inference algorithms by bounding the symmetrized KL\\ndivergence (Jeffreys divergence) between the distribution of approximate samples and the true\\nposterior distribution. Specifically, our method is applicable to algorithms which can be viewed as\\nimportance sampling over an extended state space, such as annealed importance sampling (AIS;\\n[Nea01]) or sequential Monte Carlo (SMC; [MDJ06]). BDMC was proposed as a method for\\naccurately estimating the log marginal likelihood (log-ML) on simulated data by sandwiching the true\\nvalue between stochastic upper and lower bounds which converge in the limit of infinite computation.\\nThese log-likelihood values were used to benchmark marginal likelihood estimators. We show that it\\ncan also be used to measure the accuracy of approximate posterior samples obtained from algorithms\\nlike AIS or SMC. More precisely, we refine the analysis of [GGA15] to derive an estimator which\\nupper bounds in expectation the Jeffreys divergence between the distribution of approximate samples\\nand the true posterior distribution. We show that this upper bound is quite accurate on some toy\\ndistributions for which both the true Jeffreys divergence and the upper bound can be computed exactly.\\nWe refer to our method of bounding the Jeffreys divergence by sandwiching the log-ML as Bounding\\nDivergences with REverse Annealing (BREAD).\\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\\n\\n\\fWhile our method is only directly applicable to certain algorithms such as AIS or SMC, these\\nalgorithms involve many of the same design choices as traditional MCMC methods, such as the\\nchoice of model representation (e.g. whether to collapse out certain variables), or the choice of\\nMCMC transition operators. Therefore, the ability to evaluate AIS-based inference should also yield\\ninsights which inform the design of MCMC inference algorithms more broadly.\\nOne additional hurdle must be overcome to use BREAD to evaluate posterior inference: the method\\nyields rigorous bounds only for simulated data because it requires an exact posterior sample. One\\nwould like to be sure that the results on simulated data accurately reflect the accuracy of posterior\\ninference on the real-world data of interest. We present a protocol for using BREAD to diagnose\\ninference quality on real-world data. Specifically, we infer hyperparameters on the real data, simulate\\ndata from those hyperparameters, measure inference quality on the simulated data, and validate the\\nconsistency of the inference algorithm?s behavior between the real and simulated data. (This protocol\\nis somewhat similar in spirit to the parametric bootstrap [ET98].)\\nWe integrate BREAD into the tool chains of two probabilistic programming languages: WebPPL\\n[GS] and Stan [CGHL+ p]. Both probabilistic programming systems can be used as automatic\\ninference software packages, where the user provides a program specifying a joint probabilistic model\\nover observed and unobserved quantities. In principle, probabilistic programming has the potential to\\nput the power of sophisticated probabilistic modeling and efficient statistical inference into the hands\\nof non-experts, but realizing this vision is challenging because it is difficult for a non-expert user\\nto judge the reliability of results produced by black-box inference. We believe BREAD provides a\\nrigorous, general, and automatic procedure for monitoring the quality of posterior inference, so that\\nthe user of a probabilistic programming language can have confidence in the accuracy of the results.\\nOur approach to evaluating probabilistic programming inference is closely related to independent\\nwork [CTM16] that is also based on the ideas of BDMC. We discuss the relationships between both\\nmethods in Section 4.\\nIn summary, this work includes four main technical contributions. First, we show that BDMC yields\\nan estimator which upper bounds in expectation the Jeffreys divergence of approximate samples\\nfrom the true posterior. Second, we present a technique for exactly computing both the true Jeffreys\\ndivergence and the upper bound on small examples, and show that the upper bound is often a\\ngood match in practice. Third, we propose a protocol for using BDMC to evaluate the accuracy\\nof approximate inference on real-world datasets. Finally, we extend both WebPPL and Stan to\\nimplement BREAD, and validate BREAD on a variety of probabilistic models in both frameworks.\\nAs an example of how BREAD can be used to guide modeling and algorithmic decisions, we use\\nit to analyze the effectiveness of different representations of a matrix factorization model in both\\nWebPPL and Stan.\\n\\n2\\n\\nBackground\\n\\n2.1\\n\\nAnnealed Importance Sampling\\n\\nAnnealed importance sampling (AIS; [Nea01]) is a Monte Carlo algorithm commonly used to estimate\\n(ratios of) normalizing constants. More carefully, fix a sequence of T distributions p1 , . . . , pT , with\\npt (x) = ft (x)/Zt . The final distribution in the sequence, pT , is called the target distribution; the\\nfirst distribution, p1 , is called the initial distribution. It is required that one can obtain one or more\\nexact samples from p1 .1 Given a sequence of reversible MCMC transition operators T1 , . . . , TT ,\\nwhere Tt leaves pt invariant, AIS produces a (nonnegative) unbiased estimate of ZT /Z1 as follows:\\nfirst, we sample a random initial state x1 from p1 and set the initial weight w1 = 1. For every stage\\nt 2 we update the weight w and sample the state xt according to\\nwt\\n\\nwt\\n\\n1\\n\\nft (xt 1 )\\nft 1 (xt 1 )\\n\\nxt\\n\\nsample from Tt (x | xt\\n\\n1) .\\n\\n(1)\\n\\nNeal [Nea01] justified AIS by showing that it is a simple importance sampler over an extended state\\nspace (see Appendix A for a derivation in our notation). From this analysis, it follows that the weight\\nwT is an unbiased estimate of the ratio ZT /Z1 . Two trivial facts are worth highlighting: when Z1\\n1\\n\\nTraditionally, this has meant having access to an exact sampler. However, in this work, we sometimes have\\naccess to a sample from p1 , but not a sampler.\\n\\n2\\n\\n\\fis known, Z1 wT is an unbiased estimate of ZT , and when ZT is known, wT /ZT is an unbiased\\nestimate of 1/Z1 . In practice, it is common to repeat the AIS procedure to produce K independent\\nestimates and combine these by simple averaging to reduce the variance of the overall estimate.\\nIn most applications of AIS, the normalization constant ZT for the target distribution pT is the\\nfocus of attention, and the initial distribution p1 is chosen to have a known normalization constant\\nZ1 . Any sequence of intermediate distributions satisfying a mild domination criterion suffices to\\nproduce a valid estimate, but in typical applications, the intermediate distributions are simply defined\\nto be geometric averages ft (x) = f1 (x)1 t fT (x) t , where the t are monotonically increasing\\nparameters with 1 = 0 and T = 1. (An alternative approach is to average moments [GMS13].)\\nIn the setting of Bayesian posterior inference over parameters ? and latent variables z given some\\nfixed observation y, we take f1 (?, z) = p(?, z) to be the prior distribution (hence Z1 = 1), and we\\ntake fT (?, z) = p(?, z, y) = p(?, z) p(y | ?, z). This can be viewed as the unnormalized posterior\\ndistribution, whose normalizing constant ZT = p(y) is the marginal likelihood. Using geometric\\naveraging, the intermediate distributions are then\\nft (?, z) = p(?, z) p(y | ?, z) t .\\n\\n(2)\\n\\nIn addition to moment averaging, reasonable intermediate distributions can be produced in the\\nBayesian inference setting by conditioning on a sequence of increasing subsets of data; this insight\\nrelates AIS to the seemingly different class of sequential Monte Carlo (SMC) methods [MDJ06].\\n2.2\\n\\nStochastic lower bounds on the log partition function ratio\\n\\n? of the ratio R = ZT /Z1 of partition functions.\\nAIS produces a nonnegative unbiased estimate R\\nUnfortunately, because such ratios often vary across many orders of magnitude, it frequently happens\\n? underestimates R with overwhelming probability, while occasionally taking extremely large\\nthat R\\nvalues. Correspondingly, the variance may be extremely large, or even infinite.\\nFor these reasons, it is more meaningful to estimate log R. Unfortunately, the logarithm of a\\nnonnegative unbiased estimate (such as the AIS estimate) is, in general, a biased estimator of the log\\n? Then, by Jensen?s\\nestimand. More carefully, let A? be a nonnegative unbiased estimator for A = E[A].\\n? ? log E[A]\\n? = log A, and so log A? is a lower bound on log A in expectation. The\\ninequality, E[log A]\\nestimator log A? satisfies another important property: by Markov?s inequality for nonnegative random\\nvariables, Pr(log A? > log A + b) < e b , and so log A? is extremely unlikely to overestimate log A\\nby any appreciable number of nats. These observations motivate the following definition [BGS15]: a\\n? satisfying E[X]\\n? ? X and Pr(X\\n? > X + b) < e b .\\nstochastic lower bound on X is an estimator X\\nStochastic upper bounds are defined analogously. The above analysis shows that log A? is a stochastic\\n? is a\\nlower bound on log A when A? is a nonnegative unbiased estimate of A, and, in particular, log R\\nstochastic lower bound on log R. (It is possible to strengthen the tail bound by combining multiple\\nsamples [GBD07].)\\n2.3\\n\\nReverse AIS and Bidirectional Monte Carlo\\n\\nUpper and lower bounds are most useful in combination, as one can then sandwich the true value. As\\ndescribed above, AIS produces a stochastic lower bound on the ratio R; many other algorithms do as\\nwell. Upper bounds are more challenging to obtain. The key insight behind bidirectional Monte Carlo\\n(BDMC; [GGA15]) is that, provided one has an exact sample from the target distribution pT , one can\\nrun AIS in reverse to produce a stochastic lower bound on log Rrev = log Z1 /ZT , and therefore a\\nstochastic upper bound on log R = log Rrev . (In fact, BDMC is a more general framework which\\nallows a variety of partition function estimators, but we focus on AIS for pedagogical purposes.)\\nMore carefully, for t = 1, . . . , T , define p?t = pT t+1 and T?t = TT t+1 . Then p?1 corresponds\\nto our original target distribution pT and p?T corresponds to our original initial distribution p1 . As\\nbefore, T?t leaves p?t invariant. Consider the estimate produced by AIS on the sequence of distributions\\np?1 , . . . , p?T and corresponding MCMC transition operators T?1 , . . . , T?T . (In this case, the forward\\n? rev\\nchain of AIS corresponds to the reverse chain described in Section 2.1.) The resulting estimate R\\n? rev is a stochastic lower bound\\nis a nonnegative unbiased estimator of Rrev . It follows that log R\\n? rev1 is a stochastic upper bound on log R = log Rrev1 . BDMC is\\non log Rrev , and therefore log R\\n3\\n\\n\\fsimply the combination of this stochastic upper bound with the stochastic lower bound of Section 2.2.\\nBecause AIS is a consistent estimator of the partition function ratio under the assumption of ergodicity\\n[Nea01], the two bounds converge as T ! 1; therefore, given enough computation, BDMC can\\nsandwich log R to arbitrary precision.\\nReturning to the setting of Bayesian inference, given some fixed observation y, we can apply BDMC\\nprovided we have exact samples from both the prior distribution p(?, z) and the posterior distribution\\np(?, z|y). In practice, the prior is typically easy to sample from, but it is typically infeasible to\\ngenerate exact posterior samples. However, in models where we can tractably sample from the joint\\ndistribution p(?, z, y), we can generate exact posterior samples for simulated observations using the\\nelementary fact that\\np(y) p(?, z|y) = p(?, z, y) = p(?, z) p(y|?, z).\\n(3)\\nIn other words, if one ancestrally samples ?, z, and y, this is equivalent to first generating a dataset y\\nand then sampling (?, z) exactly from the posterior. Therefore, for simulated data, one has access to a\\nsingle exact posterior sample; this is enough to obtain stochastic upper bounds on log R = log p(y).\\n2.4\\n\\nWebPPL and Stan\\n\\nWe focus on two particular probabilistic programming packages. First, we consider WebPPL [GS], a\\nlightweight probabilistic programming language built on Javascript, and intended largely to illustrate\\nsome of the important ideas in probabilistic programming. Inference is based on Metropolis?Hastings\\n(M?H) updates to a program?s execution trace, i.e. a record of all stochastic decisions made by the\\nprogram. WebPPL has a small and clean implementation, and the entire implementation is described\\nin an online tutorial on probabilistic programming [GS].\\nSecond, we consider Stan [CGHL+ p], a highly engineered automatic inference system which is\\nwidely used by statisticians and is intended to scale to large problems. Stan is based on the No U-Turn\\nSampler (NUTS; [HG14]), a variant of Hamiltonian Monte Carlo (HMC; [Nea+11]) which chooses\\ntrajectory lengths adaptively. HMC can be significantly more efficient than M?H over execution\\ntraces because it uses gradient information to simultaneously update multiple parameters of a model,\\nbut is less general because it requires a differentiable likelihood. (In particular, this disallows discrete\\nlatent variables unless they are marginalized out analytically.)\\n\\n3\\n\\nMethods\\n\\nThere are at least two criteria we would desire from a sampling-based approximate inference algorithm\\nin order that its samples be representative of the true posterior distribution: we would like the\\napproximate distribution q(?, z; y) to cover all the high-probability regions of the posterior p(?, z | y),\\nand we would like it to avoid placing probability mass in low-probability regions of the posterior. The\\nformer criterion motivates measuring the KL divergence DKL (p(?, z | y) k q(?, z; y)), and the latter\\ncriterion motivates measuring DKL (q(?, z; y) k p(?, z | y)). If we desire both simultaneously, this\\nmotivates paying attention to the Jeffreys divergence, defined as DJ (qkp) = DKL (qkp) + DKL (pkq).\\nIn this section, we present Bounding Divergences with Reverse Annealing (BREAD), a technique for\\nusing BDMC to bound the Jeffreys divergence from the true posterior on simulated data, combined\\nwith a protocol for using this technique to analyze sampler accuracy on real-world data.\\n3.1\\n\\nUpper bounding the Jeffreys divergence in expectation\\n\\nWe now present our technique for bounding the Jeffreys divergence between the target distribution\\nand the distribution of approximate samples produced by AIS. In describing the algorithm, we revert\\nto the abstract state space formalism of Section 2.1, since the algorithm itself does not depend\\non any structure specific to posterior inference (except for the ability to obtain an exact sample).\\n? Let\\nWe first repeat the derivation from [GGA15] of the bias of the stochastic lower bound log R.\\nv = (x1 , . . . , xT 1 ) denote all of the variables sampled in AIS before the final stage; the final state\\nxT corresponds to the approximate sample produced by AIS. We can write the distributions over the\\nforward and reverse AIS chains as:\\nqf wd (v, xT ) = qf wd (v) qf wd (xT | v)\\n(4)\\nqrev (v, xT ) = pT (xT ) qrev (v | xT ).\\n(5)\\n4\\n\\n\\fThe distribution of approximate samples qf wd (xT ) is obtained by marginalizing out v. Note that\\nsampling from qrev requires sampling exactly from pT , so strictly speaking, BREAD is limited\\nto those cases where one has at least one exact sample from pT ? such as simulated data from a\\nprobabilistic model (see Section 2.3).\\n? of the log partition function ratio is given by:\\nThe expectation of the estimate log R\\n?\\n? = Eq (v,x ) log fT (xT ) qrev (v | xT )\\nE[log R]\\n(6)\\nT\\nf wd\\nZ1 qf wd (v, xT )\\n= log ZT log Z1 DKL (qf wd (xT ) qf wd (v | xT ) k pT (xT ) qrev (v | xT ))\\n(7)\\n? log ZT log Z1 DKL (qf wd (xT ) k pT (xT )).\\n(8)\\n(Note that qf wd (v | xT ) is the conditional distribution of the forward chain, given that the final state is\\nxT .) The inequality follows because marginalizing out variables cannot increase the KL divergence.\\nWe now go beyond the analysis in [GGA15], to bound the bias in the other direction. The expectation\\n? rev is\\nof the reverse estimate R\\n?\\n? rev ] = Eq (x ,v) log Z1 qf wd (v, xT )\\nE[log R\\n(9)\\nrev\\nT\\nfT (xT ) qrev (v | xT )\\n= log Z1 log ZT DKL (pT (xT ) qrev (v|xT ) k qf wd (xT ) qf wd (v | xT ))\\n(10)\\n? log Z1 log ZT DKL (pT (xT ) k qf wd (xT )).\\n(11)\\n? and log R\\n? rev1 can both be seen as estimators of log ZT , the former of\\nAs discussed above, log R\\nZ1\\nwhich is a stochastic lower bound, and the latter of which is a stochastic upper bound. Consider the\\n? rev1 log R.\\n? It follows from Eqs. (8) and (11) that, in\\ngap between these two bounds, B? , log R\\nexpectation, B? upper bounds the Jeffreys divergence\\nJ , DJ (pT (xT ), qf wd (xT )) , DKL (pT (xT ) k qf wd (xT )) + DKL (qf wd (xT ) k pT (xT ))\\nbetween the target distribution pT and the distribution qf wd (pT ) of approximate samples.\\n\\n(12)\\n\\nAlternatively, if one happens to have some other lower bound L or upper bound U on log R, then one\\ncan bound either of the one-sided KL divergences by running only one direction of AIS. Specifically,\\n?\\n? rev1 L]\\nfrom Eq. (8), E[U log R]\\nDKL (qf wd (xT ) k pT (xT )), and from Eq. (11), E[log R\\nDKL (pT (xT ) k qf wd (xT )).\\n\\n? as an upper bound on J ? We evaluated both B and J exactly\\nHow tight is the expectation B , E[B]\\non some toy distributions and found them to be a fairly good match. Details are given in Appendix B.\\n3.2\\n\\nApplication to real-world data\\n\\nSo far, we have focused on the setting of simulated data, where it is possible to obtain an exact\\nposterior sample, and then to rigorously bound the Jeffreys divergence using BDMC. However, we\\nare more likely to be interested in evaluating the performance of inference on real-world data, so\\nwe would like to simulate data which resembles a real-world dataset of interest. One particular\\ndifficulty is that, in Bayesian analysis, hyperparameters are often assigned non-informative or weakly\\ninformative priors, in order to avoid biasing the inference. This poses a challenge for BREAD, as\\ndatasets generated from hyperparameters sampled from such priors (which are often very broad) can\\nbe very dissimilar to real datasets, and hence conclusions from the simulated data may not generalize.\\nIn order to generate simulated datasets which better match a real-world dataset of interest, we adopt\\nthe following heuristic scheme: we first perform approximate posterior inference on the real-world\\n? real denote the estimated hyperparameters. We then simulate parameters and data from\\ndataset. Let ?\\n? real )p(D | ?\\n? real , ?). The forward AIS chain is run on D in the usual way.\\nthe forward model p(? | ?\\nHowever, to initialize the reverse chain, we first start with (?\\n? real , ?), and then run some number of\\nMCMC transitions which preserve p(?, ? | D), yielding an approximate posterior sample (? ? , ? ? ).\\n\\n? real was not sampled from p(? | D).\\nIn general, (? ? , ? ? ) will not be an exact posterior sample, since ?\\n? real which generated D ought to be in a region of high posterior\\nHowever, the true hyperparameters ?\\n? real . Therefore, we expect even\\nmass unless the prior p(?) concentrates most of its mass away from ?\\na small number of MCMC steps to produce a plausible posterior sample. This motivates our use of\\n(? ? , ? ? ) in place of an exact posterior sample. We validate this procedure in Section 5.1.2.\\n5\\n\\n\\f4\\n\\nRelated work\\n\\nMuch work has been devoted to the diagnosis of Markov chain convergence (e.g. [CC96; GR92;\\nBG98]). Diagnostics have been developed both for estimating the autocorrelation function of\\nstatistics of interest (which determines the number of effective samples from an MCMC chain) and\\nfor diagnosing whether Markov chains have reached equilibrium. In general, convergence diagnostics\\ncannot confirm convergence; they can only identify particular forms of non-convergence. By contrast,\\nBREAD can rigorously demonstrate convergence in the simulated data setting.\\nThere has also been much interest in automatically configuring parameters of MCMC algorithms.\\nSince it is hard to reliably summarize the performance of an MCMC algorithm, such automatic\\nconfiguration methods typically rely on method-specific analyses. For instance, Roberts and Rosenthal\\n[RR01] showed that the optimal acceptance rate of Metropolis?Hastings with an isotropic proposal\\ndistribution is 0.234 under fairly general conditions. M?H algorithms are sometimes tuned to\\nachieve this acceptance rate, even in situations where the theoretical analysis doesn?t hold. Rigorous\\nconvergence measures might enable more direct optimization of algorithmic hyperparameters.\\nGorham and Mackey [GM15] presented a method for directly estimating the quality of a set of\\napproximate samples, independently of how those samples were obtained. This method has strong\\nguarantees under a strong convexity assumption. By contrast, BREAD makes no assumptions about\\nthe distribution itself, so its mathematical guarantees (for simulated data) are applicable even to\\nmultimodal or badly conditioned posteriors.\\nIt has been observed that heating and cooling processes yield bounds on log-ratios of partition\\nfunctions by way of finite difference approximations to thermodynamic integration. Neal [Nea96]\\nused such an analysis to motivate tempered transitions, an MCMC algorithm based on heating and\\ncooling a distribution. His analysis cannot be directly applied to measuring convergence, as it assumed\\nequilibrium at each temperature. Jarzynski [Jar97] later gave a non-equilibrium analysis which is\\nequivalent to that underlying AIS [Nea01].\\nWe have recently learned of independent work [CTM16] which also builds on BDMC to evaluate the\\naccuracy of posterior inference in a probabilistic programming language. In particular, CusumanoTowner and Mansinghka [CTM16] define an unbiased estimator for a quantity called the subjective\\ndivergence. The estimator is equivalent to BDMC except that the reverse chain is initialized from an\\narbitrary reference distribution, rather than the true posterior. In [CTM16], the subjective divergence\\nis shown to upper bound the Jeffreys divergence when the true posterior is used; this is equivalent to\\nour analysis in Section 3.1. Much less is known about subjective divergence when the reference distribution is not taken to be the true posterior. (Our approximate sampling scheme for hyperparameters\\ncan be viewed as a kind of reference distribution.)\\n\\n5\\n\\nExperiments\\n\\nIn order to experiment with BREAD, we extended both WebPPL and Stan to run forward and reverse\\nAIS using the sequence of distributions defined in Eq. (2). The MCMC transition kernels were the\\nstandard ones provided by both platforms. Our first set of experiments was intended to validate that\\nBREAD can be used to evaluate the accuracy of posterior inference in realistic settings. Next, we\\nused BREAD to explore the tradeoffs between two different representations of a matrix factorization\\nmodel in both WebPPL and Stan.\\n5.1\\n\\nValidation\\n\\nAs described above, BREAD returns rigorous bounds on the Jeffreys divergence only when the data\\nare sampled from the model distribution. Here, we address three ways in which it could potentially\\ngive misleading results. First, the upper bound B may overestimate the true Jeffreys divergence J .\\nSecond, results on simulated data may not correspond to results on real-world data if the simulated\\ndata are not representative of the real-world data. Finally, the fitted hyperparameter procedure of\\nSection 3.2 may not yield a sample sufficiently representative of the true posterior p(?, ? | D). The\\nfirst issue, about the accuracy of the bound, is addressed in Appendix B.1.1; the bound appears to be\\nfairly close to the true Jeffreys divergence on some toy distributions. We address the other two issues\\nin this section. In particular, we attempted to validate that the behavior of the method on simulated\\n6\\n\\n\\f(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 1: (a) Validation of the consistency of the behavior of forward AIS on real and simulated data for\\n\\nthe logistic regression model. Since the log-ML values need not match between the real and simulated data,\\nthe y-axes for each curve are shifted based on the maximum log-ML lower bound obtained by forward AIS.\\n(b) Same as (a), but for matrix factorization. The complete set of results on all datasets is given in Appendix D.\\n(c) Validation of the fitted hyperparameter scheme on the logistic regression model (see Section 5.1.2 for details).\\nReverse AIS curves are shown as the number of Gibbs steps used to initialize the hyperparameters is varied.\\n\\ndata is consistent with that on real data, and that the fitted-hyperparameter samples can be used as a\\nproxy for samples from the posterior. All experiments in this section were performed using Stan.\\n5.1.1\\n\\nValidating consistency of inference behavior between real and simulated data\\n\\nTo validate BREAD in a realistic setting, we considered five models based on examples from the Stan\\nmanual [Sta], and chose a publicly available real-world dataset for each model. These models include:\\nlinear regression, logistic regression, matrix factorization, autoregressive time series modeling, and\\nmixture-of-Gaussians clustering. See Appendix C for model details and Stan source code.\\nIn order to validate the use of simulated data as a proxy for real data in the context of BREAD,\\nwe fit hyperparameters to the real-world datasets and simulated data from those hyperparameters,\\nas described in Section 3.2. In Fig. 1 and Appendix D, we show the distributions of forward and\\nreverse AIS estimates on simulated data and forward AIS estimates on real-world data, based on 100\\nAIS chains for each condition.2 Because the distributions of AIS estimates included many outliers,\\nwe visualize quartiles of the estimates rather than means.3 The real and simulated data need not\\nhave the same marginal likelihood, so the AIS estimates for real and simulated data are shifted\\nvertically based on the largest forward AIS estimate obtained for each model. For all five models\\nunder consideration, the forward AIS curves were nearly identical (up to a vertical shift), and the\\ndistributions of AIS estimates were very similar at each number of AIS steps. (An example where the\\nforward AIS curves failed to match up due to model misspecification is given in Appendix D.) Since\\nthe inference behavior appears to match closely between the real and simulated data, we conclude\\nthat data simulated using fitted hyperparameters can be a useful proxy for real data when evaluating\\ninference algorithms.\\n5.1.2\\n\\nValidating the approximate posterior over hyperparameters\\n\\nAs described in Section 3.2, when we simulate data from fitted hyperparameters, we use an approximate (rather than exact) posterior sample (? ? , ? ? ) to initialize the reverse chain. Because of\\nthis, BREAD is not mathematically guaranteed to upper bound the Jeffreys divergence even on the\\nsimulated data. In order to determine the effect of this approximation in practice, we repeated the\\nprocedure of Section 5.1.1 for all five models, but varying S, the number of MCMC steps used to\\nobtain (? ? , ? ? ), with S 2 {10, 100, 1000, 10000}. The reverse AIS estimates are shown in Fig. 1\\nand Appendix D. (We do not show the forward AIS estimates because these are unaffected by S.) In\\nall five cases, the reverse AIS curves were statistically indistinguishable. This validates our use of\\nfitted hyperparameters, as it suggests that the use of approximate samples of hyperparameters has\\nlittle impact on the reverse AIS upper bounds.\\n2\\n\\nThe forward AIS chains are independent, while the reverse chains share an initial state.\\nNormally, such outliers are not a problem for AIS, because one averages the weights wT , and this average is\\ninsensitive to extremely small values. Unfortunately, the analysis of Section 3.1 does not justify such averaging,\\nso we report estimates corresponding to individual AIS chains.\\n3\\n\\n7\\n\\n\\f(a)\\n\\n(b)\\n\\nFigure 2: Comparison of Jeffreys divergence bounds for matrix factorization in Stan and WebPPL, using the\\ncollapsed and uncollapsed formulations. Given as a function of (a) number of MCMC steps, (b) running time.\\n\\n5.2\\n\\nScientific findings produced by BREAD\\n\\nHaving validated various aspects of BREAD, we applied it to investigate the choice of model representation in Stan and WebPPL. During our investigation, we also uncovered a bug in WebPPL, indicating\\nthe potential usefulness of BREAD as a means of testing the correctness of an implementation.\\n5.2.1\\n\\nComparing model representations\\n\\nMany models can be written in more than one way, for example by introducing or collapsing latent\\nvariables. Performance of probabilistic programming languages can be sensitive to such choices of\\nrepresentation, and the representation which gives the best performance may vary from one language\\nto another. We consider the matrix factorization model described above, which we now specify in\\nmore detail. We approximate an N ? D matrix Y as a low rank matrix, the product of matrices\\nU and V with dimensions N ? K and K ? D respectively (where K < min(N, D)). We use a\\nspherical Gaussian observation model, and spherical Gaussian priors on U and V:\\nuik ? N (0,\\n\\n2\\nu)\\n\\nvkj ? N (0,\\n\\n2\\nv)\\n\\nyij | ui , vj ? N (u>\\ni vj ,\\n\\n2\\n\\n)\\n\\nWe can also collapse U to obtain the model vkj ? N (0,\\nand yi | V ? N (0, u V> V + I).\\nIn general, collapsing variables can help MCMC samplers mix faster at the expense of greater\\ncomputational cost per update. The precise tradeoff can depend on the size of the model and dataset,\\nthe choice of MCMC algorithm, and the underlying implementation, so it would be useful to have a\\nquantitative criterion to choose between them.\\n2\\nv)\\n\\nWe fixed the values of all hyperparameters to 1, and set N = 50, K = 5 and D = 25. We ran\\nBREAD on both platforms (Stan and WebPPL) and for both formulations (collapsed and uncollapsed)\\n(see Fig. 2). The simulated data and exact posterior sample were shared between all conditions in\\norder to make the results directly comparable.\\nAs predicted, the collapsed sampler resulted in slower updates but faster convergence (in terms of\\nthe number of steps). However, the per-iteration convergence benefit of collapsing was much larger\\nin WebPPL than in Stan (perhaps because of the different underlying inference algorithm). Overall,\\nthe tradeoff between efficiency and convergence speed appears to favour the uncollapsed version in\\nStan, and the collapsed version in WebPPL (see Fig. 2(b)). (Note that this result holds only for our\\nparticular choice of problem size; the tradeoff may change given different model or dataset sizes.)\\nHence BREAD can provide valuable insights into the tricky question of which representations of\\nmodels to choose to achieve faster convergence.\\n5.2.2\\n\\nDebugging\\n\\nMathematically, the forward and reverse AIS chains yield lower and upper bounds on log p(y) with\\nhigh probability; if this behavior is not observed, that indicates a bug. In our experimentation with\\nWebPPL, we observed a case where the reverse AIS chain yielded estimates significantly lower than\\nthose produced by the forward chain, inconsistent with the theoretical guarantee. This led us to\\nfind a subtle bug in how WebPPL sampled from a multivariate Gaussian distribution (which had the\\neffect that the exact posterior samples used to initialize the reverse chain were incorrect).4 These\\ndays, while many new probabilistic programming languages are emerging and many are in active\\ndevelopment, such debugging capabilities provided by BREAD can potentially be very useful.\\n4\\n\\nIssue: https://github.com/probmods/webppl/issues/473\\n\\n8\\n\\n\\fReferences\\n[BG98]\\n\\nS. P. Brooks and A. Gelman. ?General methods for monitoring convergence of\\niterative simulations?. Journal of Computational and Graphical Statistics 7.4 (1998),\\npp. 434?455.\\n[BGS15]\\nY. Burda, R. B. Grosse, and R. Salakhutdinov. ?Accurate and conservative estimates of\\nMRF log-likelihood using reverse annealing?. In: Artificial Intelligence and Statistics.\\n2015.\\n[CC96]\\nM. K. Cowles and B. P. Carlin. ?Markov chain Monte Carlo convergence diagnostics:\\na comparative review?. Journal of the American Statistical Association 91.434 (1996),\\npp. 883?904.\\n[CGHL+ p] B. Carpenter, A. Gelman, M. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. A.\\nBrubaker, J. Guo, P. Li, and A. Riddell. ?Stan: a probabilistic programming language?.\\nJournal of Statistical Software (in press).\\n[CTM16]\\nM. F. Cusumano-Towner and V. K. Mansinghka. Quantifying the probable approximation error of probabilistic inference programs. arXiv:1606.00068. 2016.\\n[ET98]\\nB. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall/CRC,\\n1998.\\n[GBD07]\\nV. Gogate, B. Bidyuk, and R. Dechter. ?Studies in lower bounding probability of\\nevidence using the Markov inequality?. In: Conference on Uncertainty in AI. 2007.\\n[GGA15]\\nR. B. Grosse, Z. Ghahramani, and R. P. Adams. Sandwiching the marginal likelihood\\nwith bidirectional Monte Carlo. arXiv:1511.02543. 2015.\\n[GM15]\\nJ. Gorham and L. Mackey. ?Measuring sample quality with Stein?s method?. In:\\nNeural Information Processing Systems. 2015.\\n[GMRB+08] N. D. Goodman, V. K. Mansinghka, D. M. Roy, K. Bonawitz, and J. B. Tenenbaum.\\n?Church: a language for generative models?. In: Conference on Uncertainty in AI.\\n2008.\\n[GMS13]\\nR. Grosse, C. J. Maddison, and R. Salakhutdinov. ?Annealing between distributions\\nby averaging moments?. In: Neural Information Processing Systems. 2013.\\n[GR92]\\nA. Gelman and D. B. Rubin. ?Inference from iterative simulation using multiple\\nsequences?. Statistical Science 7.4 (1992), pp. 457?472.\\n[GS]\\nN. D. Goodman and A. Stuhlm?ller. The Design and Implementation of Probabilistic\\nProgramming Languages. http://dippl.org.\\n[HG14]\\nM. D. Homan and A. Gelman. ?The No-U-turn Sampler: Adaptively Setting Path\\nLengths in Hamiltonian Monte Carlo?. J. Mach. Learn. Res. 15.1 (Jan. 2014),\\npp. 1593?1623. ISSN: 1532-4435.\\n[Jar97]\\nC. Jarzynski. ?Equilibrium free-energy differences from non-equilibrium measurements: a master-equation approach?. Physical Review E 56 (1997), pp. 5018?5035.\\n[LTBS00]\\nD. J. Lunn, A. Thomas, N. Best, and D. Spiegelhalter. ?WinBUGS ? a Bayesian modelling framework: concepts, structure, and extensibility?. Statistics and Computing\\n10.4 (2000), pp. 325?337.\\n[MDJ06]\\nP. del Moral, A. Doucet, and A. Jasra. ?Sequential Monte Carlo samplers?. Journal of\\nthe Royal Statistical Society: Series B (Statistical Methodology) 68.3 (2006), pp. 411?\\n436.\\n[Nea+11]\\nR. M. Neal et al. ?MCMC using Hamiltonian dynamics?. Handbook of Markov Chain\\nMonte Carlo 2 (2011), pp. 113?162.\\n[Nea01]\\nR. M. Neal. ?Annealed importance sampling?. Statistics and Computing 11 (2001),\\npp. 125?139.\\n[Nea96]\\nR. M. Neal. ?Sampling from multimodal distributions using tempered transitions?.\\nStatistics and Computing 6.4 (1996), pp. 353?366.\\n[RR01]\\nG. O. Roberts and J. S. Rosenthal. ?Optimal scaling for various Metropolis?Hastings\\nalgorithms?. Statistical Science 16.4 (2001), pp. 351?367.\\n[Sta]\\nStan Modeling Language Users Guide and Reference Manual. Stan Development\\nTeam.\\n\\n9\\n\\n\\f\",\n          \"Learning and Forecasting Opinion Dynamics in\\nSocial Networks\\nAbir De?\\nIsabel Valera?\\nNiloy Ganguly?\\n?\\nSourangshu Bhattacharya\\nManuel Gomez-Rodriguez?\\n?\\nIIT Kharagpur\\nMPI for Software Systems?\\n{abir.de,niloy,sourangshu}@cse.iitkgp.ernet.in\\n{ivalera,manuelgr}@mpi-sws.org\\n\\nAbstract\\nSocial media and social networking sites have become a global pinboard for exposition and discussion of news, topics, and ideas, where social media users often\\nupdate their opinions about a particular topic by learning from the opinions shared\\nby their friends. In this context, can we learn a data-driven model of opinion dynamics that is able to accurately forecast users? opinions? In this paper, we introduce SLANT, a probabilistic modeling framework of opinion dynamics, which\\nrepresents users? opinions over time by means of marked jump diffusion stochastic differential equations, and allows for efficient model simulation and parameter\\nestimation from historical fine grained event data. We then leverage our framework to derive a set of efficient predictive formulas for opinion forecasting and\\nidentify conditions under which opinions converge to a steady state. Experiments\\non data gathered from Twitter show that our model provides a good fit to the data\\nand our formulas achieve more accurate forecasting than alternatives.\\n\\n1\\n\\nIntroduction\\n\\nSocial media and social networking sites are increasingly used by people to express their opinions,\\ngive their ?hot takes?, on the latest breaking news, political issues, sports events, and new products.\\nAs a consequence, there has been an increasing interest on leveraging social media and social networking sites to sense and forecast opinions, as well as understand opinion dynamics. For example,\\npolitical parties routinely use social media to sense people?s opinion about their political discourse1 ;\\nquantitative investment firms measure investor sentiment and trade using social media [18]; and,\\ncorporations leverage brand sentiment, estimated from users? posts, likes and shares in social media\\nand social networking sites, to design their marketing campaigns2 . In this context, multiple methods\\nfor sensing opinions, typically based on sentiment analysis [21], have been proposed in recent years.\\nHowever, methods for accurately forecasting opinions are still scarce [7, 8, 19], despite the extensive\\nliterature on theoretical models of opinion dynamics [6, 9].\\nIn this paper, we develop a novel modeling framework of opinion dynamics in social media and social networking sites, SLANT3 , which allows for accurate forecasting of individual users? opinions.\\nThe proposed framework is based on two simple intuitive ideas: i) users? opinions are hidden until\\nthey decide to share it with their friends (or neighbors); and, ii) users may update their opinions\\nabout a particular topic by learning from the opinions shared by their friends. While the latter is one\\nof the main underlying premises used by many well-known theoretical models of opinion dynamics [6, 9, 22], the former has been ignored by models of opinion dynamics, despite its relevance on\\nclosely related processes such as information diffusion [12].\\n1\\n\\nhttp://www.nytimes.com/2012/10/08/technology/campaigns-use-social-media-to-lure-younger-voters.html\\nhttp://www.nytimes.com/2012/07/31/technology/facebook-twitter-and-foursquare-as-corporate-focus-groups.html\\n3\\nSlant is a particular point of view from which something is seen or presented.\\n2\\n\\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\\n\\n\\fMore in detail, our proposed model represents users? latent opinions as continuous-time stochastic\\nprocesses driven by a set of marked jump stochastic differential equations (SDEs) [14]. Such construction allows each user?s latent opinion to be modulated over time by the opinions asynchronously\\nexpressed by her neighbors as sentiment messages. Here, every time a user expresses an opinion by\\nposting a sentiment message, she reveals a noisy estimate of her current latent opinion. Then, we\\nexploit a key property of our model, the Markov property, to develop:\\nI. An efficient estimation procedure to find the parameters that maximize the likelihood of a\\nset of (millions of) sentiment messages via convex programming.\\nII. A scalable simulation procedure to sample millions of sentiment messages from the proposed model in a matter of minutes.\\nIII. A set of novel predictive formulas for efficient and accurate opinion forecasting, which\\ncan also be used to identify conditions under which opinions converge to a steady state of\\nconsensus or polarization.\\nFinally, we experiment on both synthetic and real data gathered from Twitter and show that our\\nmodel provides a good fit to the data and our predictive formulas achieve more accurate opinion\\nforecasting than several alternatives [7, 8, 9, 15, 26].\\nRelated work. There is an extensive line of work on theoretical models of opinion dynamics and\\nopinion formation [3, 6, 9, 15, 17, 26]. However, previous models typically share the following\\nlimitations: (i) they do not distinguish between latent opinion and sentiment (or expressed opinion), which is a noisy observation of the opinion (e.g., thumbs up/down, text sentiment); (ii) they\\nconsider users? opinions to be updated synchronously in discrete time, however, opinions may be\\nupdated asynchronously following complex temporal patterns [12]; (iii) the model parameters are\\ndifficult to learn from real fine-grained data and instead are set arbitrarily, as a consequence, they\\nprovide inaccurate fine-grained predictions; and, (iv) they focus on analyzing only the steady state\\nof the users? opinions, neglecting the transient behavior of real opinion dynamics which allows for\\nopinion forecasting methods. More recently, there have been some efforts on designing models that\\novercome some of the above limitations and provide more accurate predictions [7, 8]. However,\\nthey do not distinguish between opinion and sentiment and still consider opinions to be updated\\nsynchronously in discrete time. Our modeling framework addresses the above limitations and, by\\ndoing so, achieves more accurate opinion forecasting than alternatives.\\n\\n2\\n\\nProposed model\\n\\nIn this section, we first formulate our model of opinion dynamics, starting from the data it is designed\\nfor, and then introduce efficient methods for model parameter estimation and model simulation.\\nOpinions data. Given a directed social network G = (V, E), we record each message as e :=\\n(u, m, t), where the triplet means that the user u ? V posted a message with sentiment m at time\\nt. Given a collection of messages {e1 = (u1 , m1 , t1 ), . . . , en = (un , mn , tn )}, the history Hu (t)\\ngathers all messages posted by user u up to but not including time t, i.e.,\\nHu (t) = {ei = (ui , mi , ti )|ui = u and ti < t},\\n\\n(1)\\n\\nand H(t) := ?u?V Hu (t) denotes the entire history of messages up to but not including time t.\\n\\nGenerative process. We represent users? latent opinions as a multidimensional stochastic process\\nx? (t), in which the u-th entry, x?u (t) ? R, represents the opinion of user u at time t and the sign ?\\nmeans that it may depend on the history H(t). Then, every time a user u posts a message at time t,\\nwe draw its sentiment m from a sentiment distribution p(m|x?u (t)). Here, we can also think of the\\nsentiment m of each message as samples from a noisy stochastic process mu (t) ? p(mu (t)|x?u (t)).\\nFurther, we represent the message times by a set of counting processes. In particular, we denote\\nthe set of counting processes as a vector N (t), in which the u-th entry, Nu (t) ? {0} ? Z+ , counts\\nthe number of sentiment messages user u posted up to but not including time t. Then, we can\\ncharacterize the message rate of the users using their corresponding conditional intensities as\\nE[dN (t) | H(t)] = ?? (t) dt,\\n(2)\\nwhere dN (t) := ( dNu (t) )u?V denotes the number of messages per user in the window [t, t + dt)\\nand ?? (t) := ( ??u (t) )u?V denotes the associated user intensities, which may depend on the history\\nH(t). We denote the set of user that u follows by N (u). Next, we specify the the intensity functions\\n?? (t), the dynamics of the users? opinions x? (t), and the sentiment distribution p(m|x?u (t)).\\n2\\n\\n\\fIntensity for messages. There is a wide variety of message intensity functions one can choose from\\nto model the users? intensity ?? (t) [1]. In this work, we consider two of the most popular functional\\nforms used in the growing literature on social activity modeling using point processes [10, 24, 5]:\\nI. Poisson process. The intensity is assumed to be independent of the history H(t) and\\nconstant, i.e., ??u (t) = ?u .\\nII. Multivariate Hawkes processes. The intensity captures a mutual excitation phenomena between message events and depends on the whole history of message events\\n?v?{u?N (u)} Hv (t) before t:\\nX\\nX\\nX\\n??u (t) = ?u +\\nbvu\\n?(t ? ti ) = ?u +\\nbvu (?(t) ? dNv (t)), (3)\\nv?u?N (u)\\n\\nei ?Hv (t)\\n\\nv?u?N (u)\\n\\nwhere the first term, ?u > 0, models the publication of messages by user u on her own\\ninitiative, and the second term, with bvu > 0, models the publication of additional messages\\nby user u due to the influence that previous messages posted by the users she follows have\\non her intensity. Here, ?(t) = e??t is an exponential triggering kernel modeling the decay\\nof influence of the past events over time and ? denotes the convolution operation.\\nIn both cases, the couple (N (t), ?? (t)) is a Markov process, i.e., future states of the process (conditional on past and present states) depends only upon the present state, and we can express the users?\\nintensity more compactly using the following jump stochastic differential equation (SDE):\\nd?? (t) = ?(? ? ?? (t))dt + BdN (t),\\n\\nwhere the initial condition is ?? (0) = ?. The Markov property will become important later.\\nStochastic process for opinion. The opinion x?u (t) of a user u at time t adopts the following form:\\nX\\nX\\nX\\nx?u (t) = ?u +\\navu\\nmi g(t ? ti ) = ?u +\\navu (g(t) ? (mv (t)dNv (t))), (4)\\nv?N (u)\\n\\nei ?Hv (t)\\n\\nv?N (u)\\n\\nwhere the first term, ?u ? R, models the original opinion a user u starts with, the second term,\\nwith avu ? R, models updates in user u?s opinion due to the influence that previous messages with\\nopinions mi posted by the users that u follows has on her opinion. Here, g(t) = e??t (where\\n? > 0) denotes an exponential triggering kernel, which models the decay of influence over time.\\nThe greater the value of ?, the greater the user?s tendency to retain her own opinion ?u . Under this\\nform, the resulting opinion dynamics are Markovian and can be compactly represented by a set of\\ncoupled marked jumped stochastic differential equations (proven in Appendix A):\\nProposition 1 The tuple (x? (t), ?? (t), N (t)) is a Markov process, whose dynamics are defined by\\nthe following marked jumped stochastic differential equations (SDE):\\ndx? (t) = ?(? ? x? (t))dt + A(m(t) \\f dN (t))\\n?\\n\\n?\\n\\nd? (t) = ?(? ? ? (t))dt + B dN (t)\\n\\n(5)\\n(6)\\n\\nwhere the initial conditions are ?? (0) = ? and x? (0) = ?, the marks are the sentiment messages\\nm(t) = ( mu (t) )u?V , with mu (t) ? p(m|x?u (t)), and the sign \\f denotes pointwise product.\\nThe above mentioned Markov property will be the key to the design of efficient model parameter\\nestimation and model simulation algorithms.\\nSentiment distribution. The particular choice of sentiment distribution p(m|x?u (t)) depends on the\\nrecorded marks. For example, one may consider:\\nI. Gaussian Distribution The sentiment is assumed to be a real random variable m ? R, i.e.,\\np(m|xu (t)) = N (xu (t), ?u ). This fits well scenarios in which sentiment is extracted from\\ntext using sentiment analysis [13].\\nII. Logistic. The sentiment is assumed to be a binary random variable m ? {?1, 1}, i.e.,\\np(m|xu (t)) = 1/(1 + exp(?m ? xu (t))). This fits well scenarios in which sentiment is\\nmeasured by means of up votes, down votes or likes.\\nOur model estimation method can be easily adapted to any log-concave sentiment distribution. However, in the remainder of the paper, we consider the Gaussian distribution since, in our experiments,\\nsentiment is extracted from text using sentiment analysis.\\n3\\n\\n\\f2.1 Model parameter estimation\\nGiven a collection of messages H(T ) = {(ui , mi , ti )} recorded during a time period [0, T ) in\\na social network G = (V, E), we can find the optimal parameters ?, ?, A and B by solving a\\nmaximum likelihood estimation (MLE) problem4 . To do so, it is easy to show that the log-likelihood\\nof the messages is given by\\nX\\nX\\nXZ T\\n??u (? ) d? .\\nL(?, ?, A, B) =\\nlog p(mi |x?ui (ti )) +\\nlog ??ui (ti ) ?\\n(7)\\nei ?H(T )\\n\\n|\\n\\n{z\\n\\nmessage sentiments\\n\\nei ?H(T )\\n\\n|\\n\\n}\\n\\nmaximize\\n\\nL(?, ?, A, B).\\n\\nu?V\\n\\nmessage times\\n\\nThen, we can find the optimal parameters (?, ?, A, B) using MLE as\\n?,??0,A,B?0\\n\\n{z\\n\\n0\\n\\n}\\n\\n(8)\\n\\nNote that, as long as the sentiment distributions are log-concave, the MLE problem above is concave and thus can be solved efficiently. Moreover, the problem decomposes in 2|V| independent\\nsubproblems, two per user u, since the first term in Eq. 7 only depends on (?, A) whereas the last\\ntwo terms only depend on (?, B), and thus can be readily parallelized. Then, we find (?? , B ? ) using spectral projected gradient descent [4], which works well in practice and achieves ? accuracy in\\nO(log(1/?)) iterations, and find (?? , A? ) analytically, since, for Gaussian sentiment distributions,\\nthe problem reduces to a least-square problem. Fortunately, in each subproblem, we can use the\\nMarkov property from Proposition 1 to precompute the sums and integrals in (8) in linear time, i.e.,\\nO(|Hu (T )| + | ?v?N (u) Hv (T )|). Appendix H summarizes the overall estimation algorithm.\\n2.2 Model simulation\\nWe leverage the efficient sampling algorithm for multivariate Hawkes introduced by Farajtabar et\\nal. [11] to design a scalable algorithm to sample opinions from our model. The two key ideas that\\nallow us to adapt the procedure by Farajtabar et al. to our model of opinion dynamics, while keeping\\nits efficiency, are as follows: (i) the opinion dynamics, defined by Eqs. 5 and 6, are Markovian and\\nthus we can update individual intensities and opinions in O(1) ? let ti and ti+1 be two consecutive\\nevents, then, we can compute ?? (ti+1 ) as (?? (ti ) ? ?) exp(??(ti+1 ? ti )) + ? and x? (ti+1 ) as\\n(x? (ti ) ? ?) exp(??(ti+1 ? ti )) + ?, respectively; and, (ii) social networks are typically sparse\\nand thus both A and B are also sparse, then, whenever a node expresses its opinion, only a small\\nnumber of opinions and intensity functions in its local neighborhood will change. As a consequence,\\nwe can reuse the majority of samples from the intensity functions and sentiment distributions for the\\nnext new sample. Appendix I summarizes the overall simulation algorithm.\\n\\n3\\n\\nOpinion forecasting\\n\\nOur goal here is developing efficient methods that leverage our model to forecast a user u?s\\nopinion xu (t) at time t given the history H(t0 ) up to time t0 <t. In the context of our probabilistic model, we will forecast this opinion by efficiently computing the conditional expectation\\nEH(t)\\\\H(t0 ) [x?u (t)|H(t0 )], where H(t)\\\\H(t0 ) denotes the average across histories from t0 to t,\\nwhile conditioning on the history up to H(t0 ).\\nTo this aim, we will develop analytical and sampling based methods to compute the above conditional expectation. Moreover, we will use the former to identify under which conditions users?\\naverage opinion converges to a steady state and, if so, find the steady state opinion. In this section,\\nwe write Ht = H(t) to lighten the notation and denote the eigenvalues of a matrix X by ?(X).\\n3.1 Analytical forecasting\\nIn this section, we derive a set of formulas to compute the conditional expectation for both Poisson\\nand Hawkes messages intensities. However, since the derivation of such formulas for general multivariate Hawkes is difficult, we focus here on the case when bvu = 0 for all v, u ? G, v 6= u, and\\nrely on the efficient sampling based method for the general case.\\nI. Poisson intensity. Consider each user?s messages follow a Poisson process with rate ?u . Then,\\nthe conditional average opinion is given by (proven in Appendix C):\\n4\\n\\nHere, if one decides to model the message intensities with a Poisson process, B = 0.\\n\\n4\\n\\n\\fTheorem 2 Given a collection of messages Ht0 recorded during a time period [0, t0 ) and ??u (t) =\\n?u for all u ? G, then,\\n\\u0010\\n\\u0011\\nEHt \\\\Ht0 [x? (t)|Ht0 ] = e(A?1 ??I)(t?t0 ) x(t0 ) + ?(A?1 ? ?I)?1 e(A?1 ??I)(t?t0 ) ? I ?,\\nwhere ?1 := diag[?] and (x(t0 ))u?V = ?u +\\n\\nP\\n\\nv?N (u)\\n\\nauv\\n\\nP\\n\\nti ?Hv (t0 )\\n\\n(9)\\n\\ne\\n\\n??(t0 ?ti )\\n\\nmv (ti ).\\n\\nRemarkably, we can efficiently compute both terms in Eq. 9 by using the iterative algorithm by AlMohy et al. [2] for the matrix exponentials and the well-known GMRES method [23] for the matrix\\ninversion. Given this predictive formula, we can easily study the stability condition and, for stable\\nsystems, find the steady state conditional average opinion (proven in Appendix D):\\nTheorem 3 Given the conditions of Theorem 2, if Re[?(A?1 )] < ?, then,\\n\\u0012\\n\\u0013?1\\nA?1\\nlim EHt \\\\Ht0 [x? (t)|Ht0 ] = I ?\\n?.\\nt??\\nw\\n\\n(10)\\n\\nThe above results indicate that the conditional average opinions are nonlinearly related to the parameter matrix A, which depends on the network structure, and the message rates ?, which in this case\\nare assumed to be constant and independent on the network structure. Figure 1 provides empirical\\nevidence of these results.\\nII. Multivariate Hawkes Process. Consider each user?s messages follow a multivariate Hawkes\\nprocess, given by Eq. 3, and bvu = 0 for all v, u ? G, v 6= u. Then, the conditional average opinion\\nis given by (proven in Appendix E):\\nTheorem P\\n4 Given a collection of messages Ht0 recorded during a time period [0, t0 ) and ??u (t) =\\n?u + buu ei ?Hu (t) e??(t?ti ) for all u ? G, then, the conditional average satisfies the following\\ndifferential equation:\\ndEHt \\\\Ht0 [x? (t)|Ht0 ]\\n(11)\\n= [??I + A?(t)]EHt \\\\Ht0 [x? (t)|Ht0 ] + ??,\\ndt\\nwhere\\n\\u0011\\n\\u0010\\n?(t) = diag EHt \\\\Ht0 [?? (t)|Ht0 ] ,\\n\\n\\u0010\\n\\u0011\\nEHt \\\\Ht0 [?? (t)|Ht0 ] = e(B??I)(t?t0 ) ?(t0 ) + ?(B ? ?I)?1 e(B??I)(t?t0 ) ? I ? ?t ? t0 ,\\nX\\nX\\n(?(t0 ))u?V = ?u +\\nbuv\\ne??(t0 ?ti ) ,\\nv?N (u)\\n\\nti ?Hv (t0 )\\n\\n\\u0001\\nB = diag [b11 , . . . , b|V||V| ]> .\\n\\nHere, we can compute the conditional average by solving numerically the differential equation\\nabove, which is not stochastic, where we can efficiently compute the vector EHt [?? (t)] by using\\nagain the algorithm by Al-Mohy et al. [2] and the GMRES method [23].\\nIn this case, the stability condition and the steady state conditional average opinion are given by\\n(proven in Appendix F):\\nTheorem 5 Given the conditions of Theorem 4, if the transition matrix ?(t) associated to the timevarying linear system described by Eq. 11 satisfies that ||?(t)|| ? ?e?ct ?t > 0, where ?, c > 0,\\nthen,\\n\\u0012\\n\\u0013?1\\nA?2\\n?\\nlim EHt \\\\Ht0 [x (t)|Ht0 ] = I ?\\n?,\\n(12)\\nt??\\nw\\nh\\ni\\n\\u0001?1\\nwhere ?2 := diag I ? B\\n?\\n?\\nThe above results indicate that the conditional average opinions are nonlinearly related to the parameter matrices A and B. This suggests that the effect of the temporal influence on the opinion\\nevolution, by means of the parameter matrix B of the multivariate Hawkes process, is non trivial.\\nWe illustrate this result empirically in Figure 1.\\n5\\n\\n\\fTheoretical\\n\\n0.01\\n\\nE[xu (t)]\\nu?V ?\\n|V ? |\\n\\nH:\\nOpinion-Trajectory?\\n\\nP:\\nOpinion-Trajectory?\\n\\nNetwork G1\\n\\n0\\n\\n-0.5\\n-1\\n\\n-1.5\\n\\nTheoretical\\nExperimental\\n\\n-2\\n0.005\\n\\n0\\n\\n0.01\\n\\n0.015\\n\\nP\\nE[xu (t)]\\nu?V ?\\n|V ? |\\n\\nNetwork G2\\n\\nP:\\n\\nHawkes (+)\\nHawkes (-)\\n\\n30\\n20\\n10\\n0\\n\\n-200\\n\\n0.005\\n\\n0.01\\n\\n0.015\\n\\n50\\n\\n40\\n\\n40\\n\\n30\\n20\\n\\nH:\\n\\nP\\nu?V E[xu (t)]\\n|V |\\n\\n0.005\\n\\n0.01\\n\\nTime\\n\\n0.015\\n\\nH: Temporal evolution\\n\\n50\\n\\n30\\n20\\n10\\n\\n00\\n\\n0.005 0.01\\nTime\\n\\nTime\\n\\nu?V E[xu (t)]\\n|V |\\n\\n20\\n\\n00\\n\\n0.015\\n\\n10\\n\\n-10\\n\\nTime\\n\\nP\\n\\n0.005 0.01\\nTime\\n\\nP: Temporal evolution\\n\\n40\\n\\n30\\n\\n10\\n\\n00\\n\\n0.015\\n\\nTime\\n\\nTime\\n\\nP\\n\\n0.01\\n\\n0.005\\n\\n20\\n10\\n\\nHawkes (-)\\n\\n-40\\n\\n0.015\\n\\n30\\n\\nNode-ID\\n\\n0.005\\n\\n40\\n\\n0\\n\\n-2 Hawkes (+)\\n\\nExperimental\\n0\\n0\\n\\n50\\n\\n40\\n\\nNode-ID\\n\\n0.5\\n\\n50\\n\\nNode-ID\\n\\n1\\n\\n4\\n2\\n\\nNode-ID\\n\\nOpinion-Trajectory?\\n\\nOpinion-Trajectory?\\n\\n1.5\\n\\n0.015\\n\\nP: Temporal evolution\\n\\n00\\n\\n0.005\\n\\n0.01\\n\\nTime\\n\\n0.015\\n\\nH: Temporal evolution\\n\\nFigure 1: Opinion dynamics on two 50-node networks G1 (top) and G2 (bottom) for Poisson (P)\\nand Hawkes (H) message intensities. The first column visualizes the two networks and opinion of\\neach node at t = 0 (positive/negative opinions in red/blue). The second column shows the temporal\\nevolution of the theoretical and empirical average opinion for Poisson intensities. The third column\\nshows the temporal evolution of the empirical average opinion for Hawkes intensities, where we\\ncompute the average separately for positive (+) and negative (?) opinions in the steady state. The\\nfourth and fifth columns shows the polarity of average opinion per user over time.\\n3.2\\n\\nSimulation based forecasting\\n\\nGiven the efficient simulation procedure described in Section 2.2, we can readily derive a general\\nsimulation based formula for opinion forecasting:\\nn\\n\\n1X ?\\n? (t) =\\nEHt \\\\Ht0 [x (t)|Ht0 ] ? x\\nxl (t),\\nn\\n?\\n\\n?\\n\\n(13)\\n\\nl=1\\n\\nwhere n is the number of times that we simulate the opinion dynamics and x?l (t) gathers the users?\\nopinion at time t for the l-th simulation. Moreover, we have the following theoretical guarantee\\n(proven in Appendix G):\\nTheorem 6 Simulate the opinion dynamics up to time t > t0 the following number of times:\\nn?\\n\\n1\\n2\\n(6?max\\n+ 4xmax \\u000f) log(2/?),\\n3\\u000f2\\n\\n(14)\\n\\n2\\n2\\nwhere ?max\\n= maxu?G ?H\\n(x?u (t)|Ht0 ) is the maximum variance of the users? opinions, which\\nt \\\\Ht0\\nwe analyze in Appendix G, and xmax ? |xu (t)|, ?u ? G is an upper bound on the users? (absolute)\\nopinions. Then, for each user u ? G, the error between her true and estimated average opinion\\nsatisfies that |?\\nx?u (t) ? EHt \\\\Ht0 [x?u (t)|Ht0 ]| ? \\u000f with probability at least 1 ? ?.\\n\\n4\\n4.1\\n\\nExperiments\\nExperiments on synthetic data\\n\\nWe first provide empirical evidence that our model is able to produce different types of opinion\\ndynamics, which may or may not converge to a steady state of consensus or polarization. Then, we\\nshow that our model estimation and simulation algorithms as well as our predictive formulas scale\\nto networks with millions of users and events. Appendix J contains an evaluation of the accuracy of\\nour model parameter estimation method.\\nDifferent types of opinion dynamics. We first simulate our model on two different small networks\\nusing Poisson intensities, i.e., ??u (t) = ?u , ?u ? U (0, 1) ?u, and then simulate our model on the\\nsame networks while using Hawkes intensities with bvu ? U (0, 1) on 5% of the nodes, chosen at\\nrandom, and the original Poisson intensities on the remaining nodes. Figure 1 summarizes the results, which show that (i) our model is able to produce opinion dynamics that converge to consensus\\n(second column) and polarization (third column); (ii) the opinion forecasting formulas described in\\nSection 3 closely match an simulation based estimation (second column); and, (iii) the evolution of\\n6\\n\\n\\fNodes\\n\\n105\\nPoisson\\nHawkes\\n104\\n103\\n102\\n101\\n100\\n10?1\\n10?2\\n102 103 104\\n\\n10\\n\\n5\\n\\n10\\n\\n6\\n\\n15\\n\\n10\\n\\n2\\n\\n10\\n\\n3\\n\\n10\\n\\n4\\n\\nNodes\\n\\n10\\n\\n5\\n\\n10\\n\\n6\\n\\nTime (s)\\n\\nTime (s)\\n\\n105\\n104\\n103\\n102\\n101\\n101\\n10?1\\n10?2101\\n\\nTime(s)\\n\\nTime (s)\\n\\n105\\nInformational\\nTemporal\\n104\\n103\\n102\\n1\\n10\\n100\\n10?1\\n10?2101 102 103 104\\n\\n10\\n\\n5\\n\\nNodes\\n\\n10\\n\\n6\\n\\n10\\n\\n7\\n\\n(a) Estimation vs # nodes (b) Simulation vs # nodes (c) Forecast vs # nodes\\n\\nPoisson\\nHawkes\\n\\n10\\n5\\n02\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\nForecast-Time[T(hr)]\\n\\n(d) Forecast vs T\\n\\nFigure 2: Panels (a) and (b) show running time of our estimation and simulation procedures against\\nnumber of nodes, where the average number of events per node is 10. Panels (c) and (d) show the\\nrunning time needed to compute our analytical formulas against number of nodes and time horizon\\nT = t ? t0 , where the number of nodes is 103 . In Panel (c), T = 6 hours. For all panels, the average\\ndegree per node is 30. The experiments are carried out in a single machine with 24 cores and 64 GB\\nof main memory.\\nthe average opinion and whether opinions converge to a steady state of consensus or polarization\\ndepend on the functional form of message intensity5 .\\nScalability. Figure 2 shows that our model estimation and simulation algorithms, described in Sections 2.1 and 2.2, and our analytical predictive formulas, described in Section 3.1, scale to networks\\nwith millions of users and events. For example, our algorithm takes 20 minutes to estimate the\\nmodel parameters from 10 million events generated by one million nodes using a single machine\\nwith 24 cores and 64 GB RAM.\\n4.2 Experiments on real data\\nWe use real data gathered from Twitter to show that our model can forecast users? opinions more\\naccurately than six state of the art methods [7, 8, 9, 15, 19, 26] (see Appendix L).\\nExperimental Setup. We experimented with five Twitter datasets about current real-world events\\n(Politics, Movie, Fight, Bollywood and US), in which, for each recorded message i, we compute its\\nsentiment value mi using a popular sentiment analysis toolbox, specially designed for Twitter [13].\\nHere, the sentiment takes values m ? (?1, 1) and we consider the sentiment polarity to be simply\\nsign(m). Appendix K contains further details and statistics about these datasets.\\nOpinion forecasting. We first evaluate the performance of our model at predicting sentiment (expressed opinion) at a message level. To do so, for each dataset, we first estimate the parameters of\\nour model, SLANT, using messages from a training set containing the (chronologically) first 90%\\nof the messages. Here, we set the decay parameters of the exponential triggering kernels ?(t) and\\ng(t) by cross-validation. Then, we evaluate the predictive performance of our opinion forecasting\\nformulas using the last 10% of the messages6 . More specifically, we predict the sentiment value m\\nfor each message posted by user u in the test set given the history up to T hours before the time\\nof the message as m\\n? = EHt \\\\Ht?T [x?u (t)|Ht?T ]. We compare the performance of our model with\\nthe asynchronous linear model (AsLM) [8], DeGroot?s model [9], the voter model [26], the biased\\nvoter model [7], the flocking model [15], and the sentiment prediction method based on collaborative filtering by Kim et al. [19], in terms of: (i) the mean squared error between the true (m) and the\\nestimated (m)\\n? sentiment value for all messages in the held-out set, i.e., E[(m ? m)\\n? 2 ], and (ii) the\\nfailure rate, defined as the probability that the true and the estimated polarity do not coincide, i.e.,\\nP(sign(m) 6= sign(m)).\\n?\\nFor the baselines algorithms, which work in discrete time, we simulate NT\\nrounds in (t ? T, t), where NT is the number of posts in time T . Figure 3 summarizes the results,\\nwhich show that: (i) our opinion forecasting formulas consistently outperform others both in terms\\nof MSE (often by an order of magnitude) and failure rate;7 (ii) its forecasting performance degrades\\ngracefully with respect to T , in contrast, competing methods often fail catastrophically; and, (iii) it\\nachieves an additional mileage by using Hawkes processes instead of Poisson processes. To some\\nextent, we believe SLANT?s superior performance is due to its ability to leverage historical data to\\nlearn its model parameters and then simulate realistic temporal patterns.\\nFinally, we look at the forecasting results at a network level and show that our forecasting formulas\\ncan also predict the evolution of opinions macroscopically (in terms of the average opinion across\\nusers). Figure 4 summarizes the results for two real world datasets, which show that the forecasted\\n5\\n\\nFor these particular networks, Poisson intensities lead to consensus while Hawkes intensities lead to polarization, however, we did find\\nother examples in which Poisson intensities lead to polarization and Hawkes intensities lead to consensus.\\n6\\nHere, we do not distinguish between analytical and sampling based forecasting since, in practice, they closely match each other.\\n7\\nThe failure rate is very close to zero for those datasets in which most users post messages with the same polarity.\\n\\n7\\n\\n\\fFlocking\\n\\nCollab-Filter\\n\\nBiasedVoter\\n\\nDeGroot\\n\\nLinear\\n\\nSLANT (H)\\n\\nSLANT (P)\\n\\nVoter\\n\\n101\\n\\nMSE\\n\\n100\\n\\n10?1\\n\\nFailure-Rate\\n\\n10?20\\n\\n2\\n\\n1\\n\\n4\\n\\n6\\n\\n8\\n\\n10 0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10 0\\n\\n2\\n\\nT, hours\\n\\n4\\n\\n6\\n\\n8\\n\\n10 0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10 0\\n\\n2\\n\\nT, hours\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n0\\n\\n2\\n\\nT, hours\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n0\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n0\\n\\n2\\n\\nT, hours\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\n4\\n\\n6\\n\\n8\\n\\n10\\n\\nT, hours\\n\\n0.8\\n0.6\\n0.4\\n\\n0.2\\n00\\n\\n2\\n\\nT, hours\\n\\n(a) Politics\\n\\nT, hours\\n\\n(b) Movie\\n\\nT, hours\\n\\n(c) Fight\\n\\nT, hours\\n\\n(d) Bollywood\\n\\nT, hours\\n\\n(e) US\\n\\n0.6\\n\\nm(t)\\n?\\nx\\n?(t)\\nT = 1h\\nT = 3h\\nT = 5h\\n\\n0.5\\n0.4\\n28 April\\n\\n2 May\\nTime\\n\\n5 May\\n\\n0.8\\n0.7\\n0.6\\n\\nm(t)\\n?\\nx\\n?(t)\\nT = 1h\\nT = 3h\\nT = 5h\\n\\n0.5\\n0.4\\n28 April\\n\\n2 May\\nTime\\n\\n5 May\\n\\n(a) Tw: Movie (Hawkes) (b) Tw: Movie (Poisson)\\n\\n0.8 m(t)\\n?\\nx\\n?(t)\\n0.6\\nT = 1h\\n0.4 T = 3h\\nT\\n= 5h\\n0.2\\n0\\n-0.2\\n-0.47 April\\n10 April\\n\\nTime\\n\\n13 April\\n\\n(c) Tw: US (Hawkes)\\n\\nAverage Opinion?\\n\\n0.7\\n\\nAverage Opinion?\\n\\n0.8\\n\\nAverage Opinion?\\n\\nAverage Opinion?\\n\\nFigure 3: Sentiment prediction performance using a 10% held-out set for each real-world dataset.\\nPerformance is measured in terms of mean squared error (MSE) on the sentiment value, E[(m ?\\nm)\\n? 2 ], and failure rate on the sentiment polarity, P(sign(m) 6= sign(m)).\\n?\\nFor each message in the\\nheld-out set, we predict the sentiment value m given the history up to T hours before the time of\\nthe message, for different values of T . Nowcasting corresponds to T = 0 and forecasting to T > 0.\\nThe sentiment value m ? (?1, 1) and the sentiment polarity sign (m) ? {?1, 1}.\\n0.8 m(t)\\n?\\nx\\n?(t)\\n0.6\\nT = 1h\\n0.4 T = 3h\\nT\\n= 5h\\n0.2\\n0\\n-0.2\\n-0.47 April\\n10 April\\n\\nTime\\n\\n13 April\\n\\n(d) Tw: US (Poisson)\\n\\nFigure 4: Macroscopic sentiment prediction given by our model for two real-world datasets. The\\npanels show the observed sentiment m(t)\\n?\\n(in blue, running average), inferred opinion x\\n?(t) on the\\ntraining set (in red), and forecasted opinion EHt \\\\Ht?T [xu (t)|Ht?T ] for T = 1, 3, and 5 hours on\\nthe test set (in black, green and gray, respectively), where the symbol ? denotes average across users.\\nopinions become less accurate as the time T becomes larger, since the average is computed on longer\\ntime periods. As expected, our model is more accurate when the message intensities are modeled\\nusing multivariate Hawkes. We found qualitatively similar results for the remaining datasets.\\n\\n5\\n\\nConclusions\\n\\nWe proposed a modeling framework of opinion dynamics, whose key innovation is modeling users?\\nlatent opinions as continuous-time stochastic processes driven by a set of marked jump stochastic\\ndifferential equations (SDEs) [14]. Such construction allows each user?s latent opinion to be modulated over time by the opinions asynchronously expressed by her neighbors as sentiment messages.\\nWe then exploited a key property of our model, the Markov property, to design efficient parameter\\nestimation and simulation algorithms, which scale to networks with millions of nodes. Moreover, we\\nderived a set of novel predictive formulas for efficient and accurate opinion forecasting and identified\\nconditions under which opinions converge to a steady state of consensus or polarization. Finally, we\\nexperimented with real data gathered from Twitter and showed that our framework achieves more\\naccurate opinion forecasting than state-of-the-arts.\\nOur model opens up many interesting venues for future work. For example, in Eq. 4, our model\\nassumes a linear dependence between users? opinions, however, in some scenarios, this may be a\\ncoarse approximation. A natural follow-up to improve the opinion forecasting accuracy would be\\nconsidering nonlinear dependences between opinions. It would be interesting to augment our model\\nto jointly consider correlations between different topics. One could leverage our modeling framework to design opinion shaping algorithms based on stochastic optimal control [14, 25]. Finally, one\\nof the key modeling ideas is realizing that users? expressed opinions (be it in the form of thumbs\\nup/down or text sentiment) can be viewed as noisy discrete samples of the users? latent opinion localized in time. It would be very interesting to generalize this idea to any type of event data and\\nderive sampling theorems and conditions under which an underlying general continuous signal of\\ninterest (be it user?s opinion or expertise) can be recovered from event data with provable guarantees.\\nAcknowledgement: Abir De is partially supported by Google India under the Google India PhD Fellowship\\nAward, and Isabel Valera is supported by a Humboldt post-doctoral fellowship.\\n\\n8\\n\\n\\fReferences\\n[1] O. Aalen, ?. Borgan, and H. Gjessing. Survival and event history analysis: a process point of view.\\nSpringer Verlag, 2008.\\n[2] A. H. Al-Mohy and N. J. Higham. Computing the action of the matrix exponential, with an application\\nto exponential integrators. SIAM journal on scientific computing, 33(2):488?511, 2011.\\n[3] R. Axelrod. The dissemination of culture a model with local convergence and global polarization. Journal\\nof conflict resolution, 41(2):203?226, 1997.\\n[4] E. G. Birgin, J. M. Mart??nez, and M. Raydan. Nonmonotone spectral projected gradient methods on\\nconvex sets. SIAM Journal on Optimization, 10(4), 2000.\\n[5] C. Blundell, J. Beck, and K. A. Heller. Modelling reciprocating relationships with hawkes processes. In\\nAdvances in Neural Information Processing Systems, pages 2600?2608, 2012.\\n[6] P. Clifford and A. Sudbury. A model for spatial conflict. Biometrika, 60(3):581?588, 1973.\\n[7] A. Das, S. Gollapudi, and K. Munagala. Modeling opinion dynamics in social networks. In WSDM, 2014.\\n[8] A. De, S. Bhattacharya, P. Bhattacharya, N. Ganguly, and S. Chakrabarti. Learning a linear influence\\nmodel from transient opinion dynamics. In CIKM, 2014.\\n[9] M. H. DeGroot. Reaching a consensus. Journal of the American Statistical Association, 69(345), 1974.\\n[10] M. Farajtabar, N. Du, M. Gomez-Rodriguez, I. Valera, L. Song, and H. Zha. Shaping social activity by\\nincentivizing users. In NIPS, 2014.\\n[11] M. Farajtabar, Y. Wang, M. Gomez-Rodriguez, S. Li, H. Zha, and L. Song. Coevolve: A joint point\\nprocess model for information diffusion and network co-evolution. In NIPS, 2015.\\n[12] M. Gomez-Rodriguez, D. Balduzzi, and B. Sch?olkopf. Uncovering the Temporal Dynamics of Diffusion\\nNetworks. In ICML, 2011.\\n[13] A. Hannak, E. Anderson, L. F. Barrett, S. Lehmann, A. Mislove, and M. Riedewald. Tweetin?in the rain:\\nExploring societal-scale effects of weather on mood. In ICWSM, 2012.\\n[14] F. B. Hanson. Applied Stochastic Processes and Control for Jump-Diffusions. SIAM, 2007.\\n[15] R. Hegselmann and U. Krause. Opinion dynamics and bounded confidence models, analysis, and simulation. Journal of Artificial Societies and Social Simulation, 5(3), 2002.\\n[16] D. Hinrichsen, A. Ilchmann, and A. Pritchard. Robustness of stability of time-varying linear systems.\\nJournal of Differential Equations, 82(2):219 ? 250, 1989.\\n[17] P. Holme and M. E. Newman. Nonequilibrium phase transition in the coevolution of networks and opinions. Physical Review E, 74(5):056108, 2006.\\n[18] T. Karppi and K. Crawford. Social media, financial algorithms and the hack crash. TC&S, 2015.\\n[19] J. Kim, J.-B. Yoo, H. Lim, H. Qiu, Z. Kozareva, and A. Galstyan. Sentiment prediction using collaborative\\nfiltering. In ICWSM, 2013.\\n[20] J. Leskovec, D. Chakrabarti, J. M. Kleinberg, C. Faloutsos, and Z. Ghahramani. Kronecker graphs: An\\napproach to modeling networks. JMLR, 2010.\\n[21] B. Pang and L. Lee. Opinion mining and sentiment analysis. F&T in information retrieval, 2(1-2), 2008.\\n[22] B. H. Raven. The bases of power: Origins and recent developments. Journal of social issues, 49(4), 1993.\\n[23] Y. Saad and M. H. Schultz. Gmres: A generalized minimal residual algorithm for solving nonsymmetric\\nlinear systems. SIAM Journal on scientific and statistical computing, 7(3):856?869, 1986.\\n[24] I. Valera and M. Gomez-Rodriguez. Modeling adoption and usage of competing products. In Proceedings\\nof the 2015 IEEE International Conference on Data Mining, 2015.\\n[25] Y. Wang, E. Theodorou, A. Verma, and L. Song. Steering opinion dynamics in information diffusion\\nnetworks. arXiv preprint arXiv:1603.09021, 2016.\\n[26] M. E. Yildiz, R. Pagliari, A. Ozdaglar, and A. Scaglione. Voting models in random networks. In Information Theory and Applications Workshop, pages 1?7, 2010.\\n\\n9\\n\\n\\f\",\n          \"693\\n\\nTeaching Artificial Neural Systems to Drive:\\nManual Training Techniques for Autonomous Systems\\n\\nJ. F. Shepanski and S. A. Macy\\n\\nTRW, Inc .\\nOne Space Park, 02/1779\\nRedondo Beach, CA 90278\\n\\nAbetract\\nWe have developed a methodology for manually training autononlous control systems\\nbased on artificial neural systems (ANS). In applications where the rule set governing an expert's\\ndecisions is difficult to formulate, ANS can be used to ext.ra.c:t rules by associating the information\\nan expert receives with the actions h~ takes . Properly constructed networks imitate rules of\\nbehavior that permits them to function autonomously when they are trained on the spanning set\\nof possible situations. This training can be provided manually, either under the direct. supervision\\nor a system trainer, or indirectly using a background mode where the network assimilates training\\ndata as the expert perrorms his day-to-day tasks. To demonstrate these methods we have trained\\nan ANS network to drive a vehicle through simulated rreeway traffic.\\n\\nI ntJooducticn\\nComputational systems employing fine grained parallelism are revolutionizing the way we\\napproach a number or long standing problems involving pattern recognition and cognitive processing. The field spans a wide variety or computational networks, rrom constructs emulating neural\\nrunctions, to more crystalline configurations that resemble systolic arrays. Several titles are used\\nto describe this broad area or research, we use the term artificial neural systems (ANS). Our concern in this work is the use or ANS ror manually training certain types or autonomous systems\\nwhere the desired rules of behavior are difficult to rormulate.\\nArtificial neural systems consist of a number or processing elements interconnected in a\\nweighted, user-specified fashion, the interconnection weights acting as memory ror the system.\\nEach processing element calculatE',> an output value based on the weighted sum or its inputs. In\\naddition, the input data is correlated with the output or desired output (specified by an instructive\\nagent) in a training rule that is used to adjust the interconnection weights. In this way the ne~\\nwork learns patterns or imitates rules of behavior and decision making.\\nThe partiCUlar ANS architecture we use is a variation of Rummelhart et. al. [lJ multi-layer\\nperceptron employing the generalized delta rule (GD R). Instead of a single, multi-layer ,structure, our final network has a a multiple component or \\\"block\\\" configuration where one blOt'k'~\\noutput reeds into another (see Figure 3). The training methodology we have developed is not\\ntied to a particular training rule or architecture and should work well with alternative networks\\nlike Grossberg's adaptive resonance model[2J.\\n\\n? American Institute of Physics 1988\\n\\n\\f694\\n\\nThe equations describing the network are derived and described in detail by Rumelhart et.\\nal.[l]. In summary, they are:\\nTransfer function:\\n\\nSj =\\n\\n?\\n\\nE WjiOi;\\n\\n(1)\\n\\ni-O\\n\\nWeight adaptation rule:\\nError calculation:\\n\\nAwl'?? =( 1- a l'..)n., l'??0 J?0??\\n\\nOJ\\n\\n+ a l'??Awp.revious\\n.'\\nl'\\n\\n'\\\"\\n\\n=0j{1- OJ) E0.tW.ti,\\n\\n( 2)\\n\\n( 3)\\n\\n.t=1\\n\\nwhere OJ is the output or processing element j or a sensor input, wi is the interconnection weight\\nleading from element ito i, n is the number of inputs to j, Aw is the adjustment of w, '1 is the\\ntraining constant, a is the training \\\"momentum,\\\" OJ is the calculated error for element i, and m\\nis the Canout oC a given element. Element zero is a constant input, equal to one, so that. WjO is\\nequivalent to the bias threshold of element j. The (1- a) factor in equation (2) differs from standard GDR formulation, but. it is useful for keeping track of the relative magnitudes of the two\\nterms. For the network's output layer the summation in equation (3) is replaced with the\\ndifference between the desired and actual output value of element j.\\nThese networks are usually trained by presenting the system with sets of input/output data\\nvectors in cyclic fashion, the entire cycle of database presentation repeated dozens of times . This\\nmethod is effective when the training agent is a computer operating in batch mode, but would be\\nintolerable for a human instructor. There are two developments that will help real-time human\\ntraining. The first is a more efficient incorporation of data/response patterns into a network. The\\nsecond, which we are addressing in this paper, is a suitable environment wherein a man and ANS\\nnetwork can interact in training situation with minimum inconvenience or boredom on the\\nhuman's part. The ability to systematically train networks in this fashion is extremely useful for\\ndeveloping certain types of expert systems including automatic signal processors, autopilots,\\nrobots and other autonomous machines. We report a number of techniques aimed at facilitating\\nthis type of training, and we propose a general method for teaching these networks .\\nSystem. Development\\n\\nOur work focuses on the utility of ANS for system control. It began as an application of\\nBarto and Sutton's associative search network[3]. Although their approach was useful in a\\nnumber of ways, it fell short when we tried to use it for capturing the subtleties of human\\ndecision-making. In response we shifted our emphasis rrom constructing goal runctions for\\nautomatic learning, to methods for training networks using direct human instruction. An integral\\npart or this is the development or suitable interraces between humans, networks and the outside\\nworld or simulator. In this section we will report various approaches to these ends, and describe a\\ngeneral methodology for manually teaching ANS networks . To demonstrate these techniques we\\ntaught a network to drive a robot vehicle down a simulated highway in traffic. This application\\ncombines binary decision making and control of continuous parameters.\\nInitially we investigated the use or automatic learning based on goal functions[3] for training control systems. We trained a network-controlled vehicle to maintain acceptable following\\ndistances from cars ahead or it. On a graphics workstation, a one lane circular track was\\n\\n\\f695\\n\\nconstructed and occupied by two vehicles: a network-controlled robot car and a pace car that\\nvaried its speed at random .. Input data to the network consisted of the separation distance and\\nthe speed of the robot vehicle . The values of a goal function were translated into desired output\\nfor GDR training. Output controls consisted of three binary decision elements : 1) accelerate one\\nincrement of speed, 2) maintain speed, and 3) decelerate one increment of speed. At all times\\nthe desired output vector had exactly one of these three elements active . The goal runction was\\nquadratic with a minimum corresponding to the optimal following distance. Although it had no\\ndirect control over the simulation, the goal function positively or negatively reinforced the\\nsystem's behavior.\\nThe network was given complete control of the robot vehicle, and the human trainer had\\nno influence except the ability to start and terminate training. This proved unsatisractory because\\nthe initial system behavior--governed by random interconnection weights--was very unstable. The\\nrobot tended to run over the car in rront of it before significant training occurred . By carerully\\nhalting and restarting training we achieved stable system behavior. At first the rollowing distance\\nmaintained by the robot car oscillated as ir the vehicle was attached by a sj)ring to the pace car.\\nThis activity gradually damped. Arter about one thousand training steps the vehicle maintained\\nthe optimal following distance and responded quickly to changes in the pace car's speed.\\nConstructing composite goal functions to promote more sophisticated abilities proved\\ndifficult, even ill-defined, because there were many unspecified parameters. To generate goal\\nrunctions ror these abilities would be similar to conventional programming--the type or labor we\\nwant to circumvent using ANS. On the other hand, humans are adept at assessing complex situations and making decisions based on qualitative data, but their \\\"goal runctions\\\" are difficult ir not\\nimpossible to capture analytically. One attraction of ANS is that it can imitate behavior based on\\nthese elusive rules without rormally specifying them. At this point we turned our efforts to\\nmanual training techniques.\\nThe initially trained network was grafted into a larger system and augmented with additional inputs: distance and speed inrormation on nearby pace cars in a second traffic lane, and an\\noutput control signal governing lane changes . The original network's ability to maintain a safe\\nfollowing distance was retained intact. Thts grafting procedure is one of two methods we studied\\nfor adding ne .... abilities to an existin, system. (The second, which employs a block structure, is\\ndescribed below.) The network remained in direct control of the robot vehicle, but a human\\ntrainer instructed it when and when not to change lanes. His commands were interpreted as the\\ndesired output and used in the GDR training algorithm. This technique, which we call coaching,\\nproved userul and the network quickly correlated its environmental inputs with the teacher's\\ninstructions. The network became adept at changing lanes and weaving through traffic. We found\\nthat the network took on the behavior pattern or its trainer. A conservative teacher produced a\\ntimid network, while an aggressive tzainer produced a network that tended to cut off other automobiles and squeeze through tight openings . Despite its success, the coaching method of training\\ndid not solve the problem or initial network instability.\\nThe stability problem was solved by giving the trainer direct control over the simulation.\\nThe system configuration (Figure 1), allows the expert to exert control or release it to the n~t?\\nwork. During initial tzaining the expert is in the driver's seat while the network acts the role of\\n\\n\\f696\\n\\napprentice. It receives sensor information, predicts system commands, and compares its predictions. against the desired output (ie. the trainer's commands) . Figure 2 shows the data and command flow in detail. Input data is processed through different channels and presented to the\\ntrainer and network. Where visual and audio formats are effective for humans, the network uses\\ninformation in vector form. This differentiation of data presentation is a limitation of the system;\\nremoving it is a cask for future ~search. The trainer issues control commands in accordance with\\nhis assigned ~k while the network takes the trainer's actions as desired system responses and\\ncorrelates these with the input. We refer to this procedure as master/apprentice training, network\\ntraining proceeds invisibly in the background as the expert proceeds with his day to day work. It\\navoids the instability problem because the network is free to make errors without the adverse\\nconsequence of throwing the operating environment into disarray.\\nI\\n\\nInput\\n\\nWorld (--> sensors)\\n\\nl+\\n\\nor\\n\\nSimulation\\n~------------------~\\n\\n~\\n\\nActuation\\n\\nI\\n\\nNe',WOrk\\n\\n~-\\n\\nI\\n\\nExpert\\n\\nCommands\\n+\\n~------~---------------------------~\\nJ\\n\\nFigure 1. A scheme for manually training ANS networks. Input data is received by both\\nthe network and trainer. The trainer issues commands that are actuated (solid command\\nline). or he coaches the network in how it ought to respond (broken command line).\\n\\n--+ Commands\\n\\nPreprocessing\\ntortunan\\nInput\\ndata\\nPreprocessing\\nfor network\\n\\nN twork\\ne\\n\\nt\\n\\n--+\\n\\nPredicted\\ncommands\\n\\n~\\n9'l. Actuation\\n\\n.1-r\\\"\\n\\n'-------------.\\nCoaching/emphasis\\n\\nTraining\\nrule\\n\\nFegure 2. Data and convnand flow In the training system. Input data is processed and presented\\n\\nto the trainer and network. In master/appre~ice training (solid command Hne). the trainer's\\norders are actuated and the network treats his commands as the system's desired output. In\\ncoaching. the network's predicted oonvnands are actuated (broken command line). and the\\ntrainer influences weight adaptation by specifying the desired system output and controlHng\\nthe values of trailing constants-his -suggestions- are not cirec:tty actuated.\\nOnce initial. bacqround wainmg is complete, the expert proceeds in a more formal\\nmanner to teach the network. He releases control of the command system to the network in\\norder to evaluate ita behavior and weaknesses. He then resumes control and works through a\\n\\n\\f697\\n\\nseries of scenarios designed to train t.he network out of its bad behavior. By switching back and\\nforth. between human and network control, the expert assesses the network's reliability and\\nteaches correct responses as needed. We find master/apprentice training works well for behavior\\ninvolving continuous functions, like steering. On the other hand, coaching is appropriate for decision Cunctions, like when Ule car ought to pass. Our methodology employs both techniques.\\nThe Driving Network\\nThe fully developed freeway simulation consists of a two lane highway that is made of\\njoined straight and curved segments which vary at. random in length (and curvature). Several\\npace cars move at random speeds near the robot vehicle. The network is given the tasks of tracking the road, negotiating curves. returning to the road if placed far afield, maintaining safe distances from the pace cars, and changing lanes when appropriate. Instead of a single multi-layer\\nstructure, the network is composed of two blocks; one controls the steering and the other regulates speed and decides when the vehicle should change lanes (Figure 3). The first block receives\\ninformation about the position and speed of the robot vehicle relative to other ears in its vicinity.\\nIts output is used to determine the automobile's speed and whet.her the robot should change\\nlanes . The passing signal is converted to a lane assignment based on the car's current lane position. The second block receives the lane assignment and data pertinent to the position and orientation of the vehicle with respect to the road. The output is used to determine the steering angle\\nof the robot car.\\n\\nBlock 1\\n\\nInputs\\n\\nOutputs\\n\\nConstant.\\nSpeed.\\nDisl. Ahead, Pl ?\\nDisl. Ahead, Ol ?\\nDist. Behind, Ol ?\\nReI. Speed Ahead, Pl ?\\nReI. Speed Ahead, Ol ?\\nReI. Speed Behind, Ol ?\\n\\nI\\n\\nSpeed\\nChange lanes\\n\\n?\\n\\nSteering Angle\\n\\nConvert lane change to lane number\\nConstant\\nRei. Orientation\\n-..--t~ lane Nurmer\\nlateral Dist.\\nCurvature\\n\\n?\\n?\\n?\\n?\\n?\\n\\n??\\n?\\n\\nFigure 3. The two blocks of the driving ANS network. Heavy arrows Indicate total interconnectivity\\nbetween layers. PL designates the traffic lane presently oca.apied by the robot vehicle, Ol refers\\nto the other lane, QJrvature refers to the road, lane nurrber is either 0 or 1, relative orientation and\\nlateral distance refers to the robot car's direction and podion relative to the road'l direction and\\ncenter line. respectively.\\n.\\n\\n\\f698\\n\\nThe input data is displayed in pictorial and textual form to the driving instructor. He views\\nthe road and nearby vehicles from the perspective of the driver's seat or overhead. The network\\nreceives information in the form of a vector whose elements have been scaled to unitary order,\\nO( 1) . Wide ranging input parameters, like distance, are compressed using the hyperbolic tangent\\nor logarithmic functions . In each block , the input layer is totally interconnected to both the ou~\\nput and a hidden layer. Our scheme trains in real time, and as we discuss later, it trains more\\nsmoothly with a small modification of the training algorithm .\\nOutput is interpreted in two ways: as a binary decision or as a continuously varying parameter. The first simply compares the sigmoid output against a threshold. The second scales the\\noutput to an appropriate range for its application . For example, on the steering output element, a\\n0.5 value is interpreted as a zero steering angle. Left and right turns of varying degrees are initiated when this output is above or below 0.5, respectively.\\nThe network is divided into two blocks that can be trained separately. Beside being conceptually easier to understand , we find this component approach is easy to train systematically.\\nBecause each block has a restricted, well-defined set of tasks, the trainer can concentrate\\nspecifically on those functions without being concerned that other aspects of the network behavior\\nare deteriorating.\\n\\\"'e trained the system from bottom up, first teaching the network to stay on the road ,\\nnegotiate curves , chan~e lanes, and how to return if the vehicle strayed off the highway. Block 2,\\nresponsible for steering, learned these skills in a few minutes using the master/apprentice mode.\\nIt tended to steer more slowly than a human but further training progressively improved its\\nresponsiveness.\\nWe experimented with different trammg constants and \\\"momentum\\\" values. Large \\\"\\nvalues, about 1, caused weights to change too coarsely. \\\" values an order of magnitude smaller\\nworked well . We found DO advantage in using momentum for this method of training , in fact,\\nthe system responded about three times more slowly when 0 =0.9 than when the momentt:m\\nterm was dropped. Our standard training parameters were\\\" =0.2, and Cl' =00\\n\\na)\\n\\n~\\n\\nDb)~~\\n\\n=D-=-~=~~--=~--= ~\\n\\nFigure 4. Typical behavior of a network-controlled vehicle (dam rectangle) when trained by\\na) a conservative miYer, ItI:I b}. reckless driver. Speed Is indicated by the length of the arrows.\\nAfter Block 2 \\\"Was trained, we gave steering control to the network and concentrated on\\nteaching the network to change lanes and adjust speed. Speed control in this ('\\\"asP. was a continuous variable and was best taught using master/apprentice training. On the other hand, the binary\\ndecision to change lanes was best taught by coaching . About ten minutes of training were needed\\nto teach the network to weave through traffic. We found that the network readily adapts the\\n\\n\\f699\\n\\nbehavioral pattern of its trainer. A conservative trainer generated a network that hardly ever\\npassed, while an aggressive trainer produced a network that drove recklessly and tended to cut off\\nother-cars (Figure 4).\\nDiscussion\\nOne of the strengths of el:pert 5ystf'mS based on ANS is that the use of input data in the\\ndecision making and control proc~ss does not have to be specified . The network adapts its internal weights to conform to input/ output correlat.ions it discovers . It is important, however, that\\ndata used by the human expert is also available to the network. The different processing of sensor data for man and network may have important consequences, key information may be\\npresented to the man but not. the machine.\\nThis difference in data processing is particularly worrisome for image data where human\\nability to extract detail is vastly superior to our au tomatic image processing capabilities. Though\\nwe would not require an image processing system to understand images, it would have to extract\\nrelevant information from cluttered backgrounds. Until we have sufficiently sophisticated algorithms or networks to do this, our efforts at constructing expert systems which halldle image data\\nare handicapped .\\nScaling input data to the unitary order of magnitude is important for training stability. 111is\\nis evident from equations (1) and (2) . The sigmoid transfer function ranges from 0.1 to 0.9 in\\napproximat.eiy four units, that is, over an 0(1) domain. If system response must change in reaction to a large, O( n) swing of a given input parameter, the weight associated with that input will\\nbe trained toward an O( n- 1) magnitude. On the other hand, if the same system responds to an\\ninput whose range is O( 1), its associated weight will also be 0(1). The weight adjustment equation does not recognize differences in weight magnitude, therefore relatively small weights will\\nundergo wild magnitude adjustments and converge weakly. On the other hand, if all input parameters are of the same magnitude their associated weights will reflect this and the training constant\\ncan be adjusted for gentle weight convergence . Because the output of hidden units are constrained between zero and one, O( 1) is a good target range for input parameters. Both the hyperbolic tangent and logarithmic functions are useful for scaling wide ranging inputs . A useful form\\nof the latter is\\n.8[I+ln(x/o)]\\n.8x/o\\n-.8[I+ln(-%/o)]\\n\\nif o<x,\\nif-o::;x::;o,\\nifx<-o,\\n\\n( 4)\\n\\nwhere 0>0 and defines the limits of the intermediate linear section, and .8 is a scaling factor.\\nThis symmetric logarithmic function is continuous in its first derivative, and useful when network\\nbehavior should change slowly as a parameter increases without bound. On the othl'r hand, if the\\nsystem should approach a limiting behavior, the tanh function is appropriate.\\nWeight adaptation is also complicated by relaxing the common practice of restricting interconnections to adjacent layers. Equation (3) shows that the calculated error for a hidden layergiven comparable weights, fanouts and output errors-will be one quarter or less than that of the\\n\\n\\f700\\n\\noutput layer. This is caused by the slope ractor, 0 .. ( 1- oil. The difference in error magnitudes is\\nnot noticeable in networks restricted to adjacent layer interconnectivity. But when this constraint\\nis released the effect of errors originating directly from an output unit has 4\\\" times the magnitude\\nand effect of an error originating from a hidden unit removed d layers from the output layer.\\nCompared to the corrections arising from the output units, those from the hidden units have little\\ninfluence on weight adjustment, and the power of a multilayer structure is weakened . The system\\nwill train if we restrict connections to adjacent layers, but it trains slowly. To compensate for this\\neffect we attenuate the error magnitudes originating from the output layer by the above factor.\\nThis heuristic procedure works well and racilitates smooth learning.\\nThough we have made progress in real-time learning systems using GDR, compared to\\nhumans-who can learn from a single data presentation-they remain relatively sluggish in learning\\nand response rates. We are interested in improvements of the GDR algorithm or alternative\\narchitectures that facilitate one-shot or rapid learning. In the latter case we are considering least\\nsquares restoration techniquesl4] and Grossberg and Carpenter's adaptive resonance modelsI3,5].\\nThe construction of automated expert systems by observation of human personnel is\\nattractive because of its efficient use of the expert's time and effort. Though the classic AI\\napproach of rule base inference is applicable when such rules are clear cut and well organized, too\\noften a human expert can not put his decision making process in words or specify the values of\\nparameters that influence him . The attraction or ANS based systems is that imitations of expert\\nbehavior emerge as a natural consequence of their training.\\n\\nReferenees\\n1) D. E. Rumelhart, G . E. Hinton, and R. J. Williams, \\\"Learning Internal Representations by\\nError Propagation,\\\" in Parallel D~tributed Proceuing: Ezploration~ in the Micro~trvcture 0/ Cognition,\\nVol. I, D. E . Rumelhart and J. L. McClelland (Eds.)' chap. 8, (1986), Bradford BooksjMIT Press,\\nCambridge\\n\\n2) S. Grossberg,\\n\\nStudie~\\n\\n0/ Mind and Brain, (1982), Reidel, Boston\\n\\n3) A. Barto and R. Sutton, \\\"Landmark Learning: An Illustration of Associative Search,\\\" BiologicaIC,6emetiu,42, (1981), p.l\\n4) A. Rosenfeld and A . Kak, Digital Pieture Proeming, Vol. 1, chap. 7, (1982), Academic Press,\\nNew York\\n\\n5) G. A. Carpenter and S. Grossberg, \\\"A Massively Parallel Architecture for a Self-organizing\\nNeural Pattern Recognition Machine,\\\" Computer Vision, Graphiu and Image Procu,ing, 37,\\n( 1987), p.54\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoZOvm6J8JIt"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xrxqc6QE8JIt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2e402c53-47ae-4545-9f12-9210cf9d5f2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2734    generalization dynamics in\\nlms trained linear...\n",
              "5914    adaptive skills adaptive partitions (asap)\\n\\n...\n",
              "4894    provable non-convex robust pca\\npraneeth netra...\n",
              "1080    gaussian process regression with\\nmismatched m...\n",
              "4807    semi-supervised learning with\\ndeep generative...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2734</th>\n",
              "      <td>generalization dynamics in\\nlms trained linear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5914</th>\n",
              "      <td>adaptive skills adaptive partitions (asap)\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4894</th>\n",
              "      <td>provable non-convex robust pca\\npraneeth netra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>gaussian process regression with\\nmismatched m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>semi-supervised learning with\\ndeep generative...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpaPvwoa8JIt"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UNiqJDB48JIu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "d8f925f0-b90c-4d64-8994-8e3b73441255"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g04kzA4h8JIu"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis <a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a_kWdXh18JIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0fcd12-1836-4d60-fa0e-5cb0def9fa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['generalization', 'dynamics', 'lms', 'trained', 'linear', 'networks', 'yves', 'chauvin', 'psychology', 'department', 'stanford', 'university', 'stanford', 'ca', 'abstract', 'simple', 'linear', 'case', 'mathematical', 'analysis', 'training', 'generalization', 'validation', 'performance', 'networks', 'trained', 'gradient', 'descent', 'least', 'mean']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vSexN5Xm8JIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250d1a3d-fc44-4e58-8a64-7d059c88f20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 20), (7, 3), (8, 6), (9, 1), (10, 2), (11, 5), (12, 1), (13, 1), (14, 1), (15, 6), (16, 6), (17, 1), (18, 1), (19, 2), (20, 6), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 4), (28, 1), (29, 1)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC3SFAhp8JIu"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HOhEupXx8JIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca07aed7-b137-4707-c70e-a853bdc662fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.007*\"learning\" + 0.005*\"algorithm\" + 0.005*\"data\" + 0.005*\"function\" + '\n",
            "  '0.005*\"set\" + 0.004*\"using\" + 0.004*\"number\" + 0.003*\"two\" + 0.003*\"one\" + '\n",
            "  '0.003*\"error\"'),\n",
            " (1,\n",
            "  '0.006*\"learning\" + 0.006*\"model\" + 0.004*\"algorithm\" + 0.004*\"set\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"number\" + 0.003*\"function\" + 0.003*\"one\" + '\n",
            "  '0.003*\"using\" + 0.003*\"also\"'),\n",
            " (2,\n",
            "  '0.006*\"learning\" + 0.004*\"algorithm\" + 0.004*\"set\" + 0.004*\"data\" + '\n",
            "  '0.004*\"two\" + 0.003*\"model\" + 0.003*\"using\" + 0.003*\"function\" + '\n",
            "  '0.003*\"results\" + 0.003*\"information\"'),\n",
            " (3,\n",
            "  '0.006*\"model\" + 0.005*\"learning\" + 0.005*\"network\" + 0.004*\"set\" + '\n",
            "  '0.004*\"data\" + 0.004*\"neural\" + 0.004*\"training\" + 0.004*\"one\" + '\n",
            "  '0.004*\"networks\" + 0.003*\"algorithm\"'),\n",
            " (4,\n",
            "  '0.007*\"learning\" + 0.005*\"model\" + 0.005*\"data\" + 0.005*\"algorithm\" + '\n",
            "  '0.005*\"set\" + 0.004*\"number\" + 0.004*\"one\" + 0.004*\"function\" + 0.003*\"two\" '\n",
            "  '+ 0.003*\"figure\"'),\n",
            " (5,\n",
            "  '0.005*\"model\" + 0.005*\"using\" + 0.004*\"one\" + 0.004*\"set\" + '\n",
            "  '0.004*\"learning\" + 0.004*\"data\" + 0.004*\"network\" + 0.004*\"function\" + '\n",
            "  '0.004*\"given\" + 0.003*\"algorithm\"'),\n",
            " (6,\n",
            "  '0.008*\"model\" + 0.005*\"learning\" + 0.005*\"data\" + 0.004*\"set\" + '\n",
            "  '0.004*\"network\" + 0.004*\"using\" + 0.003*\"algorithm\" + 0.003*\"number\" + '\n",
            "  '0.003*\"one\" + 0.003*\"time\"'),\n",
            " (7,\n",
            "  '0.007*\"data\" + 0.006*\"learning\" + 0.005*\"model\" + 0.005*\"using\" + '\n",
            "  '0.004*\"algorithm\" + 0.004*\"time\" + 0.004*\"set\" + 0.004*\"networks\" + '\n",
            "  '0.004*\"network\" + 0.003*\"figure\"'),\n",
            " (8,\n",
            "  '0.006*\"model\" + 0.005*\"set\" + 0.005*\"algorithm\" + 0.005*\"learning\" + '\n",
            "  '0.004*\"one\" + 0.004*\"using\" + 0.004*\"number\" + 0.004*\"two\" + '\n",
            "  '0.003*\"network\" + 0.003*\"models\"'),\n",
            " (9,\n",
            "  '0.005*\"algorithm\" + 0.005*\"data\" + 0.004*\"model\" + 0.004*\"one\" + '\n",
            "  '0.004*\"set\" + 0.004*\"learning\" + 0.003*\"using\" + 0.003*\"time\" + '\n",
            "  '0.003*\"network\" + 0.003*\"function\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt8ZVZRk8JIv"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsPxXp4Tcx_O",
        "outputId": "99cde8e8-dd8e-4ce1-fa52-f1898eac997b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cutL8PAl8JIv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "e68ff6b7-7c06-4ebd-e91c-bd0d0b244115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "7     -0.008257  0.001911       1        1  31.188207\n",
              "4     -0.000224 -0.003365       2        1  23.033097\n",
              "6      0.000316  0.006669       3        1   9.160718\n",
              "3     -0.004636  0.001584       4        1   9.119977\n",
              "1      0.000842  0.006089       5        1   8.109766\n",
              "8     -0.001766 -0.007214       6        1   7.378716\n",
              "9      0.000393 -0.001354       7        1   7.185356\n",
              "0      0.006068 -0.002804       8        1   2.619201\n",
              "5     -0.001995 -0.003018       9        1   1.988435\n",
              "2      0.009259  0.001504      10        1   0.216527, topic_info=             Term         Freq        Total Category  logprob  loglift\n",
              "269      learning  1308.000000  1308.000000  Default  30.0000  30.0000\n",
              "971         model  1243.000000  1243.000000  Default  29.0000  29.0000\n",
              "111          data  1197.000000  1197.000000  Default  28.0000  28.0000\n",
              "13      algorithm  1006.000000  1006.000000  Default  27.0000  27.0000\n",
              "438           set  1014.000000  1014.000000  Default  26.0000  26.0000\n",
              "...           ...          ...          ...      ...      ...      ...\n",
              "334           one     1.227585   797.109215  Topic10  -5.9977  -0.3407\n",
              "1055  probability     1.011213   477.879727  Topic10  -6.1915  -0.0230\n",
              "1643       method     1.029892   536.495893  Topic10  -6.1732  -0.1204\n",
              "503      training     1.046189   675.463188  Topic10  -6.1575  -0.3350\n",
              "630         based     1.011775   532.983270  Topic10  -6.1910  -0.1316\n",
              "\n",
              "[877 rows x 6 columns], token_table=       Topic      Freq        Term\n",
              "term                              \n",
              "6754       1  0.196453  abstention\n",
              "6754       2  0.392905  abstention\n",
              "6754       3  0.098226  abstention\n",
              "6754       4  0.049113  abstention\n",
              "6754       5  0.122783  abstention\n",
              "...      ...       ...         ...\n",
              "15517      2  0.179531          zy\n",
              "15517      4  0.179531          zy\n",
              "15517      5  0.179531          zy\n",
              "15517      6  0.179531          zy\n",
              "15517      7  0.179531          zy\n",
              "\n",
              "[3224 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 5, 7, 4, 2, 9, 10, 1, 6, 3])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el2241344626287585766110286463\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el2241344626287585766110286463_data = {\"mdsDat\": {\"x\": [-0.00825667177258891, -0.0002237153043146617, 0.00031599690765463, -0.004635775395395925, 0.0008415527554266695, -0.0017663606013516495, 0.0003929490915165782, 0.006068036154752705, -0.0019945946435195017, 0.009258582807820078], \"y\": [0.0019109387308665791, -0.0033652025554142474, 0.006669260418088472, 0.0015838693301011516, 0.006088553750245387, -0.007214356084988733, -0.0013540351513367723, -0.0028044915579831333, -0.00301829755572893, 0.0015037606761502502], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [31.188207295871745, 23.033097409523748, 9.160718493044566, 9.119976669554376, 8.109765766342418, 7.378715645006027, 7.185356354809988, 2.619201201630234, 1.988434515111728, 0.21652664910517228]}, \"tinfo\": {\"Term\": [\"learning\", \"model\", \"data\", \"algorithm\", \"set\", \"using\", \"function\", \"two\", \"one\", \"number\", \"results\", \"problem\", \"matrix\", \"distribution\", \"given\", \"network\", \"neural\", \"error\", \"time\", \"information\", \"models\", \"used\", \"also\", \"log\", \"training\", \"networks\", \"figure\", \"xi\", \"vector\", \"based\", \"phoneme\", \"dtw\", \"excerpted\", \"asthma\", \"tdnn\", \"hild\", \"speaker\", \"traits\", \"sentences\", \"speakers\", \"buffer\", \"fanty\", \"haffner\", \"probw\", \"acoustics\", \"multitask\", \"cole\", \"hfw\", \"hm\", \"emu\", \"jfc\", \"sphinx\", \"cfgj\", \"sentence\", \"melscale\", \"yhi\", \"word\", \"gating\", \"est\", \"phenotype\", \"lr\", \"heterogeneous\", \"eest\", \"deletion\", \"snps\", \"boundaries\", \"packets\", \"prediction\", \"perturbations\", \"rbf\", \"data\", \"time\", \"networks\", \"tasks\", \"level\", \"max\", \"map\", \"lasso\", \"using\", \"method\", \"algorithms\", \"samples\", \"link\", \"xi\", \"based\", \"gaussian\", \"may\", \"network\", \"regression\", \"training\", \"section\", \"log\", \"results\", \"learning\", \"inference\", \"models\", \"parameters\", \"two\", \"figure\", \"algorithm\", \"set\", \"model\", \"function\", \"problem\", \"distribution\", \"used\", \"given\", \"error\", \"one\", \"neural\", \"number\", \"input\", \"hedge\", \"dtol\", \"friendster\", \"rit\", \"quantile\", \"cse\", \"replication\", \"normalhedge\", \"hull\", \"recommender\", \"personalized\", \"amin\", \"rounds\", \"advice\", \"substances\", \"regrets\", \"movies\", \"spark\", \"minima\", \"suboptimality\", \"labeler\", \"dyadic\", \"nonconvex\", \"ln\", \"saddle\", \"guarantee\", \"bsrm\", \"bianchi\", \"pit\", \"private\", \"poly\", \"ct\", \"cumulative\", \"abstention\", \"zeta\", \"fast\", \"active\", \"hypothesis\", \"regret\", \"fu\", \"space\", \"users\", \"learning\", \"matrix\", \"theorem\", \"actions\", \"global\", \"also\", \"number\", \"items\", \"algorithm\", \"set\", \"point\", \"information\", \"different\", \"et\", \"action\", \"log\", \"probability\", \"used\", \"let\", \"input\", \"model\", \"distribution\", \"data\", \"two\", \"function\", \"one\", \"kernel\", \"figure\", \"neural\", \"case\", \"problem\", \"results\", \"models\", \"given\", \"using\", \"first\", \"training\", \"time\", \"error\", \"network\", \"ohira\", \"resonance\", \"vet\", \"delayed\", \"residence\", \"harmonic\", \"ql\", \"crossings\", \"longtin\", \"milton\", \"sato\", \"p_\", \"qlq\", \"investigations\", \"pee\", \"interspike\", \"eht\", \"church\", \"quantiles\", \"concisely\", \"ook\", \"reductions\", \"mensour\", \"impossibility\", \"foss\", \"gotanda\", \"maas\", \"spiking\", \"resonant\", \"fractional\", \"streams\", \"opinion\", \"steady\", \"cowan\", \"estimator\", \"pl\", \"sin\", \"entropic\", \"model\", \"projections\", \"state\", \"kernel\", \"require\", \"mse\", \"variables\", \"updates\", \"optimal\", \"probabilistic\", \"skill\", \"network\", \"binary\", \"next\", \"probability\", \"performance\", \"bound\", \"values\", \"states\", \"data\", \"parameters\", \"learning\", \"information\", \"see\", \"using\", \"vector\", \"number\", \"set\", \"training\", \"models\", \"time\", \"matrix\", \"random\", \"one\", \"figure\", \"neural\", \"two\", \"given\", \"used\", \"algorithm\", \"xi\", \"distribution\", \"function\", \"networks\", \"problem\", \"cancers\", \"trader\", \"profit\", \"freq\", \"cmtt\", \"gam\", \"xpt\", \"lo\", \"eighteen\", \"epidemiological\", \"lungsq\", \"deviance\", \"rts\", \"age\", \"statsci\", \"consisted\", \"collard\", \"year\", \"ggood\", \"ard\", \"stopped\", \"reporting\", \"contract\", \"tobacco\", \"cfn\", \"alcohol\", \"agesq\", \"replications\", \"ord\", \"lancet\", \"trading\", \"day\", \"sites\", \"subjects\", \"timeline\", \"days\", \"certainty\", \"timestamp\", \"null\", \"network\", \"error\", \"neural\", \"control\", \"long\", \"training\", \"processing\", \"recurrent\", \"dropout\", \"networks\", \"al\", \"would\", \"non\", \"output\", \"models\", \"omp\", \"model\", \"units\", \"one\", \"set\", \"performance\", \"learning\", \"representations\", \"linear\", \"following\", \"results\", \"image\", \"data\", \"function\", \"methods\", \"using\", \"input\", \"figure\", \"algorithm\", \"used\", \"log\", \"time\", \"also\", \"problem\", \"based\", \"distribution\", \"number\", \"two\", \"given\", \"information\", \"nav\", \"ham\", \"destsp\", \"pham\", \"qiip\", \"cq\", \"todoor\", \"cart\", \"shorter\", \"hamq\", \"hypotheses\", \"ell\", \"sbm\", \"battery\", \"pddp\", \"iearning\", \"rectified\", \"iin\", \"diameter\", \"hams\", \"transitivity\", \"qllp\", \"wli\", \"pjlll\", \"interrupt\", \"parr\", \"hab\", \"fpras\", \"zl\", \"vw\", \"patrol\", \"splittable\", \"phams\", \"pram\", \"desirability\", \"dt\", \"margin\", \"active\", \"items\", \"clusters\", \"asymptotically\", \"lsbm\", \"probability\", \"choice\", \"language\", \"control\", \"example\", \"labels\", \"ij\", \"exponential\", \"updates\", \"also\", \"model\", \"without\", \"learning\", \"classifier\", \"figure\", \"stochastic\", \"number\", \"section\", \"approach\", \"algorithm\", \"possible\", \"ie\", \"parameters\", \"given\", \"function\", \"image\", \"gaussian\", \"set\", \"results\", \"log\", \"one\", \"variables\", \"bound\", \"two\", \"using\", \"time\", \"distribution\", \"training\", \"first\", \"case\", \"data\", \"based\", \"methods\", \"neural\", \"network\", \"networks\", \"models\", \"movshon\", \"grating\", \"ferster\", \"gratings\", \"drifting\", \"rauschecker\", \"counterphase\", \"amplification\", \"lappe\", \"hammond\", \"shapley\", \"thalamic\", \"heading\", \"excite\", \"mstd\", \"striate\", \"yishai\", \"bergen\", \"nelson\", \"triple\", \"hawken\", \"optic\", \"alonso\", \"deg\", \"zy\", \"gmax\", \"phaseinvariance\", \"selectivities\", \"martinez\", \"therangeofc\", \"neurophysiol\", \"spatial\", \"selectivity\", \"phase\", \"population\", \"flow\", \"pk\", \"vec\", \"cortical\", \"cells\", \"orientation\", \"neuron\", \"variational\", \"center\", \"visual\", \"density\", \"rank\", \"recurrent\", \"model\", \"convergence\", \"number\", \"image\", \"high\", \"one\", \"algorithm\", \"set\", \"also\", \"models\", \"two\", \"matrix\", \"first\", \"using\", \"low\", \"information\", \"network\", \"log\", \"distribution\", \"used\", \"al\", \"et\", \"problem\", \"learning\", \"method\", \"neural\", \"training\", \"input\", \"function\", \"figure\", \"given\", \"inference\", \"networks\", \"data\", \"results\", \"time\", \"diffeomorphic\", \"transistors\", \"capacitance\", \"gaze\", \"pupil\", \"defender\", \"htr\", \"gti\", \"mann\", \"mutual\", \"wkt\", \"adjoint\", \"bacon\", \"kuwada\", \"parallelogram\", \"rotational\", \"receptive\", \"sps\", \"asap\", \"wk\", \"hyperplane\", \"cue\", \"winfo\", \"charge\", \"boxed\", \"infomax\", \"basket\", \"clock\", \"harper\", \"composes\", \"skills\", \"penalization\", \"micro\", \"executed\", \"skill\", \"stimulus\", \"basketball\", \"invariances\", \"multiresolution\", \"optimal\", \"triangulated\", \"images\", \"sign\", \"policy\", \"trajectory\", \"space\", \"one\", \"weight\", \"transfer\", \"kernel\", \"xi\", \"image\", \"algorithm\", \"approach\", \"single\", \"state\", \"large\", \"small\", \"time\", \"neural\", \"network\", \"function\", \"used\", \"example\", \"figure\", \"two\", \"data\", \"set\", \"using\", \"model\", \"number\", \"training\", \"matrix\", \"given\", \"based\", \"results\", \"models\", \"learning\", \"error\", \"also\", \"problem\", \"networks\", \"onoda\", \"adaboost\", \"arc\", \"llatlll\", \"firstgmddebsc\", \"doom\", \"ringnorm\", \"lialil\", \"lexp\", \"titanic\", \"miillert\", \"schwenk\", \"lialh\", \"downs\", \"onodatt\", \"panems\", \"iog\", \"kita\", \"benchmarkshtml\", \"blobs\", \"oft\", \"liallt\", \"lial\", \"mika\", \"street\", \"sackinger\", \"distorting\", \"chaussee\", \"gauss\", \"conveys\", \"ratsch\", \"slack\", \"muller\", \"smola\", \"pv\", \"margins\", \"gv\", \"ladder\", \"zi\", \"relevance\", \"svm\", \"lsdd\", \"function\", \"margin\", \"penalization\", \"boosting\", \"ranking\", \"regret\", \"dk\", \"learning\", \"algorithm\", \"receptive\", \"error\", \"methods\", \"solution\", \"xi\", \"number\", \"data\", \"linear\", \"set\", \"using\", \"problem\", \"local\", \"new\", \"optimal\", \"two\", \"given\", \"point\", \"space\", \"however\", \"first\", \"approach\", \"one\", \"matrix\", \"vector\", \"distribution\", \"results\", \"used\", \"input\", \"neural\", \"also\", \"model\", \"training\", \"based\", \"log\", \"method\", \"figure\", \"kxg\", \"gbad\", \"holdout\", \"kvi\", \"rni\", \"ggood\", \"xg\", \"kvkbad\", \"maxgj\", \"xgj\", \"hang\", \"ibshirani\", \"rdm\", \"rit\", \"rdj\", \"xgbad\", \"info\", \"ols\", \"kxgj\", \"dtol\", \"minrd\", \"grouplasso\", \"ropp\", \"vandergheynst\", \"xggood\", \"oore\", \"conjugation\", \"philosophical\", \"hermitian\", \"vjm\", \"group\", \"quantile\", \"oracle\", \"groups\", \"kv\", \"omp\", \"xgi\", \"regret\", \"gj\", \"hedge\", \"ct\", \"stops\", \"actions\", \"charge\", \"cumulative\", \"replication\", \"action\", \"ln\", \"given\", \"one\", \"using\", \"network\", \"case\", \"selection\", \"weights\", \"model\", \"function\", \"set\", \"approach\", \"problem\", \"hidden\", \"results\", \"input\", \"variables\", \"methods\", \"learning\", \"time\", \"performance\", \"data\", \"also\", \"algorithm\", \"distribution\", \"figure\", \"matrix\", \"number\", \"two\", \"xi\", \"used\", \"models\", \"training\", \"information\", \"based\", \"bunching\", \"funsactiolls\", \"fuformation\", \"ex_sf\", \"ijx\", \"ileq\", \"eqlleq\", \"qllp\", \"wlixli\", \"roo\", \"sytem\", \"thore\", \"cq\", \"qln\", \"cyj\", \"wtxt\", \"inp\", \"cy\", \"wwwdaiedacuk\", \"ftmction\", \"mprove\", \"unstated\", \"pjlll\", \"ilxt\", \"errorcorrecting\", \"equalization\", \"nimrod\", \"ccs\", \"xllell\", \"pyk\", \"sm\", \"randomization\", \"coins\", \"ecoc\", \"distribute\", \"parity\", \"relevance\", \"opinion\", \"attained\", \"pac\", \"protein\", \"opinions\", \"hidden\", \"xj\", \"ranking\", \"marmann\", \"code\", \"search\", \"message\", \"ij\", \"labels\", \"theorem\", \"binary\", \"show\", \"classifier\", \"kernel\", \"active\", \"learning\", \"information\", \"social\", \"complexity\", \"label\", \"two\", \"vector\", \"results\", \"matrix\", \"non\", \"distribution\", \"function\", \"problem\", \"error\", \"variables\", \"algorithm\", \"set\", \"using\", \"neural\", \"space\", \"log\", \"data\", \"models\", \"used\", \"number\", \"model\", \"networks\", \"also\", \"given\", \"time\", \"one\", \"probability\", \"method\", \"training\", \"based\"], \"Freq\": [1308.0, 1243.0, 1197.0, 1006.0, 1014.0, 863.0, 749.0, 723.0, 797.0, 734.0, 612.0, 575.0, 570.0, 608.0, 582.0, 740.0, 672.0, 540.0, 731.0, 519.0, 668.0, 632.0, 588.0, 656.0, 675.0, 634.0, 740.0, 504.0, 434.0, 532.0, 19.514218202350563, 8.955268874320367, 5.13397150237168, 6.55755361177709, 16.78825906632837, 4.063029728570779, 13.44816877046229, 8.30904716276744, 6.578918315741899, 5.931460599623678, 5.0291984970338355, 2.557041791541143, 4.524330762129899, 1.8239007903642497, 6.7178090859740065, 16.53752256920109, 3.0260534844977416, 3.654900710426937, 23.83024465424364, 1.764815681470394, 1.7753833392713103, 2.9767927219035544, 1.7649631269249257, 12.089862407814316, 1.7628785666489604, 2.3617341702806827, 67.75270197750925, 11.613038047542616, 2.836057006440554, 2.2437190355961576, 9.33265260253172, 8.802495188964642, 8.067359337520553, 6.546013771918638, 10.100033829884692, 22.96814001376196, 16.32407333170798, 121.32243021158604, 16.587370783483305, 32.54862214667209, 490.6912482426969, 304.7113241747598, 257.7953373229037, 86.48815756149506, 80.84159575526813, 65.21436388136105, 87.0611583004632, 44.04952087092719, 327.7980460626268, 212.68784625352757, 150.6142077724649, 122.96863295055012, 26.753238555735564, 190.60949696826452, 198.71989070185418, 123.34909245049076, 128.7494765296754, 252.6854717931054, 78.79989641013125, 231.97280658018713, 146.99300747474138, 224.61161698795962, 210.65615507606478, 397.9807858317848, 154.10048007454296, 224.9981244700212, 170.17049880386415, 239.26867796550874, 242.04279623039548, 306.2156975905349, 302.67002250515964, 348.3645323761732, 232.9650401839259, 190.65849390835095, 196.65179564296767, 196.44008312910756, 183.82132468481922, 174.38983624843922, 221.90502071524477, 198.18703862228048, 197.7918154180392, 157.99746835698025, 15.16415027990389, 5.881620410335798, 3.877201495291793, 10.55934663815086, 4.354205196930663, 1.6142030411351111, 5.910818584425924, 6.308666029605489, 24.189700423874243, 2.6030767040577536, 8.742869386530392, 2.9177615712077403, 23.238821901719778, 3.0744455898384655, 6.932083929903672, 2.3225709052328787, 13.333481781829656, 4.121342942098211, 20.81655854177752, 1.1193239521549596, 16.631958816510448, 4.485955489998054, 7.255823317072766, 39.83382458626331, 4.649028628815423, 15.09089270241385, 10.590426653356559, 2.5828141203815864, 1.8160833690053388, 17.49610056271078, 7.609604093056603, 21.999615186727095, 10.121196206006715, 16.48080425969859, 18.40411853777448, 49.66187652403869, 57.48094356771703, 24.8527524755119, 30.646725973365406, 11.749149000897376, 140.85560482333466, 38.957621619013345, 385.38287609491357, 179.72191603913473, 92.6972554881194, 47.73172016076876, 50.26646109917919, 173.21006202961783, 208.8123269050435, 32.12225460922846, 270.9578563176426, 269.6192122156146, 90.4842256326153, 149.2148004202652, 131.25328815951977, 109.88690466877243, 50.9827737285882, 172.91564892399134, 132.6843960142438, 165.899931095356, 108.1658624979252, 132.36795607499465, 287.7726760942517, 158.4988415753624, 272.7338554383783, 182.0099166702995, 186.7589509478649, 195.79333018097898, 104.26156952691876, 181.1634988769593, 165.5948961610715, 111.6928847574946, 142.6584644439603, 142.7860120477653, 149.37584104830643, 136.3397001054056, 162.44732282204185, 119.76527874352824, 131.65642816333366, 134.93050895013613, 121.55142244486022, 131.45617977144408, 2.5700189760224865, 8.221659614180544, 0.9635487497715294, 5.595436507365973, 0.9502516378299524, 3.885349857991005, 6.552717235710313, 0.4306874983384107, 0.6763483638733493, 0.6533193319312497, 1.130173602013587, 1.3681432058633953, 0.6456891732648609, 0.6266866287357332, 0.4004574737364779, 0.8623858010640169, 2.3435204658366944, 1.4417037879130736, 0.7755443048860683, 0.9954440692781416, 0.6204188734697957, 1.9660819099147586, 0.3836406834278561, 0.5814160979717412, 0.3787538868039045, 0.38085705499036776, 0.782155142005213, 2.001470101548084, 0.5849449398904454, 5.9597884468732465, 2.319661771250576, 15.71606448329195, 2.7507372946706647, 2.200383811718642, 30.092722810816596, 7.203837695473661, 11.942132767029465, 5.5600951601129145, 164.4603449898458, 8.033250300008486, 60.55114813937675, 50.240128176093116, 12.550358983682377, 2.9834035975609305, 51.71934959774381, 14.709354416605604, 43.52382184732949, 29.057023981536293, 15.084698359524161, 78.37333474274742, 22.128793676443603, 22.31429498882764, 53.1906524940236, 43.96773622906212, 46.07888225683235, 31.023363687634667, 22.11645596174464, 103.21842110188803, 49.53105756772772, 106.70285454809886, 52.15019622248759, 35.13824462106519, 75.13874610287628, 45.00523339584982, 66.35663910644112, 83.98736481886803, 61.70517952342359, 60.889449736506826, 64.59806943604241, 54.16972680982657, 35.568307667084504, 66.11463008734613, 62.6155763275433, 58.19004536544902, 60.21199969319041, 52.338060438416534, 54.177659936043256, 71.3127745387169, 46.0537882698549, 49.21170300149431, 52.78892482274881, 48.21139418895306, 47.005038240516264, 0.6521052686004333, 0.8421887045872629, 4.166942795565868, 1.0820728471662577, 1.969383134004238, 1.9776966142070436, 0.5955718240866252, 2.1484700789066737, 0.769159160705368, 1.191063852087887, 1.4138866982634246, 2.432782925393648, 0.9311089499277486, 4.872850763992239, 0.7844782678143444, 2.1812060065337815, 0.9312016920573218, 1.8473029970157384, 22.540383920043826, 0.7688123429946947, 0.9752598059863126, 0.5602745481103569, 0.9012079031511467, 1.5579508838931289, 1.0933116791133162, 1.5650315372894277, 1.7708415400924689, 0.9553765747765591, 0.761820116379899, 0.9535169625898593, 3.315753864769792, 6.152243053072143, 6.706576010407012, 3.613148878200436, 9.238526694357892, 2.893955694347526, 4.554984042149251, 4.4550420950067, 2.7701881805558197, 96.82623070760854, 71.4962313642452, 84.87035810259769, 37.538062706570045, 18.190875251952544, 84.12061528272406, 35.918294840114356, 23.917944775333122, 20.926847113369686, 75.54855700091629, 36.73483258581888, 30.38597379539993, 42.651071221350485, 42.52613992632139, 71.3784662724952, 15.232874644112597, 119.22520445679355, 24.192314908451237, 76.34946622148445, 91.53125441240361, 42.473582923638304, 106.74872341532225, 12.740576709374187, 41.60598295125584, 39.6342643811593, 58.036366987607735, 43.63255035239991, 91.28384354571203, 64.38573453485176, 45.49090821126077, 70.30539153171765, 45.693156007793874, 59.76817522020542, 72.46941049109105, 53.15375505769701, 52.999025148627084, 55.7633577803913, 49.760758885126876, 48.1644689348232, 46.697277240211235, 47.232610552780635, 46.767350013586615, 44.52869885062442, 43.785395438565466, 43.26300393056567, 1.2706276827350098, 4.716070230391421, 1.495741465800328, 7.244577005951997, 0.541130812629113, 1.0608285263593111, 0.8686016324016188, 1.8047849019673523, 0.3430420434079654, 0.7159409574693079, 7.472774126317453, 0.6999387734904676, 6.718181044885237, 0.67160882801841, 2.818866130193616, 1.421213298593038, 0.675412979564674, 0.511521450481936, 2.2422887872453368, 1.536391430761465, 1.101208463680473, 0.6509222602837139, 0.6659459895830737, 0.49314428892068823, 1.3648940558309905, 1.6517491774258912, 1.7250923467476567, 1.2504530971000418, 0.6708241254077024, 2.2874479647034875, 2.1560874281642453, 1.8458622255569015, 1.9547607171864145, 1.6635387013270868, 1.9680183943592564, 15.138017437661228, 20.565748845634523, 23.55850873666551, 14.44932214115402, 12.099565988574506, 7.307861628854251, 3.8598301550141256, 53.52977739748091, 16.555415476615106, 10.301268886074714, 31.628700603922066, 39.62876795592498, 21.213031017537922, 29.530008355129713, 16.303121469353098, 13.0026752144978, 58.187186125910586, 107.21506055543216, 18.483403400987807, 111.20331884510519, 15.341999184805932, 67.23316026746274, 31.40248866806112, 65.51731887512035, 40.40593914500722, 35.68476323119128, 80.27891387799352, 25.293210997523328, 30.591174028938784, 44.68971094441587, 51.89993122494422, 61.291575214413186, 40.0592438518119, 32.13632617693915, 72.09667918827905, 50.85573063846342, 52.71882683451839, 59.51563755843785, 36.068049255595575, 37.8774108264786, 52.9544379673334, 58.71744474114618, 51.11668061028225, 45.553704603276394, 47.93805876252156, 40.056335551127056, 36.25549246617912, 55.83397878985172, 40.04349594217766, 37.58560270724515, 40.71451631055981, 40.58286723289244, 38.880929672120416, 38.22618286179258, 1.497955778095721, 2.661464769122847, 1.3891261830127708, 0.9800539566660149, 2.0995173799704427, 2.558480679194314, 1.707751908735754, 2.783316036703148, 1.7047944869080067, 0.7300697943300851, 1.0399720383322972, 0.5340468503832658, 3.122739515057539, 0.5162985829781204, 1.941934978234333, 1.341343747856135, 0.658774073177858, 0.49160724687478385, 1.2883797910712984, 1.3770311473036667, 0.4905432500258778, 3.9261078841690233, 0.48642208068756004, 1.298060597379822, 1.1375529929427908, 0.6471140569339928, 0.31376468027643967, 0.9738879140768477, 0.47219951067884003, 0.49388482062527317, 1.80415508887742, 12.93163045089955, 4.122345887724749, 9.545229202890766, 12.588682714596416, 9.299290684829703, 23.561614685829827, 7.505729563368632, 5.284403302958123, 15.144413619947658, 6.124544353815801, 13.519276942664984, 33.12246587751453, 7.317091598558186, 14.225330073082647, 24.543377297520177, 22.935047488859034, 19.870811047770438, 101.29404947409348, 24.529958459968952, 64.49628826157323, 42.026403619969, 23.46667337200151, 67.36510479541386, 80.01541106154006, 80.2146904036607, 52.02403738591637, 57.10522154782339, 59.56861121063052, 49.37240765838978, 43.802559062573096, 64.86973023760805, 19.606956245070613, 44.4650921155514, 57.153349817942114, 52.326182722002585, 49.2914398901213, 50.567244788754316, 27.695000831005242, 33.58449648661467, 44.86799829930899, 76.28626788762003, 42.13625148646707, 48.20305965503608, 47.27894747426256, 38.16349937211227, 48.80812652640077, 48.42451555690761, 40.78927539421954, 34.60530034959706, 41.97838903559056, 56.46329697541967, 39.734138437671966, 38.956022268608265, 2.063517190568042, 3.020685302175156, 0.8694790562775718, 6.227758057385331, 1.6184276089014742, 1.3002524129352935, 0.5146845491005743, 1.8967246872757684, 1.5979325715156794, 2.731500324661591, 1.4039949996150634, 0.25265463122783793, 0.7222382579605685, 0.24697444593187415, 0.36525121124856785, 1.5808509480106747, 9.656209903441507, 5.071234311520496, 9.938352366187708, 4.085600525578259, 7.363608776366237, 0.2399564762980506, 0.617406141732228, 8.054316827000502, 0.23463283665393078, 1.8738629383259644, 0.6076687323516802, 1.0161173505997152, 0.34864578719341616, 0.6501705248983152, 9.474513642782268, 3.2787939336530703, 9.363722099355979, 1.1606093922000433, 13.85551661012597, 6.817501684498385, 2.6054200559658693, 2.834577001570188, 4.932885890003584, 39.21590217646312, 3.58807096696751, 36.86315014269101, 5.903701692338346, 23.352071365291238, 8.550304366674993, 40.74924973159283, 69.39661841864, 19.049682494086305, 7.235654846103686, 33.50054773037384, 42.9624007367901, 37.58703772625913, 75.88294129506416, 31.234492329253676, 23.315658436005204, 37.40187902109998, 36.9275508468956, 24.990238987762357, 55.9685027890841, 51.98913097695581, 55.90084578371256, 55.886801314787334, 48.91223177171482, 30.584125383639087, 53.56337126139433, 52.36204426628177, 75.21447954577857, 65.42398443952423, 57.29030147496656, 70.6764325790428, 49.0109591583265, 45.57532474410429, 40.56748093902014, 40.36872059135471, 38.0634769608918, 41.315597999149404, 42.81530601631523, 60.32551009985081, 38.051074003691994, 39.18637023528193, 37.550542721386826, 37.00178535157623, 0.5379526250945185, 3.801832271371407, 3.4260976972297685, 0.2241803168649916, 0.14036415661005858, 0.1429616314155756, 0.1384872936126825, 0.13955638866001724, 0.21494213545169544, 0.13919257672429367, 0.13745046492797222, 0.13699496346988518, 0.13539087517082798, 0.1349261656234071, 0.1347519729251821, 0.1333576051117679, 0.13172040247963987, 0.13225294966276718, 0.13091165610216046, 0.1305028908613654, 0.13177555317289927, 0.13024758652852414, 0.1284347018141821, 0.4756624990193243, 0.12846695730539057, 0.12737302107254475, 0.12968542560827115, 0.12645482127899288, 0.1265774508512314, 0.1274961398585023, 0.5298879043219515, 0.1946266769268367, 0.797697105135413, 1.4860660329104354, 0.7819413793859592, 0.5294254342862222, 1.22795304800828, 1.7397344350752308, 4.473898684658916, 2.117012402400518, 4.8669489022859445, 2.9022236427988544, 28.09290500024521, 5.740309062291818, 1.3088579517783059, 5.081776779155705, 3.2927135984772606, 3.9426997310556104, 1.0962172298607824, 41.42487203953043, 32.28227907322429, 3.071770245719895, 18.926892394781344, 16.550294408955875, 8.622980332996184, 16.785424524708663, 22.834106432541585, 32.01972585415116, 13.209941395947144, 27.42554964984353, 24.019546503965934, 17.301878359391885, 9.412455255769995, 10.10183592100165, 11.04628331340693, 19.382441252242035, 16.253979616190257, 9.64792003719227, 12.49302769563678, 10.331740799378489, 13.932190265391835, 10.747980008201836, 19.213967353678434, 14.87486900887439, 12.229532403140617, 14.88767953757825, 14.854375780955449, 14.689146956575794, 12.472846172956778, 14.416041680239864, 13.565988096665537, 18.697432809602986, 13.788926919568604, 12.496310341434349, 13.117114462558355, 12.080879618074496, 12.175088351919827, 2.6675043933345117, 0.8418502560119032, 1.3543075787639498, 0.3267237443974612, 0.2637878912573055, 8.198177281864824, 0.8445690968315669, 0.17868857966504992, 0.6863539933416802, 0.49273872799636015, 0.2948528000871307, 0.1100976525061717, 0.7687989555361627, 1.634120523683101, 0.2869128776890683, 0.22541123652070646, 0.10561746297431587, 0.7356669703589918, 0.22199243185531317, 0.871919798260869, 0.10584485012104031, 0.10613223888500864, 0.16330411548381, 0.1068951507822816, 0.2212031010891547, 0.10534443498256271, 0.1615817802028023, 0.1028518918717314, 0.10362105978356705, 0.22105535020168562, 8.329278521120516, 0.6266564943360484, 1.4304157099120876, 3.770105072213862, 0.9572880854660515, 5.753846815779702, 0.2710736392143795, 4.274269989889819, 2.070722728984153, 1.531018498521987, 2.736048128024697, 0.653664319948737, 6.128840581985688, 2.5893213121742624, 1.321975042920027, 0.7553675068562836, 6.038095819742757, 3.803188858017621, 16.19797809164951, 20.22785454521171, 21.224713722010062, 17.344825267108657, 10.899728656152867, 4.616773680930019, 6.202235154437276, 24.237765629829003, 16.616832495341843, 20.059585118454642, 9.054406269310498, 12.895725717660795, 7.072610195793564, 12.976979561093154, 10.956265332687302, 9.357846282969996, 10.429426373333772, 19.857376994267877, 13.582289741422782, 9.148398480843689, 18.002842584715335, 11.553043827763792, 15.374838436236866, 11.446743212951219, 12.548840241391684, 10.60744237366011, 11.843920839809831, 11.718021435396443, 9.719557622394447, 10.46120573883462, 10.578578740117985, 10.380711464706001, 9.547942498857207, 9.437035872689549, 0.11311970203658393, 0.010828758656058824, 0.015878981206207635, 0.015895593307370656, 0.010429570332450388, 0.010291978564453545, 0.010305022712705008, 0.020495910476302746, 0.010271264665535277, 0.009975786936922996, 0.010048397924223572, 0.009994824706828657, 0.03040435477352404, 0.009908549448478387, 0.04080374939137534, 0.019445165419694894, 0.014417398504671552, 0.0698285160769484, 0.009649625712000032, 0.009561709375056187, 0.08844536460174492, 0.009391020125479032, 0.014327738634063118, 0.009475815177671786, 0.009493081553323372, 0.013727273728537003, 0.0092558971652021, 0.009122659737723263, 0.009227606084672414, 0.024254039771687358, 0.09033741138540886, 0.013916057981620772, 0.02834478675892607, 0.10438760372574417, 0.018724791233208422, 0.1001920146440847, 0.17177822834982487, 0.3271026468238863, 0.06475276963191591, 0.06490377043201362, 0.0862542496683398, 0.17433369508523697, 0.9086949472086406, 0.6988376847903347, 0.27174645235950134, 0.11260010024521833, 0.6104687552664795, 0.4511427940203404, 0.3055439429341162, 0.8192918326030764, 0.5772930986695264, 0.8932153017175471, 0.557848405981124, 1.053619133449709, 0.420483779079258, 1.0808634107468336, 0.5159206561573673, 2.997109089987318, 1.3787013518935811, 0.18503072348183527, 0.6977049127659088, 0.5017278175988652, 1.7323072921817158, 1.1445780696501089, 1.5017141587260527, 1.3768793915683188, 0.9412636096361124, 1.3592583272456331, 1.575536222484625, 1.2788542908913918, 1.215537545914438, 0.9388809576436367, 1.865962657262155, 1.8492517678411913, 1.5856255664881056, 1.2988950492083675, 0.9517883121981835, 1.25799895364632, 1.8187619439689549, 1.2460737298075788, 1.199472475742497, 1.2945312253051917, 1.692614716566814, 1.1858409040375721, 1.128838554959952, 1.0964265732979182, 1.1968057967236876, 1.2275853994858936, 1.0112134037578209, 1.0298923508403948, 1.0461893361541295, 1.0117747856486705], \"Total\": [1308.0, 1243.0, 1197.0, 1006.0, 1014.0, 863.0, 749.0, 723.0, 797.0, 734.0, 612.0, 575.0, 570.0, 608.0, 582.0, 740.0, 672.0, 540.0, 731.0, 519.0, 668.0, 632.0, 588.0, 656.0, 675.0, 634.0, 740.0, 504.0, 434.0, 532.0, 29.016634553620044, 14.102326530477164, 8.13660197066906, 10.777603531809229, 27.63476358359093, 6.736332725250379, 22.654527444925222, 14.182044481792975, 11.256506960175129, 10.169493758797946, 8.626326416915594, 4.390759487944511, 7.823554403630813, 3.217589269635345, 11.902652239406416, 29.367816357399715, 5.400869763876692, 6.577881507545891, 43.00261711459171, 3.1877885829015944, 3.207922875246649, 5.395817101632827, 3.1994715528071453, 22.01799638011364, 3.2122972006533788, 4.307484254369, 123.72769339842704, 21.257997385531237, 5.205286649919405, 4.123122535010826, 17.188267890390474, 16.21074181458221, 14.942753879555163, 12.087437401369035, 18.837854048812414, 44.042545457072755, 31.099715696488364, 248.71870969894925, 32.28916406029659, 66.56786972969061, 1197.2804540225607, 731.1045570122384, 634.4773582155649, 194.75342530570512, 181.48494220576467, 144.23551015848474, 199.04435656769365, 94.41159002522156, 863.3968687654475, 536.4958930677382, 372.3845314350835, 299.12104835471047, 55.02083736239279, 504.07051879220546, 532.9832704059659, 316.83215026780306, 335.3776837081783, 740.2673408504178, 191.63708177966868, 675.4631882509855, 398.83640753694516, 656.344147768299, 612.4352842836064, 1308.9096948464812, 423.5153477679031, 668.4355005346644, 477.6944182663931, 723.737156603689, 740.5437382787468, 1006.6560853393064, 1014.8775945196493, 1243.6361136816315, 749.1704272630644, 575.798059523448, 608.7045629968468, 632.4674386541484, 582.8907921588628, 540.9299418582311, 797.109215275922, 672.1357839435211, 734.725256235787, 482.3618005456761, 27.745202706231197, 12.782755831022314, 8.51574390579721, 23.30030767634081, 9.61184574793717, 3.5840251162322376, 13.140650207005539, 14.050253240766025, 54.07835129127658, 5.849489364652245, 19.67603708848373, 6.649930522895399, 53.10521770830741, 7.028130592742908, 15.952099969793279, 5.363044070114735, 30.87129199410055, 9.562104597648805, 48.357852445789895, 2.6186797760598, 39.09168321753409, 10.568212761622636, 17.120385858141606, 94.00881967206088, 10.973290313703382, 35.84913889033532, 25.2702914393538, 6.163013632034947, 4.3356662238739085, 41.78124396537817, 18.201847149450735, 53.903402609663026, 24.526119585005976, 40.722293088468206, 45.874936576653965, 129.64478925571316, 151.4811767392122, 62.962799496813474, 80.12419265821316, 28.9254459403384, 418.0020492970126, 105.83002508971555, 1308.9096948464812, 570.9703142195484, 278.35061689567755, 135.18910138884468, 144.10015598693178, 588.9647096183188, 734.725256235787, 88.74021404668544, 1006.6560853393064, 1014.8775945196493, 290.517453678562, 519.2154548503108, 449.5179738977107, 372.72499677214626, 151.36885755005633, 656.344147768299, 477.87972673250954, 632.4674386541484, 375.649376448378, 482.3618005456761, 1243.6361136816315, 608.7045629968468, 1197.2804540225607, 723.737156603689, 749.1704272630644, 797.109215275922, 363.6077586118204, 740.5437382787468, 672.1357839435211, 405.2118986429837, 575.798059523448, 612.4352842836064, 668.4355005346644, 582.8907921588628, 863.3968687654475, 491.19951687875226, 675.4631882509855, 731.1045570122384, 540.9299418582311, 740.2673408504178, 9.273369281701067, 29.895009981581303, 3.533552434066385, 20.533465874704124, 3.550154118827672, 14.539339921932411, 25.081990061413368, 1.6825726024055025, 2.684777793851843, 2.6174151135984745, 4.618692285506999, 5.609740516671461, 2.6490071146636067, 2.6140841507877375, 1.6928178047215054, 3.65251744658657, 10.074874528299418, 6.212276527546069, 3.368099375285431, 4.329746161912421, 2.7131595079111483, 8.607321206171587, 1.6822899087245748, 2.55298662374953, 1.6786201086506511, 1.688788186140941, 3.474895663138443, 8.98105425077385, 2.655153868843797, 27.10419737399732, 10.619867456543146, 76.83690941001653, 12.994498362823975, 10.39136060416272, 175.0436172489864, 37.645839878674, 66.2963862997804, 29.237125722565455, 1243.6361136816315, 44.16669521701244, 437.0742472338397, 363.6077586118204, 75.46212799976432, 14.888905305415074, 384.6962550479398, 94.00157774044673, 333.9004532649968, 210.75785300932935, 100.11868760327746, 740.2673408504178, 161.6727430430189, 165.57020507736485, 477.87972673250954, 390.21433588351016, 418.6578801345098, 256.9127482543224, 170.91216260132583, 1197.2804540225607, 477.6944182663931, 1308.9096948464812, 519.2154548503108, 310.76638215702735, 863.3968687654475, 434.2309801539741, 734.725256235787, 1014.8775945196493, 675.4631882509855, 668.4355005346644, 731.1045570122384, 570.9703142195484, 318.4207840147128, 797.109215275922, 740.5437382787468, 672.1357839435211, 723.737156603689, 582.8907921588628, 632.4674386541484, 1006.6560853393064, 504.07051879220546, 608.7045629968468, 749.1704272630644, 634.4773582155649, 575.798059523448, 2.555401062489353, 3.313345132505964, 16.87085279919523, 4.400861135821924, 8.014150187904047, 8.080080086892657, 2.4708651537680932, 9.130502360381362, 3.2871498593657895, 5.168171457355417, 6.170381940803652, 10.692308864228547, 4.1037619648068455, 21.747339581953092, 3.513410409201732, 9.773468842356404, 4.186268164794984, 8.322465469633466, 101.57200369504042, 3.4761081973068926, 4.4237738254963, 2.5483044397607886, 4.108542452354174, 7.127606739509834, 5.020999796498503, 7.192775796279786, 8.161803709783902, 4.416848312823737, 3.528714257506538, 4.4202679384443595, 15.384928318407216, 28.9004576765475, 32.33258012972505, 17.477114021940448, 49.930865379475705, 14.410542322310786, 24.197121105826227, 23.71616614755582, 14.076059391481829, 740.2673408504178, 540.9299418582311, 672.1357839435211, 266.6667635278931, 118.42578963970745, 675.4631882509855, 261.46395370878037, 164.81192851552657, 141.2192063622934, 634.4773582155649, 278.7757009920576, 230.21514733206638, 350.2411335584473, 353.96159420026135, 668.4355005346644, 103.99187773122655, 1243.6361136816315, 187.6765933161677, 797.109215275922, 1014.8775945196493, 390.21433588351016, 1308.9096948464812, 85.86612787401927, 396.288556862147, 372.98628388730464, 612.4352842836064, 429.1402960150355, 1197.2804540225607, 749.1704272630644, 466.10375505209686, 863.3968687654475, 482.3618005456761, 740.5437382787468, 1006.6560853393064, 632.4674386541484, 656.344147768299, 731.1045570122384, 588.9647096183188, 575.798059523448, 532.9832704059659, 608.7045629968468, 734.725256235787, 723.737156603689, 582.8907921588628, 519.2154548503108, 4.865769310034551, 18.135822346353226, 6.3819919840441734, 31.35057473313187, 2.379953565985683, 4.718105308722231, 3.943276528698263, 8.209344499613492, 1.562910715058179, 3.269325011649443, 34.34298936669547, 3.2210972438870424, 31.049331581119137, 3.112055123733968, 13.185281658279614, 6.650038148171427, 3.1795384668283457, 2.4081720508263618, 10.560275405908325, 7.257148069531942, 5.207644844640606, 3.0992610439620027, 3.196990830639798, 2.372426558215832, 6.569366319847405, 7.971811069064503, 8.331537758809066, 6.039377874285473, 3.2462904375545567, 11.128953151077376, 10.540007683330465, 9.069977123164717, 9.699494352024912, 8.220990246598022, 9.899702991045052, 85.72251040418284, 121.09613151698568, 151.4811767392122, 88.74021404668544, 74.85170751916195, 44.581798729582474, 21.722871779775637, 477.87972673250954, 122.66676120300512, 70.46249479940631, 266.6667635278931, 349.28202646517695, 166.32202176410328, 248.24142849444948, 122.51744403065344, 94.00157774044673, 588.9647096183188, 1243.6361136816315, 146.4305666777316, 1308.9096948464812, 117.29623478428535, 740.5437382787468, 286.77989901912804, 734.725256235787, 398.83640753694516, 344.93136623842275, 1006.6560853393064, 226.18013101865293, 289.7513932697658, 477.6944182663931, 582.8907921588628, 749.1704272630644, 429.1402960150355, 316.83215026780306, 1014.8775945196493, 612.4352842836064, 656.344147768299, 797.109215275922, 384.6962550479398, 418.6578801345098, 723.737156603689, 863.3968687654475, 731.1045570122384, 608.7045629968468, 675.4631882509855, 491.19951687875226, 405.2118986429837, 1197.2804540225607, 532.9832704059659, 466.10375505209686, 672.1357839435211, 740.2673408504178, 634.4773582155649, 668.4355005346644, 5.3231710221201824, 10.03984481799713, 5.3021676495050425, 3.835109459603305, 8.534399887034452, 10.748589294794922, 7.199345274341135, 11.777436964133729, 7.29895879341886, 3.1588514594890262, 4.512508823157038, 2.3353545152613675, 13.9215346789818, 2.3608744253236456, 8.909736033395882, 6.279653546187367, 3.084190544458791, 2.3206760226573713, 6.170357090785716, 6.6150986792672235, 2.356589809032966, 18.900393411907338, 2.3525278317733758, 6.345028299095911, 5.5700755280233905, 3.178439719939947, 1.5480720752535746, 4.8168487481635465, 2.3361635021400797, 2.456678498162186, 9.052842777857519, 68.6947963003562, 21.665568430739437, 53.09323018891407, 76.66591228247547, 56.847467411845116, 156.20737547797748, 45.47037520105655, 30.92470898590203, 105.7487689728371, 38.03681225611335, 98.01394526749719, 289.0182781977698, 48.642972898465565, 107.66931951520426, 210.6220499934123, 195.23931511693294, 164.81192851552657, 1243.6361136816315, 216.64995453475038, 734.725256235787, 429.1402960150355, 207.88195821994756, 797.109215275922, 1006.6560853393064, 1014.8775945196493, 588.9647096183188, 668.4355005346644, 723.737156603689, 570.9703142195484, 491.19951687875226, 863.3968687654475, 168.4114220521266, 519.2154548503108, 740.2673408504178, 656.344147768299, 608.7045629968468, 632.4674386541484, 278.7757009920576, 372.72499677214626, 575.798059523448, 1308.9096948464812, 536.4958930677382, 672.1357839435211, 675.4631882509855, 482.3618005456761, 749.1704272630644, 740.5437382787468, 582.8907921588628, 423.5153477679031, 634.4773582155649, 1197.2804540225607, 612.4352842836064, 731.1045570122384, 10.810767489837064, 16.182802253952662, 4.830197951086643, 36.054821472834426, 9.482552227882934, 7.834512496383289, 3.1011841628848704, 11.441564761010758, 9.65807679528734, 16.624008035820793, 8.636389454293036, 1.560787889606753, 4.496444216982273, 1.5467829754395321, 2.28949172487775, 10.030398796324961, 61.339911794116674, 32.38215945579157, 63.85282563273451, 26.48161552483897, 47.79586872436775, 1.5577697159201405, 4.023995309764685, 52.50781232962253, 1.5320752757714946, 12.342753605377508, 4.003788490948325, 6.717949693712913, 2.311532390714785, 4.310990493180807, 64.22878624405003, 22.223842422260116, 65.09009524478046, 7.746384767002804, 100.11868760327746, 48.51354992018517, 17.913415489159313, 19.97029020987971, 36.16121737155406, 333.9004532649968, 26.14687035036502, 331.7659174651628, 45.36537316100789, 216.44938530895254, 69.4951556301918, 418.0020492970126, 797.109215275922, 179.98292608090208, 58.82380920057462, 363.6077586118204, 504.07051879220546, 429.1402960150355, 1006.6560853393064, 344.93136623842275, 242.97571212881093, 437.0742472338397, 430.6758908636016, 266.1351718367802, 731.1045570122384, 672.1357839435211, 740.2673408504178, 749.1704272630644, 632.4674386541484, 349.28202646517695, 740.5437382787468, 723.737156603689, 1197.2804540225607, 1014.8775945196493, 863.3968687654475, 1243.6361136816315, 734.725256235787, 675.4631882509855, 570.9703142195484, 582.8907921588628, 532.9832704059659, 612.4352842836064, 668.4355005346644, 1308.9096948464812, 540.9299418582311, 588.9647096183188, 575.798059523448, 634.4773582155649, 5.012974163903977, 39.1921494990059, 36.061531675846226, 2.5061188927137734, 1.5741070014536138, 1.6192365207912494, 1.5825209970215885, 1.6023932749586263, 2.468085521086191, 1.6061812982748196, 1.5950094662888328, 1.607874904846017, 1.5957690299382412, 1.6026879462962718, 1.6236449018226886, 1.6196344898165416, 1.6011281875569663, 1.6087569751844812, 1.6187232790936554, 1.6184053162142835, 1.6421345202887356, 1.6289620627554222, 1.6169284856311057, 6.032103434447336, 1.6387808242352793, 1.627472087644477, 1.6573343864053887, 1.619285987623093, 1.6387019644212963, 1.6530476846477458, 7.105945009439002, 2.552044836599484, 11.311837160988546, 22.332424100450634, 11.350008880895418, 7.529172728834847, 19.128066353093498, 28.375221170357854, 84.49708301772726, 37.14935695060821, 96.24442227861127, 53.537187059913435, 749.1704272630644, 121.09613151698568, 22.223842422260116, 106.22550008822789, 64.80351696366465, 80.12419265821316, 18.181713273576207, 1308.9096948464812, 1006.6560853393064, 61.339911794116674, 540.9299418582311, 466.10375505209686, 224.28254426245059, 504.07051879220546, 734.725256235787, 1197.2804540225607, 396.288556862147, 1014.8775945196493, 863.3968687654475, 575.798059523448, 267.9967452823302, 295.94654845550355, 333.9004532649968, 723.737156603689, 582.8907921588628, 290.517453678562, 418.0020492970126, 322.0176052427888, 491.19951687875226, 344.93136623842275, 797.109215275922, 570.9703142195484, 434.2309801539741, 608.7045629968468, 612.4352842836064, 632.4674386541484, 482.3618005456761, 672.1357839435211, 588.9647096183188, 1243.6361136816315, 675.4631882509855, 532.9832704059659, 656.344147768299, 536.4958930677382, 740.5437382787468, 29.987488081463933, 9.623104623319428, 16.118840287374173, 3.9014160203621713, 3.233847749312737, 101.57200369504042, 10.674106335742078, 2.3721503579033056, 9.214781608470927, 6.739237535915153, 4.063422284311266, 1.544985222837794, 10.940528537001097, 23.30030767634081, 4.155908336226409, 3.272109533914221, 1.5384169732182964, 10.7266225666291, 3.240786334942419, 12.782755831022314, 1.5535358683668896, 1.559918863905738, 2.404484837175986, 1.5745518960309128, 3.2638480881564838, 1.5754878049419994, 2.4244738911547175, 1.5445825208380788, 1.5676671525246628, 3.3488304386720147, 129.40325804062243, 9.61184574793717, 23.04212931001821, 66.19036229525878, 15.638684701759942, 103.99187773122655, 4.199663385404217, 80.12419265821316, 37.80798051274285, 27.745202706231197, 53.903402609663026, 11.165057899574322, 135.18910138884468, 52.50781232962253, 24.526119585005976, 13.140650207005539, 151.36885755005633, 94.00881967206088, 582.8907921588628, 797.109215275922, 863.3968687654475, 740.2673408504178, 405.2118986429837, 129.7585473683068, 193.10657583155998, 1243.6361136816315, 749.1704272630644, 1014.8775945196493, 344.93136623842275, 575.798059523448, 251.28174571511545, 612.4352842836064, 482.3618005456761, 384.6962550479398, 466.10375505209686, 1308.9096948464812, 731.1045570122384, 390.21433588351016, 1197.2804540225607, 588.9647096183188, 1006.6560853393064, 608.7045629968468, 740.5437382787468, 570.9703142195484, 734.725256235787, 723.737156603689, 504.07051879220546, 632.4674386541484, 668.4355005346644, 675.4631882509855, 519.2154548503108, 532.9832704059659, 15.391001636445637, 1.501903457225322, 2.306834035473693, 2.348442152614171, 1.5525974789066588, 1.5412111091646559, 1.5433720194894587, 3.0992610439620027, 1.5623998932566283, 1.5204844534985753, 1.5523351485490366, 1.5495565391422872, 4.718105308722231, 1.5556081942889914, 6.469533386303661, 3.1367066185044106, 2.3339353156134752, 11.34043720771975, 1.5695449773754242, 1.5613459135869883, 14.484450257883221, 1.5531483730646258, 2.372426558215832, 1.5733798113881292, 1.58146955996108, 2.292152813454366, 1.5485542266586634, 1.5266363883312657, 1.5494688832568144, 4.072762828304694, 15.559747436238068, 2.3469853326024412, 4.869322219581909, 19.265507417220746, 3.209778303352078, 19.117846590078635, 37.14935695060821, 76.83690941001653, 12.716535809345194, 12.757392109630203, 17.509333883241176, 38.85513804689607, 251.28174571511545, 187.979717949648, 64.80351696366465, 23.960440467258717, 164.34631213666452, 120.28148576338862, 77.4784974509932, 248.24142849444948, 166.32202176410328, 278.35061689567755, 161.6727430430189, 346.6004390123634, 117.29623478428535, 363.6077586118204, 151.4811767392122, 1308.9096948464812, 519.2154548503108, 45.27463203813138, 229.9067981578367, 153.37443983450555, 723.737156603689, 434.2309801539741, 612.4352842836064, 570.9703142195484, 350.2411335584473, 608.7045629968468, 749.1704272630644, 575.798059523448, 540.9299418582311, 384.6962550479398, 1006.6560853393064, 1014.8775945196493, 863.3968687654475, 672.1357839435211, 418.0020492970126, 656.344147768299, 1197.2804540225607, 668.4355005346644, 632.4674386541484, 734.725256235787, 1243.6361136816315, 634.4773582155649, 588.9647096183188, 582.8907921588628, 731.1045570122384, 797.109215275922, 477.87972673250954, 536.4958930677382, 675.4631882509855, 532.9832704059659], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.2016, -8.9805, -9.5369, -9.2922, -8.3521, -9.7709, -8.5739, -9.0554, -9.2889, -9.3925, -9.5575, -10.2339, -9.6633, -10.5718, -9.268, -8.3671, -10.0655, -9.8767, -8.0018, -10.6047, -10.5988, -10.0819, -10.6047, -8.6804, -10.6058, -10.3134, -6.9569, -8.7207, -10.1304, -10.3646, -8.9393, -8.9977, -9.085, -9.2939, -8.8602, -8.0387, -8.3801, -6.3743, -8.3641, -7.69, -4.977, -5.4534, -5.6206, -6.7128, -6.7803, -6.9951, -6.7062, -7.3875, -5.3804, -5.813, -6.1581, -6.3609, -7.8861, -5.9226, -5.8809, -6.3578, -6.3149, -5.6406, -6.8059, -5.7262, -6.1824, -5.7584, -5.8226, -5.1864, -6.1352, -5.7567, -6.036, -5.6952, -5.6837, -5.4485, -5.4601, -5.3195, -5.7219, -5.9223, -5.8913, -5.8924, -5.9588, -6.0115, -5.7705, -5.8836, -5.8856, -6.1102, -8.1507, -9.0978, -9.5146, -8.5127, -9.3985, -10.3908, -9.0929, -9.0277, -7.6837, -9.913, -8.7014, -9.7989, -7.7238, -9.7465, -8.9335, -10.027, -8.2794, -9.4535, -7.8339, -10.7569, -8.0583, -9.3687, -8.8879, -7.185, -9.333, -8.1556, -8.5097, -9.9208, -10.273, -8.0077, -8.8403, -7.7786, -8.555, -8.0675, -7.9571, -6.9644, -6.8182, -7.6567, -7.4471, -8.4059, -5.9219, -7.2072, -4.9154, -5.6783, -6.3403, -7.0041, -6.9523, -5.7152, -5.5282, -7.4001, -5.2677, -5.2727, -6.3645, -5.8643, -5.9925, -6.1702, -6.9382, -5.7169, -5.9817, -5.7583, -6.186, -5.9841, -5.2075, -5.8039, -5.2612, -5.6656, -5.6399, -5.5926, -6.2228, -5.6703, -5.7601, -6.1539, -5.9092, -5.9083, -5.8632, -5.9545, -5.7793, -6.0841, -5.9895, -5.9649, -6.0693, -5.991, -9.0038, -7.8409, -9.9848, -8.2257, -9.9987, -8.5905, -8.0678, -10.79, -10.3387, -10.3734, -9.8253, -9.6342, -10.3851, -10.415, -10.8628, -10.0957, -9.096, -9.5818, -10.2019, -9.9522, -10.425, -9.2716, -10.9057, -10.49, -10.9185, -10.913, -10.1934, -9.2538, -10.4839, -8.1626, -9.1062, -7.193, -8.9358, -9.159, -6.5434, -7.9731, -7.4676, -8.2321, -4.845, -7.8641, -5.8442, -6.0309, -7.4179, -8.8546, -6.0018, -7.2592, -6.1744, -6.5784, -7.234, -5.5862, -6.8508, -6.8424, -5.9738, -6.1642, -6.1173, -6.5129, -6.8513, -5.3108, -6.0451, -5.2776, -5.9935, -6.3884, -5.6283, -6.1409, -5.7526, -5.517, -5.8253, -5.8386, -5.7795, -5.9555, -6.3762, -5.7563, -5.8107, -5.884, -5.8498, -5.9899, -5.9554, -5.6806, -6.1179, -6.0515, -5.9814, -6.0721, -6.0974, -10.3708, -10.115, -8.516, -9.8643, -9.2655, -9.2613, -10.4614, -9.1785, -10.2057, -9.7684, -9.5969, -9.0542, -10.0146, -8.3595, -10.1859, -9.1633, -10.0145, -9.3295, -6.8279, -10.2061, -9.9683, -10.5225, -10.0472, -9.4998, -9.854, -9.4953, -9.3718, -9.9889, -10.2153, -9.9908, -8.7445, -8.1264, -8.0401, -8.6586, -7.7198, -8.8806, -8.427, -8.4492, -8.9243, -5.3703, -5.6736, -5.5021, -6.3179, -7.0423, -5.511, -6.362, -6.7686, -6.9022, -5.6184, -6.3395, -6.5292, -6.1902, -6.1931, -5.6752, -7.2198, -5.1622, -6.7572, -5.6079, -5.4265, -6.1943, -5.2727, -7.3984, -6.215, -6.2635, -5.8821, -6.1674, -5.4292, -5.7783, -6.1257, -5.6904, -6.1213, -5.8527, -5.66, -5.97, -5.9729, -5.9221, -6.036, -6.0686, -6.0995, -6.0881, -6.098, -6.1471, -6.1639, -6.1759, -9.5863, -8.2748, -9.4232, -7.8456, -10.4399, -9.7668, -9.9667, -9.2354, -10.8957, -10.16, -7.8145, -10.1826, -7.921, -10.2239, -8.7895, -9.4743, -10.2182, -10.4962, -9.0183, -9.3964, -9.7294, -10.2552, -10.2324, -10.5328, -9.5147, -9.324, -9.2805, -9.6023, -10.2251, -8.9984, -9.0575, -9.2129, -9.1555, -9.3169, -9.1488, -7.1086, -6.8022, -6.6663, -7.1552, -7.3326, -7.8369, -8.4752, -5.8456, -7.0191, -7.4935, -6.3717, -6.1463, -6.7712, -6.4404, -7.0345, -7.2607, -5.7621, -5.151, -6.9089, -5.1145, -7.0952, -5.6176, -6.3789, -5.6435, -6.1268, -6.2511, -5.4403, -6.5953, -6.4051, -6.0261, -5.8765, -5.7102, -6.1355, -6.3558, -5.5478, -5.8968, -5.8608, -5.7396, -6.2404, -6.1915, -5.8564, -5.7531, -5.8917, -6.0069, -5.9559, -6.1355, -6.2352, -5.8034, -6.1358, -6.1992, -6.1192, -6.1225, -6.1653, -6.1823, -9.3272, -8.7525, -9.4027, -9.7515, -8.9896, -8.7919, -9.1962, -8.7077, -9.1979, -10.046, -9.6921, -10.3586, -8.5926, -10.3924, -9.0677, -9.4377, -10.1487, -10.4414, -9.478, -9.4114, -10.4436, -8.3637, -10.452, -9.4705, -9.6025, -10.1666, -10.8905, -9.7578, -10.4817, -10.4368, -9.1412, -7.1717, -8.3149, -7.4753, -7.1985, -7.5014, -6.5717, -7.7157, -8.0666, -7.0137, -7.919, -7.1272, -6.2311, -7.7411, -7.0763, -6.5309, -6.5987, -6.7421, -5.1133, -6.5314, -5.5647, -5.993, -6.5758, -5.5212, -5.3491, -5.3466, -5.7796, -5.6864, -5.6442, -5.8319, -5.9516, -5.559, -6.7555, -5.9366, -5.6856, -5.7738, -5.8336, -5.808, -6.4101, -6.2173, -5.9276, -5.3968, -5.9904, -5.8559, -5.8753, -6.0895, -5.8434, -5.8513, -6.0229, -6.1873, -5.9942, -5.6978, -6.0491, -6.0689, -8.9804, -8.5993, -9.8446, -7.8758, -9.2233, -9.4422, -10.369, -9.0647, -9.2361, -8.6999, -9.3655, -11.0805, -10.0302, -11.1033, -10.712, -9.2468, -7.4372, -8.0812, -7.4084, -8.2973, -7.7082, -11.1321, -10.187, -7.6186, -11.1545, -9.0768, -10.2029, -9.6888, -10.7585, -10.1353, -7.4562, -8.5173, -7.4679, -9.5558, -7.0761, -7.7853, -8.7472, -8.6629, -8.1089, -6.0357, -8.4272, -6.0976, -7.9292, -6.5541, -7.5588, -5.9973, -5.4649, -6.7577, -7.7258, -6.1932, -5.9445, -6.0781, -5.3756, -6.2633, -6.5557, -6.0831, -6.0958, -6.4863, -5.68, -5.7538, -5.6812, -5.6815, -5.8148, -6.2843, -5.7239, -5.7466, -5.3844, -5.5239, -5.6567, -5.4467, -5.8127, -5.8854, -6.0018, -6.0067, -6.0655, -5.9835, -5.9479, -5.605, -6.0659, -6.0365, -6.0791, -6.0938, -9.3156, -7.3601, -7.4642, -10.1909, -10.6591, -10.6408, -10.6726, -10.6649, -10.233, -10.6675, -10.6801, -10.6834, -10.6952, -10.6986, -10.6999, -10.7103, -10.7227, -10.7187, -10.7288, -10.732, -10.7223, -10.7339, -10.7479, -9.4387, -10.7477, -10.7562, -10.7383, -10.7635, -10.7625, -10.7553, -9.3307, -10.3323, -8.9216, -8.2995, -8.9416, -9.3316, -8.4903, -8.1419, -7.1974, -7.9456, -7.1131, -7.6301, -5.3601, -6.9481, -8.4265, -7.07, -7.5039, -7.3237, -8.6037, -4.9717, -5.2211, -7.5734, -5.755, -5.8892, -6.5412, -5.8751, -5.5674, -5.2293, -6.1146, -5.3841, -5.5167, -5.8448, -6.4536, -6.3829, -6.2935, -5.7312, -5.9073, -6.4289, -6.1704, -6.3604, -6.0614, -6.3209, -5.74, -5.9959, -6.1918, -5.9951, -5.9973, -6.0085, -6.1721, -6.0273, -6.088, -5.7672, -6.0717, -6.1702, -6.1217, -6.204, -6.1962, -7.4389, -8.5922, -8.1168, -9.5387, -9.7527, -6.3162, -8.589, -10.1422, -8.7965, -9.1279, -9.6414, -10.6265, -8.683, -7.929, -9.6687, -9.9099, -10.668, -8.7271, -9.9252, -8.5571, -10.6659, -10.6632, -10.2322, -10.656, -9.9288, -10.6706, -10.2428, -10.6946, -10.6871, -9.9294, -6.3003, -8.8874, -8.0621, -7.093, -8.4637, -6.6702, -9.7255, -6.9675, -7.6922, -7.9942, -7.4136, -8.8453, -6.6071, -7.4687, -8.141, -8.7006, -6.622, -7.0842, -5.6352, -5.413, -5.3649, -5.5668, -6.0314, -6.8904, -6.5952, -5.2322, -5.6097, -5.4214, -6.2168, -5.8632, -6.4639, -5.8569, -6.0262, -6.1839, -6.0755, -5.4315, -5.8113, -6.2065, -5.5296, -5.9731, -5.6874, -5.9824, -5.8905, -6.0585, -5.9483, -5.959, -6.1459, -6.0724, -6.0613, -6.0801, -6.1638, -6.1754, -8.382, -10.7282, -10.3455, -10.3444, -10.7658, -10.7791, -10.7778, -10.0902, -10.7811, -10.8103, -10.803, -10.8084, -9.6959, -10.8171, -9.4017, -10.1429, -10.442, -8.8644, -10.8435, -10.8527, -8.6281, -10.8707, -10.4483, -10.8617, -10.8599, -10.4911, -10.8852, -10.8997, -10.8883, -9.9219, -8.6069, -10.4774, -9.766, -8.4623, -10.1806, -8.5034, -7.9643, -7.3202, -8.9399, -8.9375, -8.6532, -7.9495, -6.2984, -6.561, -7.5056, -8.3866, -6.6962, -6.9987, -7.3884, -6.402, -6.7521, -6.3156, -6.7864, -6.1505, -7.069, -6.1249, -6.8645, -5.1051, -5.8816, -7.8899, -6.5627, -6.8924, -5.6532, -6.0677, -5.7961, -5.8829, -6.2632, -5.8958, -5.7481, -5.9567, -6.0075, -6.2658, -5.5789, -5.5879, -5.7417, -5.9412, -6.2521, -5.9732, -5.6045, -5.9827, -6.0208, -5.9446, -5.6764, -6.0322, -6.0815, -6.1106, -6.023, -5.9977, -6.1915, -6.1732, -6.1575, -6.191], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7684, 0.711, 0.7046, 0.6683, 0.6667, 0.6595, 0.6436, 0.6305, 0.6281, 0.626, 0.6256, 0.6245, 0.6175, 0.5975, 0.5931, 0.5909, 0.5858, 0.5775, 0.5748, 0.5738, 0.5735, 0.5704, 0.5703, 0.5656, 0.5651, 0.5642, 0.5629, 0.5605, 0.5579, 0.5567, 0.5544, 0.5545, 0.5487, 0.5518, 0.5418, 0.5141, 0.5206, 0.4473, 0.499, 0.4496, 0.2731, 0.2899, 0.2645, 0.3534, 0.3564, 0.3714, 0.3382, 0.4028, 0.1967, 0.2399, 0.2599, 0.2762, 0.4441, 0.1926, 0.1785, 0.2218, 0.2077, 0.0903, 0.2764, 0.0964, 0.167, 0.0928, 0.0979, -0.0254, 0.1541, 0.0763, 0.133, 0.0583, 0.0469, -0.025, -0.0447, -0.1074, -0.0029, 0.0599, 0.0352, -0.0041, 0.0111, 0.0331, -0.1136, -0.0561, -0.1472, 0.049, 0.8641, 0.692, 0.6814, 0.6768, 0.6764, 0.6706, 0.6693, 0.6675, 0.6637, 0.6586, 0.6571, 0.6444, 0.6418, 0.6414, 0.6348, 0.6314, 0.6287, 0.6266, 0.6254, 0.6183, 0.6137, 0.6113, 0.6098, 0.6096, 0.6094, 0.603, 0.5986, 0.5986, 0.598, 0.5978, 0.5961, 0.5721, 0.5831, 0.5637, 0.5549, 0.5087, 0.4992, 0.5387, 0.5072, 0.5673, 0.3805, 0.4689, 0.2455, 0.3123, 0.3687, 0.4272, 0.4151, 0.2444, 0.2102, 0.4521, 0.1558, 0.1427, 0.3017, 0.2213, 0.2372, 0.2468, 0.38, 0.1344, 0.1869, 0.13, 0.2232, 0.1751, 0.0046, 0.1227, -0.0111, 0.0879, 0.0791, 0.0643, 0.2191, 0.0603, 0.0673, 0.1796, 0.0729, 0.0121, -0.0302, 0.0154, -0.2023, 0.0569, -0.167, -0.2216, -0.0247, -0.2601, 1.107, 1.0993, 1.0908, 1.0901, 1.0722, 1.0706, 1.048, 1.0275, 1.0116, 1.0024, 0.9825, 0.9792, 0.9786, 0.962, 0.9487, 0.9468, 0.9319, 0.9295, 0.9217, 0.9202, 0.9148, 0.9137, 0.912, 0.9107, 0.9014, 0.9009, 0.899, 0.889, 0.8775, 0.8756, 0.8689, 0.8032, 0.8376, 0.8379, 0.6295, 0.7366, 0.6762, 0.7304, 0.3671, 0.6859, 0.4136, 0.411, 0.5964, 0.7827, 0.3836, 0.5354, 0.3527, 0.4088, 0.4976, 0.1447, 0.4016, 0.3861, 0.1948, 0.207, 0.1835, 0.2762, 0.3454, -0.0607, 0.1239, -0.1167, 0.0921, 0.2105, -0.0513, 0.1234, -0.0142, -0.1016, -0.0028, -0.0056, -0.0361, 0.035, 0.1983, -0.0994, -0.0801, -0.0565, -0.0963, -0.02, -0.0671, -0.2571, -0.0027, -0.125, -0.2624, -0.187, -0.1153, 1.0289, 1.025, 0.9963, 0.9918, 0.9912, 0.9872, 0.9719, 0.9478, 0.9422, 0.927, 0.9213, 0.9142, 0.9114, 0.8989, 0.8954, 0.8949, 0.8916, 0.8895, 0.8892, 0.8859, 0.8827, 0.8799, 0.8776, 0.8741, 0.8703, 0.8695, 0.8667, 0.8636, 0.8617, 0.8609, 0.86, 0.8477, 0.8217, 0.8184, 0.7074, 0.7894, 0.7247, 0.7226, 0.7691, 0.3606, 0.3711, 0.3254, 0.4341, 0.5213, 0.3116, 0.4097, 0.4645, 0.4854, 0.2667, 0.368, 0.3697, 0.2891, 0.2756, 0.1578, 0.4738, 0.0499, 0.346, 0.049, -0.0111, 0.1769, -0.1118, 0.4867, 0.1408, 0.1529, 0.0383, 0.1087, -0.1791, -0.0594, 0.0678, -0.1133, 0.038, -0.1222, -0.2365, -0.0817, -0.1217, -0.1787, -0.0764, -0.0864, -0.0401, -0.1615, -0.3596, -0.3936, -0.194, -0.0903, 1.1694, 1.1652, 1.0612, 1.0471, 1.0309, 1.0197, 0.9992, 0.9973, 0.9956, 0.9934, 0.987, 0.9856, 0.9813, 0.9787, 0.9693, 0.969, 0.9629, 0.9629, 0.9625, 0.9596, 0.9584, 0.9516, 0.9433, 0.9412, 0.9408, 0.938, 0.9373, 0.9373, 0.9353, 0.93, 0.9252, 0.9201, 0.9103, 0.9144, 0.8966, 0.7782, 0.7391, 0.6511, 0.697, 0.6898, 0.7037, 0.7844, 0.323, 0.5093, 0.5893, 0.3802, 0.3358, 0.4528, 0.3831, 0.4952, 0.5339, 0.1974, 0.0611, 0.4424, 0.0465, 0.478, 0.1129, 0.3003, 0.0949, 0.2225, 0.2435, -0.0168, 0.3213, 0.2638, 0.1429, 0.0934, 0.0088, 0.1407, 0.2237, -0.1324, 0.0237, -0.0096, -0.0827, 0.1451, 0.1094, -0.1029, -0.176, -0.1483, -0.0803, -0.1334, 0.0055, 0.0983, -0.5533, -0.0764, -0.0057, -0.2918, -0.3916, -0.2802, -0.3493, 1.3386, 1.2789, 1.2671, 1.2422, 1.2042, 1.1712, 1.1678, 1.164, 1.1523, 1.1417, 1.1389, 1.1311, 1.1118, 1.0865, 1.0831, 1.0629, 1.0629, 1.0546, 1.0402, 1.0371, 1.0371, 1.035, 1.0304, 1.0198, 1.018, 1.0149, 1.0104, 1.008, 1.0077, 1.0023, 0.9936, 0.9366, 0.9473, 0.8906, 0.7999, 0.7961, 0.715, 0.8052, 0.8398, 0.6631, 0.7803, 0.6256, 0.4403, 0.7123, 0.5825, 0.4569, 0.465, 0.491, 0.0988, 0.4282, 0.1737, 0.2831, 0.4252, 0.1357, 0.0744, 0.0688, 0.1799, 0.1465, 0.1093, 0.1586, 0.1894, 0.0181, 0.456, 0.149, 0.0453, 0.0774, 0.093, 0.0802, 0.2974, 0.1998, 0.0545, -0.2359, 0.0624, -0.0285, -0.0528, 0.0698, -0.1245, -0.1208, -0.053, 0.102, -0.1091, -0.4476, -0.1287, -0.3256, 0.977, 0.9547, 0.9184, 0.8771, 0.8651, 0.8371, 0.8371, 0.836, 0.834, 0.8271, 0.8165, 0.8122, 0.8044, 0.7985, 0.7976, 0.7855, 0.7843, 0.7791, 0.7729, 0.7641, 0.7627, 0.7626, 0.7586, 0.7584, 0.7568, 0.7481, 0.7478, 0.7443, 0.7415, 0.7414, 0.7193, 0.7194, 0.6942, 0.7348, 0.6555, 0.6708, 0.7052, 0.6808, 0.6411, 0.4914, 0.647, 0.4359, 0.594, 0.4065, 0.5378, 0.3051, 0.192, 0.3873, 0.5376, 0.2486, 0.1707, 0.198, 0.0479, 0.2313, 0.2893, 0.1747, 0.1767, 0.2676, 0.0634, 0.0737, 0.0497, 0.0375, 0.0735, 0.1977, 0.0066, 0.0069, -0.1343, -0.1085, -0.0796, -0.2346, -0.0743, -0.0629, -0.0112, -0.0368, -0.0061, -0.0631, -0.1149, -0.4441, -0.0212, -0.0769, -0.0969, -0.2087, 1.4103, 1.3093, 1.2885, 1.2283, 1.2251, 1.2152, 1.2063, 1.2015, 1.2015, 1.1965, 1.1909, 1.1796, 1.1754, 1.1676, 1.1533, 1.1454, 1.1445, 1.1438, 1.1274, 1.1245, 1.1196, 1.116, 1.1094, 1.1022, 1.0963, 1.0946, 1.0944, 1.0924, 1.0815, 1.08, 1.0463, 1.0687, 0.9904, 0.9324, 0.9671, 0.9876, 0.8965, 0.8505, 0.7038, 0.7774, 0.6579, 0.7274, 0.3589, 0.5932, 0.8103, 0.6024, 0.6627, 0.6306, 0.8337, 0.1892, 0.2024, 0.6481, 0.2896, 0.3043, 0.3838, 0.2401, 0.1711, 0.0208, 0.2411, 0.0313, 0.0603, 0.1374, 0.2934, 0.2648, 0.2336, 0.0222, 0.0626, 0.2374, 0.132, 0.2029, 0.0797, 0.1737, -0.0831, -0.0054, 0.0726, -0.0685, -0.0768, -0.1202, -0.0128, -0.1998, -0.1285, -0.5551, -0.2492, -0.1108, -0.2705, -0.1511, -0.4657, 1.4982, 1.4815, 1.4411, 1.4378, 1.4115, 1.401, 1.3811, 1.3319, 1.3207, 1.3021, 1.2945, 1.2764, 1.2624, 1.2605, 1.2447, 1.2426, 1.2391, 1.2381, 1.2369, 1.2327, 1.2315, 1.2301, 1.2283, 1.2279, 1.2262, 1.2127, 1.2095, 1.2086, 1.2012, 1.1999, 1.1747, 1.1875, 1.1385, 1.0524, 1.1244, 1.0234, 1.1775, 0.9869, 1.0132, 1.0207, 0.9371, 1.0799, 0.8242, 0.9083, 0.9972, 1.0616, 0.6962, 0.7103, 0.3347, 0.2439, 0.2121, 0.1641, 0.3022, 0.5818, 0.4795, -0.0201, 0.1093, -0.006, 0.2777, 0.119, 0.3475, 0.0636, 0.133, 0.2016, 0.118, -0.2706, -0.068, 0.1647, -0.2795, -0.0136, -0.2638, -0.0558, -0.1599, -0.068, -0.2099, -0.2055, -0.0308, -0.1841, -0.2283, -0.2576, -0.0782, -0.116, 1.2221, 1.2029, 1.1566, 1.1397, 1.1322, 1.1263, 1.1261, 1.1165, 1.1106, 1.1086, 1.0951, 1.0916, 1.0906, 1.079, 1.0691, 1.0519, 1.0483, 1.0451, 1.0436, 1.0397, 1.0368, 1.0269, 1.0257, 1.023, 1.0197, 1.0173, 1.0154, 1.0152, 1.0117, 1.0117, 0.9863, 1.0074, 0.9889, 0.9173, 0.9911, 0.8839, 0.7587, 0.676, 0.8551, 0.8543, 0.822, 0.7286, 0.5129, 0.5405, 0.661, 0.7749, 0.5397, 0.5494, 0.5995, 0.4215, 0.4719, 0.3934, 0.466, 0.3393, 0.5042, 0.3169, 0.4529, 0.0559, 0.204, 0.6352, 0.3376, 0.4126, 0.1002, 0.1967, 0.1244, 0.1077, 0.2161, 0.0308, -0.0292, 0.0254, 0.0371, 0.1197, -0.1554, -0.1725, -0.1647, -0.1137, 0.0503, -0.122, -0.3544, -0.1497, -0.1325, -0.2061, -0.4643, -0.1471, -0.122, -0.1407, -0.2797, -0.3407, -0.023, -0.1204, -0.335, -0.1316]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 4, 1, 2, 5, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 4, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 3, 5, 1, 1, 2, 3, 4, 5, 7, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 5, 7, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 7, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 4, 1, 2, 5, 1, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 9, 2, 6, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 7, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 4, 1, 1, 2, 3, 4, 5, 6, 7, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 7, 1, 2, 3, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 8, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 1, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 5, 2, 3, 1, 2, 4, 5, 6, 7, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 6, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 1, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 2, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 5, 1, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 1, 2, 3, 4, 5, 7, 9, 1, 1, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 7, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 1, 2, 4, 5, 6, 7], \"Freq\": [0.196452591277711, 0.392905182555422, 0.0982262956388555, 0.04911314781942775, 0.12278286954856937, 0.024556573909713875, 0.07366972172914163, 0.024556573909713875, 0.5881042190600949, 0.08401488843715642, 0.08401488843715642, 0.08401488843715642, 0.08401488843715642, 0.08401488843715642, 0.21801049789311644, 0.3369253149257254, 0.07927654468840598, 0.07927654468840598, 0.06606378724033832, 0.06606378724033832, 0.09909568086050748, 0.019819136172101494, 0.03963827234420299, 0.2441025175918733, 0.3550582074063612, 0.05917636790106019, 0.07397045987632524, 0.07397045987632524, 0.05177932191342767, 0.08136750586395776, 0.014794091975265048, 0.04438227592579515, 0.1716384871023363, 0.37628437557050654, 0.09242072382433493, 0.0726162830048346, 0.15843552655600274, 0.039608881639000684, 0.046210361912167465, 0.026405921092667123, 0.019804440819500342, 0.006601480273166781, 0.2551531397953472, 0.2296378258158125, 0.10206125591813889, 0.051030627959069444, 0.1275765698976736, 0.07654594193860417, 0.051030627959069444, 0.10206125591813889, 0.14228534697869413, 0.4268560409360824, 0.14228534697869413, 0.14228534697869413, 0.2299131800079685, 0.1379479080047811, 0.18393054400637482, 0.2299131800079685, 0.09196527200318741, 0.045982636001593705, 0.09196527200318741, 0.24504387401555797, 0.12252193700777898, 0.12252193700777898, 0.24504387401555797, 0.12252193700777898, 0.12252193700777898, 0.2116396794628845, 0.2582721512089438, 0.0932649434921186, 0.13272318881570724, 0.07891649064717728, 0.10043916991458926, 0.08967783028088326, 0.025109792478647315, 0.014348452844941323, 0.0035871132112353307, 0.13902838463520792, 0.27805676927041584, 0.13902838463520792, 0.27805676927041584, 0.13902838463520792, 0.13902838463520792, 0.30397670510962915, 0.26920812772780883, 0.07053054268883552, 0.07152393061403038, 0.07947103401558932, 0.07947103401558932, 0.07549748231480985, 0.03178841360623572, 0.014900818877922996, 0.0019867758503897327, 0.4054948239071077, 0.22825867570929906, 0.08324728172927377, 0.059078716065936225, 0.048337131326675095, 0.06713490462038207, 0.05370792369630566, 0.029539358032968113, 0.021483169478522263, 0.002685396184815283, 0.24619471698731812, 0.2937357657848692, 0.07640525699606425, 0.08489472999562694, 0.09847788679492725, 0.08829051919545201, 0.06621788939658901, 0.02377052439877554, 0.020374735198950465, 0.0016978945999125388, 0.15037751094647483, 0.4511325328394245, 0.15037751094647483, 0.15037751094647483, 0.15037751094647483, 0.15037751094647483, 0.16981623472837723, 0.2547243520925659, 0.08490811736418861, 0.08490811736418861, 0.08490811736418861, 0.2547243520925659, 0.08490811736418861, 0.08490811736418861, 0.28411449230819047, 0.21163630549487658, 0.07537731428584646, 0.08987295164850924, 0.10436858901117202, 0.08697382417597668, 0.08987295164850924, 0.03189040219785812, 0.026092147252793005, 0.002899127472532556, 0.24957342580177036, 0.24957342580177036, 0.08319114193392346, 0.055460761289282304, 0.16638228386784693, 0.055460761289282304, 0.027730380644641152, 0.08319114193392346, 0.027730380644641152, 0.2876780420053518, 0.2876780420053518, 0.2876780420053518, 0.2876780420053518, 0.2662372390186732, 0.18793216871906343, 0.1409491265392976, 0.04698304217976586, 0.06264405623968781, 0.10962709841945367, 0.15661014059921954, 0.015661014059921954, 0.015661014059921954, 0.6494950365672725, 0.09278500522389607, 0.09278500522389607, 0.09278500522389607, 0.22430678628864856, 0.22430678628864856, 0.08972271451545942, 0.06729203588659458, 0.15701475040205398, 0.08972271451545942, 0.08972271451545942, 0.022430678628864855, 0.022430678628864855, 0.23591330571297134, 0.23591330571297134, 0.07863776857099045, 0.07863776857099045, 0.07863776857099045, 0.07863776857099045, 0.07863776857099045, 0.2223979553050335, 0.2223979553050335, 0.2223979553050335, 0.2223979553050335, 0.373370068160722, 0.20450923331416432, 0.08630664892157393, 0.08818288041986902, 0.07504925993180342, 0.05816317644714765, 0.07129679693521325, 0.022514777979541024, 0.01688608348465577, 0.0018762314982950854, 0.24976344336389836, 0.24976344336389836, 0.24976344336389836, 0.24976344336389836, 0.22329633354514028, 0.1674722501588552, 0.05582408338628507, 0.11164816677257014, 0.11164816677257014, 0.05582408338628507, 0.1674722501588552, 0.05582408338628507, 0.05582408338628507, 0.3213310690975679, 0.3213310690975679, 0.3213310690975679, 0.1622582813709943, 0.4867748441129829, 0.24122804664496003, 0.26596938476239185, 0.1360773596458749, 0.09278001794036925, 0.11133602152844309, 0.06803867982293745, 0.04329734170550565, 0.018556003588073848, 0.018556003588073848, 0.00618533452935795, 0.3953852885146786, 0.16945083793486226, 0.06589754808577976, 0.06589754808577976, 0.10355328984908249, 0.047069677204128406, 0.08472541896743113, 0.047069677204128406, 0.018827870881651363, 0.31290465608317525, 0.24124710125496718, 0.10987491740325238, 0.07882331031102888, 0.09076623611573023, 0.06926896966726781, 0.05254887354068592, 0.02388585160940269, 0.016720096126581884, 0.0023885851609402693, 0.522222313930914, 0.15893722597897383, 0.045410635993992525, 0.09082127198798505, 0.06811595399098878, 0.022705317996996262, 0.045410635993992525, 0.022705317996996262, 0.3165772749079372, 0.4352937529984136, 0.03957215936349215, 0.03957215936349215, 0.03957215936349215, 0.0791443187269843, 0.03957215936349215, 0.5796210064802749, 0.11592420129605498, 0.11592420129605498, 0.19491908784520234, 0.32486514640867054, 0.12994605856346822, 0.12994605856346822, 0.06497302928173411, 0.06497302928173411, 0.06497302928173411, 0.3913280050943731, 0.2070308525916689, 0.2070308525916689, 0.2070308525916689, 0.2070308525916689, 0.24362481073783213, 0.12181240536891606, 0.12181240536891606, 0.12181240536891606, 0.24362481073783213, 0.12181240536891606, 0.3134162654781633, 0.27639859632719915, 0.0913102505723783, 0.06416395986167123, 0.08884240596231402, 0.05922827064154268, 0.0567604260314784, 0.019742756880514224, 0.02714629071070706, 0.002467844610064278, 0.25532212112024955, 0.18912749712611077, 0.09456374856305538, 0.11347649827566646, 0.06619462399413877, 0.14184562284458307, 0.06619462399413877, 0.037825499425222155, 0.028369124568916614, 0.20557954015009325, 0.3083693102251399, 0.041115908030018654, 0.10278977007504662, 0.06167386204502798, 0.14390567810506527, 0.10278977007504662, 0.020557954015009327, 0.020557954015009327, 0.3719450739878705, 0.1653089217723869, 0.12398169132929018, 0.20663615221548365, 0.041327230443096725, 0.041327230443096725, 0.041327230443096725, 0.041327230443096725, 0.041327230443096725, 0.6251032293896298, 0.19916352131648568, 0.19916352131648568, 0.19916352131648568, 0.19916352131648568, 0.19916352131648568, 0.24758220583236884, 0.2094926357043121, 0.07617914025611348, 0.03808957012805674, 0.1333134954481986, 0.057134355192085116, 0.15235828051222697, 0.057134355192085116, 0.34239104047503843, 0.16304335260716116, 0.0978260115642967, 0.08152167630358058, 0.13858684971608698, 0.07336950867322252, 0.06521734104286446, 0.016304335260716115, 0.024456502891074174, 0.16097158514529505, 0.16097158514529505, 0.16097158514529505, 0.16097158514529505, 0.27281353113209367, 0.25576268543633784, 0.08525422847877928, 0.07672880563090134, 0.12788134271816892, 0.051152537087267566, 0.07672880563090134, 0.025576268543633783, 0.017050845695755854, 0.14885494021127665, 0.14885494021127665, 0.14885494021127665, 0.14885494021127665, 0.14885494021127665, 0.14885494021127665, 0.24047547606568656, 0.21375597872505472, 0.12023773803284328, 0.0667987433515796, 0.16031698404379105, 0.09351824069221144, 0.0667987433515796, 0.02671949734063184, 0.01335974867031592, 0.24955858738692582, 0.12477929369346291, 0.12477929369346291, 0.24955858738692582, 0.12477929369346291, 0.12477929369346291, 0.3407438796279919, 0.20079550049506664, 0.10952481845185454, 0.07910125777078383, 0.06693183349835555, 0.08518596990699798, 0.06693183349835555, 0.024338848544856566, 0.018254136408642423, 0.0060847121362141415, 0.20536739096429366, 0.20536739096429366, 0.20536739096429366, 0.20536739096429366, 0.5554660880855289, 0.1851553626951763, 0.23887624027759183, 0.23887624027759183, 0.23887624027759183, 0.3044712055532315, 0.29142243960095016, 0.086991773015209, 0.06524382976140675, 0.09134136166596944, 0.047845475158364945, 0.0695934184121672, 0.0260975319045627, 0.0173983546030418, 0.00434958865076045, 0.2319652528999579, 0.2319652528999579, 0.2319652528999579, 0.23096042183644003, 0.23096042183644003, 0.23096042183644003, 0.23096042183644003, 0.4124606182183816, 0.3069534520843363, 0.10231781736144542, 0.10231781736144542, 0.20463563472289084, 0.10231781736144542, 0.10231781736144542, 0.2433953188014414, 0.2433953188014414, 0.2433953188014414, 0.34499987468583376, 0.16874993870502739, 0.08624996867145844, 0.14249994823980092, 0.11999995641246393, 0.044999983654673975, 0.06749997548201096, 0.011249995913668494, 0.014999994551557991, 0.31848610422363366, 0.2077083288415002, 0.06462036897291118, 0.08308333153660008, 0.10616203474121122, 0.11539351602305567, 0.05538888769106672, 0.02769444384553336, 0.018462962563688907, 0.22635621254160107, 0.19401961074994376, 0.09700980537497188, 0.09700980537497188, 0.06467320358331459, 0.16168300895828647, 0.09700980537497188, 0.032336601791657295, 0.032336601791657295, 0.13890151977625734, 0.13890151977625734, 0.13890151977625734, 0.13890151977625734, 0.2778030395525147, 0.13890151977625734, 0.1924675772678711, 0.2887013659018066, 0.1924675772678711, 0.09623378863393554, 0.09623378863393554, 0.09623378863393554, 0.21194948704331112, 0.21194948704331112, 0.21194948704331112, 0.21194948704331112, 0.27901590183365277, 0.5580318036673055, 0.148413636481009, 0.40813750032277474, 0.05565511368037837, 0.09275852280063063, 0.05565511368037837, 0.0742068182405045, 0.09275852280063063, 0.018551704560126125, 0.05565511368037837, 0.16309143344654475, 0.4077285836163619, 0.04077285836163619, 0.08154571672327238, 0.08154571672327238, 0.08154571672327238, 0.08154571672327238, 0.04077285836163619, 0.1763600435650351, 0.26454006534755264, 0.08818002178251755, 0.08818002178251755, 0.08818002178251755, 0.08818002178251755, 0.08818002178251755, 0.1545706529804842, 0.3091413059609684, 0.1545706529804842, 0.1545706529804842, 0.1545706529804842, 0.4100960625811302, 0.2280167516998952, 0.08602829826040002, 0.07600558389996506, 0.04677266701536312, 0.04677266701536312, 0.06264196475271847, 0.026727238294493208, 0.01503407154065243, 0.0016704523934058255, 0.3460152815543445, 0.1384061126217378, 0.10380458446630335, 0.2076091689326067, 0.03460152815543445, 0.03460152815543445, 0.10380458446630335, 0.27757456385295737, 0.13878728192647868, 0.06939364096323934, 0.20818092288971804, 0.06939364096323934, 0.06939364096323934, 0.13878728192647868, 0.2552807211582439, 0.12764036057912195, 0.12764036057912195, 0.12764036057912195, 0.12764036057912195, 0.12764036057912195, 0.1576037100011812, 0.1576037100011812, 0.1576037100011812, 0.1576037100011812, 0.1576037100011812, 0.1576037100011812, 0.24350492169759175, 0.2922059060371101, 0.2922059060371101, 0.048700984339518354, 0.09740196867903671, 0.048700984339518354, 0.048700984339518354, 0.5791136506077933, 0.08273052151539903, 0.08273052151539903, 0.08273052151539903, 0.08273052151539903, 0.08273052151539903, 0.28961829970749936, 0.2326442079617618, 0.09020897859741783, 0.08546113761860637, 0.0664697737033605, 0.11869602447028663, 0.07121761468217197, 0.02848704587286879, 0.023739204894057325, 0.004747840978811465, 0.30303939448624895, 0.10101313149541631, 0.10101313149541631, 0.10101313149541631, 0.20202626299083262, 0.10101313149541631, 0.1566908893806405, 0.1566908893806405, 0.1566908893806405, 0.1566908893806405, 0.1566908893806405, 0.28057550881611676, 0.1870503392107445, 0.1870503392107445, 0.1870503392107445, 0.09352516960537224, 0.09352516960537224, 0.09352516960537224, 0.18938900010893922, 0.28408350016340883, 0.09469450005446961, 0.09469450005446961, 0.18938900010893922, 0.09469450005446961, 0.1850007413331339, 0.1850007413331339, 0.09250037066656695, 0.09250037066656695, 0.09250037066656695, 0.1850007413331339, 0.2980970901744012, 0.2914232747227355, 0.08231039057054362, 0.08898420602220931, 0.06451354936610175, 0.0734119699683227, 0.06006433906499129, 0.020021446354997096, 0.017796841204441863, 0.002224605150555233, 0.3115479966188527, 0.3115479966188527, 0.3236381193367537, 0.2595676287066349, 0.08049882156091843, 0.0772131553747585, 0.07557032228167852, 0.08049882156091843, 0.057499158257798874, 0.024642496396199517, 0.018071164023879645, 0.001642833093079968, 0.3300019040955784, 0.1650009520477892, 0.11000063469852613, 0.11000063469852613, 0.05500031734926306, 0.05500031734926306, 0.11000063469852613, 0.05500031734926306, 0.11717285494428381, 0.23434570988856762, 0.11717285494428381, 0.11717285494428381, 0.23434570988856762, 0.11717285494428381, 0.26908521141601804, 0.3328159193829697, 0.021243569322317215, 0.1487049852562205, 0.04956832841874017, 0.10621784661158608, 0.04248713864463443, 0.007081189774105739, 0.021243569322317215, 0.32663532446704613, 0.11665547302394505, 0.11665547302394505, 0.10498992572155054, 0.17498320953591756, 0.058327736511972525, 0.04666218920957802, 0.02333109460478901, 0.02333109460478901, 0.15646078407804867, 0.46938235223414604, 0.07823039203902434, 0.07823039203902434, 0.07823039203902434, 0.07823039203902434, 0.07823039203902434, 0.07823039203902434, 0.6381925691870558, 0.07091028546522841, 0.07091028546522841, 0.14182057093045683, 0.07091028546522841, 0.07091028546522841, 0.2838701365754281, 0.37849351543390414, 0.09462337885847603, 0.09462337885847603, 0.09462337885847603, 0.09462337885847603, 0.15571871194622197, 0.31143742389244394, 0.10381247463081464, 0.10381247463081464, 0.05190623731540732, 0.10381247463081464, 0.05190623731540732, 0.5353765486926534, 0.13384413717316335, 0.06692206858658167, 0.06692206858658167, 0.06692206858658167, 0.06692206858658167, 0.2977704577434954, 0.1985136384956636, 0.1985136384956636, 0.0992568192478318, 0.0992568192478318, 0.0992568192478318, 0.0992568192478318, 0.30421491041875903, 0.30421491041875903, 0.31045321649254376, 0.31045321649254376, 0.31045321649254376, 0.6273941787505734, 0.4104370625850646, 0.2052185312925323, 0.2052185312925323, 0.06840617709751076, 0.03420308854875538, 0.03420308854875538, 0.03420308854875538, 0.03420308854875538, 0.19349203257890862, 0.19349203257890862, 0.19349203257890862, 0.19349203257890862, 0.4362710872199485, 0.3216682726089556, 0.2255375244729459, 0.07024939286862249, 0.13125544457032096, 0.06470338816846809, 0.061006051701698476, 0.07024939286862249, 0.035124696434311244, 0.01663801410046322, 0.0018486682333848023, 0.576337136024286, 0.192112378674762, 0.2627916442938245, 0.25136592062887564, 0.17138585497423336, 0.10283151298454002, 0.045702894659795566, 0.0742672038221678, 0.05712861832474446, 0.022851447329897783, 0.011425723664948892, 0.2548796051317032, 0.2951237533103932, 0.08317123956929263, 0.10195184205268129, 0.06439063708590398, 0.09122006920503063, 0.0697565235097293, 0.021463545695301324, 0.016097659271475995, 0.0026829432119126655, 0.42581419299038253, 0.31493175046316585, 0.19754809801780404, 0.08875349331234673, 0.09161650922564825, 0.11452063653206031, 0.06012333417933166, 0.08875349331234673, 0.028630159133015077, 0.017178095479809048, 0.0028630159133015078, 0.6145071392239748, 0.12290142784479496, 0.12290142784479496, 0.4235718720460589, 0.2581849546796823, 0.2581849546796823, 0.12909247733984114, 0.12909247733984114, 0.12909247733984114, 0.12909247733984114, 0.2448630906182968, 0.2530251936389067, 0.09794523624731873, 0.07345892718548905, 0.1305936483297583, 0.07345892718548905, 0.07345892718548905, 0.03264841208243958, 0.01632420604121979, 0.6832530928275508, 0.22775103094251692, 0.20054797535068875, 0.38566918336670913, 0.0462803020040051, 0.0925606040080102, 0.084847220340676, 0.084847220340676, 0.06942045300600765, 0.02314015100200255, 0.015426767334668365, 0.18860210881739095, 0.18860210881739095, 0.18860210881739095, 0.18860210881739095, 0.18860210881739095, 0.32678691006487076, 0.24441500298240332, 0.08507262534746635, 0.08102154794996795, 0.0904740618774642, 0.06481723835997436, 0.07291939315497116, 0.01620430958999359, 0.017554668722493057, 0.0013503591324994658, 0.3013032279437936, 0.24429991454902186, 0.08957663533464134, 0.08754080271339951, 0.08143330484967395, 0.08957663533464134, 0.059039146016013615, 0.028501656697385885, 0.01628666096993479, 0.0020358326212418488, 0.22868213118129574, 0.24627306434908772, 0.07036373267116793, 0.0879546658389599, 0.07036373267116793, 0.15831839851012783, 0.0879546658389599, 0.03518186633558396, 0.01759093316779198, 0.321727648398613, 0.24665786377226998, 0.07238872088968792, 0.10724254946620433, 0.07238872088968792, 0.056302338469757275, 0.08043191209965325, 0.024129573629895974, 0.01876744615658576, 0.0026810637366551085, 0.16557996880072873, 0.16557996880072873, 0.16557996880072873, 0.16557996880072873, 0.16557996880072873, 0.2213679275283082, 0.18447327294025684, 0.2213679275283082, 0.07378930917610273, 0.036894654588051363, 0.036894654588051363, 0.14757861835220545, 0.036894654588051363, 0.2272282558202636, 0.2272282558202636, 0.2272282558202636, 0.2272282558202636, 0.1174295529623943, 0.4697182118495772, 0.1174295529623943, 0.1174295529623943, 0.1174295529623943, 0.1174295529623943, 0.17285818204196388, 0.4148596369007133, 0.06914327281678555, 0.10371490922517833, 0.06914327281678555, 0.06914327281678555, 0.06914327281678555, 0.034571636408392774, 0.034571636408392774, 0.3110106746354313, 0.24960942556577537, 0.07074491740634274, 0.08542782479256483, 0.08142339550541335, 0.06540567835680744, 0.07474934669349423, 0.037374673346747114, 0.022691765960525034, 0.002669619524767651, 0.24752229909754975, 0.12376114954877487, 0.12376114954877487, 0.24752229909754975, 0.12376114954877487, 0.5644934366285849, 0.14112335915714622, 0.047041119719048737, 0.09408223943809747, 0.047041119719048737, 0.047041119719048737, 0.047041119719048737, 0.38821817765663613, 0.17043724872730368, 0.07574988832324607, 0.06943739762964224, 0.10099985109766144, 0.08206237901684992, 0.07259364297644415, 0.018937472080811518, 0.018937472080811518, 0.00315624534680192, 0.22188433261353452, 0.13867770788345907, 0.08320662473007544, 0.11094216630676726, 0.11094216630676726, 0.13867770788345907, 0.16641324946015087, 0.027735541576691815, 0.027735541576691815, 0.3117497021418821, 0.10391656738062736, 0.10391656738062736, 0.20783313476125473, 0.10391656738062736, 0.10391656738062736, 0.3938093032022477, 0.06891662806039335, 0.07876186064044954, 0.22644034934129245, 0.07876186064044954, 0.02953569774016858, 0.02953569774016858, 0.009845232580056193, 0.07876186064044954, 0.3156680504739421, 0.23331986339378327, 0.08921053600350537, 0.07548583815681224, 0.08921053600350537, 0.07033907646430232, 0.06862348923346567, 0.02744939569338627, 0.02744939569338627, 0.0017155872308366419, 0.343842750226198, 0.18514609627564504, 0.07934832697527645, 0.13224721162546074, 0.07934832697527645, 0.07934832697527645, 0.0528988846501843, 0.02644944232509215, 0.0528988846501843, 0.24982623893387584, 0.3469808874081609, 0.09715464847428505, 0.0763357952297954, 0.05551694198530575, 0.0763357952297954, 0.048577324237142526, 0.027758470992652873, 0.020818853244489656, 0.31461977829137305, 0.31461977829137305, 0.31461977829137305, 0.19920626625770738, 0.19920626625770738, 0.09960313312885369, 0.09960313312885369, 0.09960313312885369, 0.2988093993865611, 0.09960313312885369, 0.2607487505984869, 0.2607487505984869, 0.2607487505984869, 0.3863890349986701, 0.11591671049960103, 0.10046114909965423, 0.11591671049960103, 0.06955002629976062, 0.04636668419984041, 0.07727780699973402, 0.023183342099920207, 0.06182224559978722, 0.3474826123084613, 0.18129527598702327, 0.07553969832792637, 0.12086351732468219, 0.060431758662341094, 0.04532381899675582, 0.060431758662341094, 0.030215879331170547, 0.060431758662341094, 0.1748012655415253, 0.1748012655415253, 0.08740063277076265, 0.08740063277076265, 0.08740063277076265, 0.08740063277076265, 0.1748012655415253, 0.16736803688240218, 0.41842009220600546, 0.11157869125493479, 0.08368401844120109, 0.055789345627467395, 0.055789345627467395, 0.055789345627467395, 0.027894672813733697, 0.027894672813733697, 0.31367519796530013, 0.20911679864353344, 0.05227919966088336, 0.05227919966088336, 0.10455839932176672, 0.05227919966088336, 0.05227919966088336, 0.05227919966088336, 0.24005172369114794, 0.3600775855367219, 0.12002586184557397, 0.12002586184557397, 0.24005172369114794, 0.6390957028022408, 0.12781914056044816, 0.12781914056044816, 0.12781914056044816, 0.1654184708422248, 0.1654184708422248, 0.11027898056148319, 0.055139490280741596, 0.275697451403708, 0.055139490280741596, 0.055139490280741596, 0.055139490280741596, 0.31657075770247184, 0.31657075770247184, 0.30587353549639257, 0.30587353549639257, 0.30587353549639257, 0.1377951766201866, 0.1377951766201866, 0.1377951766201866, 0.1377951766201866, 0.2755903532403732, 0.24609797604865377, 0.24609797604865377, 0.24609797604865377, 0.27511565321930814, 0.13755782660965407, 0.27511565321930814, 0.06877891330482704, 0.06877891330482704, 0.06877891330482704, 0.06877891330482704, 0.06877891330482704, 0.1436623221590306, 0.1436623221590306, 0.0718311610795153, 0.1436623221590306, 0.0718311610795153, 0.2154934832385459, 0.0718311610795153, 0.10812680057753699, 0.5406340028876849, 0.036042266859178995, 0.07208453371835799, 0.036042266859178995, 0.036042266859178995, 0.07208453371835799, 0.036042266859178995, 0.07208453371835799, 0.5551874246682617, 0.18506247488942054, 0.06168749162980685, 0.06168749162980685, 0.06168749162980685, 0.6080985185597149, 0.15202462963992872, 0.15202462963992872, 0.2944901540277251, 0.2029594304785673, 0.10346951357730882, 0.11540830360545984, 0.0915307235491578, 0.07163274016890611, 0.07163274016890611, 0.015918386704201357, 0.027857176732352374, 0.003979596676050339, 0.28862533580965, 0.21646900185723753, 0.08658760074289501, 0.08658760074289501, 0.06734591168891833, 0.11063971206036584, 0.08658760074289501, 0.028862533580965, 0.019241689053976666, 0.5937948974827882, 0.14844872437069706, 0.14844872437069706, 0.5581055668320309, 0.13952639170800774, 0.06976319585400387, 0.046508797236002576, 0.06976319585400387, 0.046508797236002576, 0.046508797236002576, 0.023254398618001288, 0.023254398618001288, 0.31019601353805093, 0.12407840541522037, 0.06203920270761019, 0.12407840541522037, 0.06203920270761019, 0.06203920270761019, 0.06203920270761019, 0.06203920270761019, 0.32606912880068795, 0.2298011002976277, 0.09937344877735252, 0.08074092713159893, 0.06831924603442986, 0.0652138257601376, 0.07142466630872213, 0.031054202742922664, 0.015527101371461332, 0.003105420274292266, 0.3224574702683094, 0.3224574702683094, 0.3224574702683094, 0.2588836320950922, 0.4438005121630152, 0.0554750640203769, 0.0554750640203769, 0.0369833760135846, 0.0184916880067923, 0.09245844003396149, 0.0369833760135846, 0.0184916880067923, 0.29291234522247284, 0.18830079335730399, 0.12553386223820265, 0.041844620746067554, 0.06276693111910132, 0.10461155186516888, 0.14645617261123642, 0.020922310373033777, 0.020922310373033777, 0.23294419465295868, 0.26206221898457854, 0.0873540729948595, 0.05823604866323967, 0.20382617032133885, 0.029118024331619835, 0.05823604866323967, 0.05823604866323967, 0.029118024331619835, 0.20647112428121825, 0.3970598543869582, 0.047647182526434975, 0.06352957670191331, 0.14294154757930494, 0.031764788350956655, 0.047647182526434975, 0.047647182526434975, 0.015882394175478327, 0.30025740003603996, 0.2277814758894096, 0.08282962759614895, 0.07937839311297608, 0.10698826897835906, 0.08628086207932183, 0.07247592414663033, 0.02415864138221011, 0.020707406899037238, 0.003451234483172873, 0.15037507721289867, 0.15037507721289867, 0.15037507721289867, 0.15037507721289867, 0.15037507721289867, 0.41525272235297767, 0.2779552164941832, 0.17321846824999823, 0.10876508471511516, 0.09668007530232459, 0.12085009412790573, 0.10070841177325478, 0.07653839294767363, 0.03222669176744153, 0.012085009412790573, 0.004028336470930191, 0.3169126769564307, 0.18874946201081536, 0.07223744842389229, 0.10253057195649229, 0.09320961086953844, 0.09787009141301536, 0.08854913032606153, 0.025632642989123072, 0.013981441630430767, 0.002330240271738461, 0.3586866333624992, 0.18085040337605, 0.06932598796081917, 0.07535433474002083, 0.08741102829842416, 0.0813826815192225, 0.11152441541523084, 0.018085040337605, 0.015070866948004167, 0.0030141733896008336, 0.3916980961425157, 0.3916980961425157, 0.3916980961425157, 0.36362318582229, 0.20542348809441058, 0.08736401217808266, 0.09444758073306234, 0.07791925410477643, 0.08264163314142954, 0.05430735892151084, 0.01652832662828591, 0.01652832662828591, 0.0023611895183265584, 0.16203839628854272, 0.24305759443281408, 0.08101919814427136, 0.16203839628854272, 0.08101919814427136, 0.16203839628854272, 0.16203839628854272, 0.27541553061286805, 0.28697142700221917, 0.10015110204104294, 0.08281725745701628, 0.05970546467831406, 0.08474324018857479, 0.06548341287298962, 0.023111792778702216, 0.01925982731558518, 0.0019259827315585179, 0.4284608889159166, 0.32755495941275015, 0.27365351039546215, 0.058047714326310146, 0.09536410210750954, 0.062193979635332304, 0.07877904087142092, 0.053901449017287995, 0.02487759185413292, 0.022804459199621843, 0.0020731326545110765, 0.30444336677612105, 0.15222168338806052, 0.15222168338806052, 0.15222168338806052, 0.15222168338806052, 0.27378377095352185, 0.27378377095352185, 0.27378377095352185, 0.3505206948137868, 0.15022315492019434, 0.10014876994679622, 0.10014876994679622, 0.05007438497339811, 0.05007438497339811, 0.15022315492019434, 0.3825431555822931, 0.3825431555822931, 0.13522617822047292, 0.36060314192126114, 0.1014196336653547, 0.056344240925197055, 0.15776387459055174, 0.07888193729527587, 0.056344240925197055, 0.02253769637007882, 0.01126884818503941, 0.6234563852618262, 0.2695212015666108, 0.2860224996217094, 0.13751081712582183, 0.05500432685032873, 0.04400346148026299, 0.07150562490542735, 0.09350735564555884, 0.030252379767680804, 0.011000865370065747, 0.002750216342516437, 0.38366397906371075, 0.1278879930212369, 0.06394399651061845, 0.1278879930212369, 0.06394399651061845, 0.06394399651061845, 0.06394399651061845, 0.06394399651061845, 0.25631719221452554, 0.25631719221452554, 0.42155843817753574, 0.40016689518644677, 0.10004172379661169, 0.06669448253107446, 0.13338896506214892, 0.10004172379661169, 0.03334724126553723, 0.03334724126553723, 0.10004172379661169, 0.308567087320111, 0.308567087320111, 0.21515971002474557, 0.3064395870049407, 0.12387983304455048, 0.09779986819306617, 0.09779986819306617, 0.0586799209158397, 0.05215992970296862, 0.03259995606435539, 0.013039982425742155, 0.006519991212871078, 0.20464711011501543, 0.4348751089944078, 0.07674266629313078, 0.05116177752875386, 0.10232355505750772, 0.02558088876437693, 0.05116177752875386, 0.02558088876437693, 0.22246001826792117, 0.3006216463080016, 0.11423622559704061, 0.09619892681856052, 0.12626109144936068, 0.04809946340928026, 0.054111896335440286, 0.02404973170464013, 0.018037298778480097, 0.006012432926160032, 0.3524201605324046, 0.1762100802662023, 0.10572604815972138, 0.07048403210648092, 0.10572604815972138, 0.07048403210648092, 0.10572604815972138, 0.07048403210648092, 0.22623062989071507, 0.22623062989071507, 0.22623062989071507, 0.22623062989071507, 0.2554550481251433, 0.19868725965288925, 0.11353557694450814, 0.09934362982644462, 0.14191947118063516, 0.0851516827083811, 0.05676778847225407, 0.014191947118063517, 0.028383894236127034, 0.1370058426554832, 0.1370058426554832, 0.1370058426554832, 0.1370058426554832, 0.2740116853109664, 0.1370058426554832, 0.3343581636558475, 0.24148089597366762, 0.07894567752985288, 0.08823340429807086, 0.0650140873775259, 0.0626921556854714, 0.08591147260601636, 0.02554124861259946, 0.01857545353643597, 0.0023219316920544963, 0.46604447598272236, 0.13769495881307706, 0.07414343936088766, 0.09532727917828412, 0.06355151945218941, 0.042367679634792946, 0.05295959954349118, 0.031775759726094706, 0.031775759726094706, 0.3040698694241702, 0.294137939015843, 0.08174742720700053, 0.08174742720700053, 0.08480340579417812, 0.05806359315637421, 0.04583967880766385, 0.03132378051857029, 0.01527989293588795, 0.0022919839403831924, 0.27951597043161647, 0.28750214101537697, 0.09050993328261868, 0.07187553525384424, 0.07986170583760471, 0.07453759211509774, 0.06921347839259075, 0.026620568612534905, 0.018634398028774434, 0.0026620568612534903, 0.4463180196413404, 0.17081306924545125, 0.07163128710293118, 0.09918178214252009, 0.04408079206334226, 0.0661211880950134, 0.06061108908709561, 0.02204039603167113, 0.01653029702375335, 0.4051723457134927, 0.4051723457134927, 0.32047354837999587, 0.22458382524267426, 0.07822582676992025, 0.10598337820440808, 0.06813217170283375, 0.07570241300314863, 0.07570241300314863, 0.03280437896803107, 0.017663896367401344, 0.002523413766771621, 0.49072317497033824, 0.12722452684416177, 0.0726997296252353, 0.09087466203154412, 0.0726997296252353, 0.05452479721892647, 0.0726997296252353, 0.018174932406308825, 0.018174932406308825, 0.39902336752951933, 0.39902336752951933, 0.17019679702196228, 0.4254919925549057, 0.06382379888323586, 0.09573569832485378, 0.0744610986971085, 0.04254919925549057, 0.053186499069363215, 0.021274599627745285, 0.04254919925549057, 0.2190459978060248, 0.2190459978060248, 0.1095229989030124, 0.2190459978060248, 0.1095229989030124, 0.29104831074656623, 0.20895776156163728, 0.07835916058561399, 0.1007474921815037, 0.08209054918492893, 0.1007474921815037, 0.08209054918492893, 0.033582497393834565, 0.01865694299657476, 0.003731388599314952, 0.34280796250723783, 0.263581233394454, 0.07008518344592418, 0.08075032005726046, 0.08075032005726046, 0.07922672911278386, 0.0487549102232516, 0.019806682278195965, 0.0121887275558129, 0.0015235909444766126, 0.2364349867135002, 0.21110266670848232, 0.07599696001505364, 0.15199392003010728, 0.09288517335173223, 0.07599696001505364, 0.10977338668841082, 0.02533232000501788, 0.02533232000501788, 0.3724703036094853, 0.3724703036094853, 0.2315757418634512, 0.24345142093337177, 0.11875679069920574, 0.10094327209432488, 0.08312975348944401, 0.11875679069920574, 0.05344055581464258, 0.029689197674801434, 0.02375135813984115, 0.5236129700440422, 0.1745376566813474, 0.05817921889378247, 0.05817921889378247, 0.05817921889378247, 0.05817921889378247, 0.05817921889378247, 0.13810328719028075, 0.3222410034439884, 0.13810328719028075, 0.04603442906342692, 0.18413771625370767, 0.09206885812685384, 0.04603442906342692, 0.18678605562165612, 0.2615004778703186, 0.09339302781082806, 0.11207163337299368, 0.1307502389351593, 0.09339302781082806, 0.07471442224866245, 0.05603581668649684, 0.1620645220334216, 0.1620645220334216, 0.1620645220334216, 0.1620645220334216, 0.1620645220334216, 0.2877784247187508, 0.2877784247187508, 0.2877784247187508, 0.3106208475649987, 0.20708056504333247, 0.10354028252166624, 0.10354028252166624, 0.10354028252166624, 0.20708056504333247, 0.4370885037899172, 0.18588821425548202, 0.09043210423239667, 0.06531207527895315, 0.07033608106964186, 0.04019204632550963, 0.07033608106964186, 0.020096023162754814, 0.01507201737206611, 0.2725107696390055, 0.18167384642600368, 0.08257902110272894, 0.09083692321300184, 0.1734159443157308, 0.05780531477191026, 0.08257902110272894, 0.04954741266163737, 0.01651580422054579, 0.26563343305174825, 0.26563343305174825, 0.13281671652587412, 0.13281671652587412, 0.13281671652587412, 0.13281671652587412, 0.13281671652587412, 0.16694183921476277, 0.29214821862583484, 0.16694183921476277, 0.12520637941107207, 0.12520637941107207, 0.04173545980369069, 0.08347091960738139, 0.04173545980369069, 0.2627106808609358, 0.3152528170331229, 0.09457584510993687, 0.07005618156291621, 0.05254213617218716, 0.08581882241457235, 0.0718075861019891, 0.02627106808609358, 0.019265449929801958, 0.001751404539072905, 0.45065185354548654, 0.14559521422238794, 0.06239794895245198, 0.09706347614825864, 0.07626415983077464, 0.055464843513290646, 0.06239794895245198, 0.027732421756645323, 0.020799316317483992, 0.4340851655478081, 0.10852129138695203, 0.10852129138695203, 0.21704258277390406, 0.10852129138695203, 0.10852129138695203, 0.3846409772221057, 0.21468333612396598, 0.08050625104648725, 0.09243310305337425, 0.06857939903960025, 0.06857939903960025, 0.053670834030991495, 0.023853704013774, 0.0178902780103305, 0.00298171300172175, 0.6226073974703217, 0.2710429434086915, 0.16778848877680902, 0.1419748751188384, 0.09034764780289717, 0.1290680682898531, 0.06453403414492655, 0.07744084097391186, 0.02581361365797062, 0.02581361365797062, 0.39702074657467423, 0.19385050536979398, 0.08574156968279349, 0.0782857810147245, 0.0633742036785865, 0.0782857810147245, 0.0633742036785865, 0.022367366004207, 0.01304763016912075, 0.00186394716701725, 0.30465319890874626, 0.2252717315874532, 0.09010869263498128, 0.09654502782319423, 0.08152691238403069, 0.07294513213308008, 0.06865424200760478, 0.03647256606654004, 0.021454450627376494, 0.0021454450627376494, 0.19972316757429331, 0.1689965264090174, 0.09217992349582768, 0.12290656466110358, 0.10754324407846563, 0.09217992349582768, 0.13826988524374154, 0.04608996174791384, 0.030726641165275896, 0.3315593012843025, 0.3315593012843025, 0.16577965064215125, 0.38205632526709915, 0.38205632526709915, 0.14475415358544175, 0.43426246075632524, 0.062037494393760745, 0.12407498878752149, 0.062037494393760745, 0.10339582398960125, 0.062037494393760745, 0.020679164797920247, 0.020679164797920247, 0.279824617644617, 0.2315789939127865, 0.1318713715336701, 0.09568715373479719, 0.08603802898843109, 0.08121346661524804, 0.05709065474933278, 0.015277780848412997, 0.01929824949273221, 0.001608187457727684, 0.33660689747930544, 0.2229085676640734, 0.09125786998327837, 0.10621817653791417, 0.05684916490761603, 0.08527374736142405, 0.06432931818493394, 0.01795236786556296, 0.016456337210099377, 0.0014960306554635799, 0.1943553253665765, 0.42110320496091574, 0.03239255422776275, 0.0647851084555255, 0.0647851084555255, 0.09717766268328824, 0.0647851084555255, 0.03239255422776275, 0.03239255422776275, 0.1878579508049897, 0.1878579508049897, 0.1878579508049897, 0.1878579508049897, 0.1878579508049897, 0.2071186649536276, 0.3451977749227127, 0.13807910996908507, 0.06903955498454253, 0.06903955498454253, 0.06903955498454253, 0.06903955498454253, 0.20149231514750143, 0.20149231514750143, 0.20149231514750143, 0.1343282100983343, 0.06716410504916714, 0.06716410504916714, 0.06716410504916714, 0.06716410504916714, 0.11223677068004642, 0.22447354136009284, 0.11223677068004642, 0.11223677068004642, 0.11223677068004642, 0.22447354136009284, 0.11223677068004642, 0.26520890968499666, 0.26520890968499666, 0.08840296989499888, 0.08840296989499888, 0.08840296989499888, 0.08840296989499888, 0.08840296989499888, 0.08840296989499888, 0.2765393625234095, 0.22123149001872758, 0.11061574500936379, 0.11061574500936379, 0.055307872504681894, 0.055307872504681894, 0.13826968126170475, 0.027653936252340947, 0.027653936252340947, 0.5788649654136292, 0.13620352127379512, 0.06810176063689756, 0.03405088031844878, 0.06810176063689756, 0.03405088031844878, 0.03405088031844878, 0.03405088031844878, 0.03405088031844878, 0.18046189544276636, 0.24061586059035517, 0.12030793029517758, 0.06015396514758879, 0.06015396514758879, 0.06015396514758879, 0.18046189544276636, 0.06015396514758879, 0.06015396514758879, 0.20551734705912295, 0.20551734705912295, 0.20551734705912295, 0.20551734705912295, 0.16206517471951737, 0.16206517471951737, 0.16206517471951737, 0.16206517471951737, 0.16206517471951737, 0.16206517471951737, 0.34176842072940034, 0.17696309531838517, 0.10536733919720644, 0.1310337423349875, 0.055385396244685435, 0.07699920941334316, 0.07564834609030205, 0.012157769907369973, 0.022964676491698838, 0.001350863323041108, 0.40663389585029763, 0.18125154272396987, 0.07565281783261352, 0.11978362823497139, 0.06146791448899848, 0.06619621560353682, 0.058315713745972914, 0.015761003715127814, 0.014184903343615034, 0.0015761003715127815, 0.29458333379946594, 0.24697390611470377, 0.08629208767863143, 0.12646254228764953, 0.06099957922110153, 0.07141414152714326, 0.07736531998773853, 0.02082912461208345, 0.013390151536339361, 0.001487794615148818, 0.19384996643228347, 0.275471004930087, 0.09182366831002901, 0.10202629812225446, 0.07141840868557812, 0.14283681737115625, 0.08162103849780357, 0.020405259624450892, 0.020405259624450892, 0.22092507835128092, 0.22092507835128092, 0.11046253917564046, 0.11046253917564046, 0.11046253917564046, 0.22092507835128092, 0.11046253917564046, 0.3176249241309641, 0.2365291988209307, 0.10136965663754173, 0.09123269097378756, 0.0844747138646181, 0.06082179398252504, 0.05406381687335559, 0.03378988554584724, 0.01689494277292362, 0.0033789885545847244, 0.27782776483550226, 0.20535095661754513, 0.13287414839958803, 0.11475494634509875, 0.09663574429060948, 0.0785165422361202, 0.042278138127141644, 0.03019867009081546, 0.018119202054489275, 0.23126923778780809, 0.2598209955393893, 0.09707597635537624, 0.12277255833179936, 0.08565527325474373, 0.07708974592926936, 0.07708974592926936, 0.028551757751581246, 0.019986230426106873, 0.0028551757751581245, 0.2336395939404484, 0.4088692893957847, 0.0584098984851121, 0.0584098984851121, 0.0584098984851121, 0.0584098984851121, 0.0584098984851121, 0.0584098984851121, 0.14234618876456345, 0.42703856629369036, 0.07117309438228173, 0.07117309438228173, 0.07117309438228173, 0.07117309438228173, 0.07117309438228173, 0.07117309438228173, 0.2841704406576042, 0.1420852203288021, 0.1420852203288021, 0.2131278304932032, 0.07104261016440105, 0.07104261016440105, 0.07104261016440105, 0.26948849017986953, 0.28446007296764003, 0.08982949672662317, 0.06396949009320135, 0.08982949672662317, 0.08710739076521035, 0.06669159605461417, 0.031304218556247466, 0.01633263576847694, 0.0013610529807064117, 0.10783567111613658, 0.32350701334840976, 0.32350701334840976, 0.10783567111613658, 0.3729039569681645, 0.09322598924204112, 0.09322598924204112, 0.18645197848408224, 0.09322598924204112, 0.09322598924204112, 0.09322598924204112, 0.09322598924204112, 0.4134938318080587, 0.11539362748131872, 0.06731294936410258, 0.1442420343516484, 0.09616135623443227, 0.0384645424937729, 0.048080678117216134, 0.009616135623443225, 0.05769681374065936, 0.2785063774769609, 0.24588851344812762, 0.08279919330396134, 0.09534452562274336, 0.07527199391269213, 0.08405372653583955, 0.08656279299959595, 0.02383613140568584, 0.025090664637564042, 0.0012545332318782021, 0.1994823765900332, 0.1994823765900332, 0.1994823765900332, 0.1994823765900332, 0.36857398066134944, 0.36857398066134944, 0.2602915728074922, 0.18220410096524453, 0.20823325824599376, 0.11713120776337149, 0.05205831456149844, 0.06507289320187305, 0.06507289320187305, 0.02602915728074922, 0.02602915728074922, 0.23162959784462692, 0.25736621982736324, 0.18015635387915427, 0.1029464879309453, 0.05147324396547265, 0.05147324396547265, 0.07720986594820897, 0.025736621982736324, 0.025736621982736324, 0.15872685476007, 0.15872685476007, 0.052908951586690005, 0.10581790317338001, 0.10581790317338001, 0.21163580634676002, 0.15872685476007, 0.052908951586690005, 0.2935006497946358, 0.164719752435765, 0.131775801948612, 0.09583694687171782, 0.09583694687171782, 0.050913378025600094, 0.1168012789999061, 0.032943950487153, 0.01497452294870591, 0.0029949045897411817, 0.3037913686629887, 0.13019630085556658, 0.08679753390371106, 0.13019630085556658, 0.08679753390371106, 0.08679753390371106, 0.08679753390371106, 0.04339876695185553, 0.04339876695185553, 0.2833893387294613, 0.2833893387294613, 0.2833893387294613, 0.2891935298345633, 0.18403224625835848, 0.07887096268215363, 0.10516128357620484, 0.05258064178810242, 0.15774192536430726, 0.05258064178810242, 0.02629032089405121, 0.02629032089405121, 0.3220688398626153, 0.2514397083137962, 0.06780396628686639, 0.12148210626396894, 0.06780396628686639, 0.05932847050100808, 0.06215363576296085, 0.025426487357574894, 0.02260132209562213, 0.002825165261952766, 0.1782613646795467, 0.3565227293590934, 0.1782613646795467, 0.1782613646795467, 0.1567718529628211, 0.3135437059256422, 0.07838592648141054, 0.07838592648141054, 0.1567718529628211, 0.07838592648141054, 0.07838592648141054, 0.5144741564890462, 0.16077317390282692, 0.032154634780565385, 0.09646390434169616, 0.032154634780565385, 0.032154634780565385, 0.06430926956113077, 0.032154634780565385, 0.35587604439036397, 0.19468513016649322, 0.10466942482069529, 0.08373553985655623, 0.09420248233862576, 0.06489504338883108, 0.06280165489241717, 0.02512066195696687, 0.01465371947489734, 0.002093388496413906, 0.26153573188492846, 0.2092285855079428, 0.2092285855079428, 0.0523071463769857, 0.1046142927539714, 0.0523071463769857, 0.0523071463769857, 0.12544200951783854, 0.2508840190356771, 0.12544200951783854, 0.12544200951783854, 0.2508840190356771, 0.12544200951783854, 0.12544200951783854, 0.18975318235897468, 0.18975318235897468, 0.18975318235897468, 0.09487659117948734, 0.18975318235897468, 0.09487659117948734, 0.09487659117948734, 0.37921070854487826, 0.15168428341795132, 0.07584214170897566, 0.07584214170897566, 0.22752642512692697, 0.07584214170897566, 0.3149770353387934, 0.13499015800234002, 0.13499015800234002, 0.08999343866822668, 0.04499671933411334, 0.04499671933411334, 0.13499015800234002, 0.04499671933411334, 0.31264868760849523, 0.1922020620544028, 0.11275854307191631, 0.10763315475046557, 0.08456890730393724, 0.08456890730393724, 0.05894196569668353, 0.020501553285802967, 0.023064247446528338, 0.002562694160725371, 0.1524697268311149, 0.4574091804933447, 0.0508232422770383, 0.1016464845540766, 0.0508232422770383, 0.1016464845540766, 0.0508232422770383, 0.0508232422770383, 0.5264924161013986, 0.12388056849444674, 0.12388056849444674, 0.06194028424722337, 0.06194028424722337, 0.030970142123611686, 0.06194028424722337, 0.030970142123611686, 0.22328139307131328, 0.15948670933665235, 0.12758936746932187, 0.0956920256019914, 0.22328139307131328, 0.06379468373466093, 0.06379468373466093, 0.03189734186733047, 0.03189734186733047, 0.20619631574737404, 0.20619631574737404, 0.10309815787368702, 0.10309815787368702, 0.20619631574737404, 0.10309815787368702, 0.18834792994169738, 0.2448523089242066, 0.09417396497084869, 0.09417396497084869, 0.07533917197667896, 0.18834792994169738, 0.07533917197667896, 0.01883479299416974, 0.01883479299416974, 0.48506926074045204, 0.24253463037022602, 0.6892598093359814, 0.06892598093359814, 0.03446299046679907, 0.1033889714003972, 0.03446299046679907, 0.03446299046679907, 0.03446299046679907, 0.2306450608429221, 0.4612901216858442, 0.42150935991546284, 0.26247160144996023, 0.32008731884141495, 0.051213971014626394, 0.06401746376828299, 0.04481222463779809, 0.15364191304387917, 0.06401746376828299, 0.025606985507313197, 0.012803492753656599, 0.21250688059510983, 0.26563360074388725, 0.1859435205207211, 0.05312672014877746, 0.13281680037194363, 0.05312672014877746, 0.07969008022316618, 0.02656336007438873, 0.02656336007438873, 0.24439151280238303, 0.30979205848189395, 0.09637975152770034, 0.08605334957830388, 0.07228481364577526, 0.06540054567951095, 0.06884267966264311, 0.034421339831321555, 0.024094937881925085, 0.003442133983132155, 0.28182108215706253, 0.18018069187090885, 0.11550044350699284, 0.08778033706531456, 0.10164039028615371, 0.07854030158475514, 0.10626040802643341, 0.02310008870139857, 0.02310008870139857, 0.16481843712716426, 0.4395158323391047, 0.054939479042388085, 0.054939479042388085, 0.054939479042388085, 0.054939479042388085, 0.054939479042388085, 0.054939479042388085, 0.19565409910903447, 0.22174131232357241, 0.09130524625088275, 0.10434885285815172, 0.06521803303634482, 0.16956688589449653, 0.10434885285815172, 0.02608721321453793, 0.02608721321453793, 0.2962240745827252, 0.2741178003601338, 0.08400384204584745, 0.07516133235681087, 0.11053137111295716, 0.053055058134219436, 0.04863380328970115, 0.026527529067109718, 0.026527529067109718, 0.004421254844518286, 0.2432796950255028, 0.2432796950255028, 0.1216398475127514, 0.1216398475127514, 0.2432796950255028, 0.1216398475127514, 0.4864933568787776, 0.17288606897344988, 0.06835030633834065, 0.06835030633834065, 0.06835030633834065, 0.04824727506235811, 0.05226788131755462, 0.016082425020786037, 0.016082425020786037, 0.16753929121403177, 0.4068811358055057, 0.023934184459147394, 0.07180255337744218, 0.07180255337744218, 0.11967092229573698, 0.09573673783658958, 0.023934184459147394, 0.023934184459147394, 0.32738993596098964, 0.1992808305849502, 0.1375986687372275, 0.08540607025069294, 0.06642694352831674, 0.08540607025069294, 0.06168216184772268, 0.01897912672237621, 0.01897912672237621, 0.2594795155840714, 0.2783127062313024, 0.11090656714480472, 0.06696245563459907, 0.11299914388338594, 0.06068472541885541, 0.0690550323731803, 0.023018344124393433, 0.016740613908649768, 0.002092576738581221, 0.3317135180310936, 0.2483509585258973, 0.08162583951550471, 0.0833625595051963, 0.05731175965982246, 0.07815239953612153, 0.0659953596082804, 0.029524239824757024, 0.022577359865990666, 0.0017367199896915896, 0.6215833757509589, 0.3289172322995894, 0.2141786628927559, 0.07649237960455567, 0.1376862832882002, 0.06501852266387233, 0.07266776062432789, 0.0688431416441001, 0.022947713881366703, 0.015298475920911134, 0.2370953055906461, 0.11854765279532305, 0.11854765279532305, 0.2370953055906461, 0.05927382639766152, 0.05927382639766152, 0.11854765279532305, 0.05927382639766152, 0.05927382639766152, 0.22641494797981013, 0.22641494797981013, 0.1811319583838481, 0.09056597919192405, 0.045282989595962025, 0.09056597919192405, 0.09056597919192405, 0.045282989595962025, 0.28556197701990726, 0.17133718621194435, 0.1142247908079629, 0.05711239540398145, 0.1142247908079629, 0.05711239540398145, 0.1142247908079629, 0.05711239540398145, 0.21091368145794206, 0.21091368145794206, 0.10545684072897103, 0.10545684072897103, 0.10545684072897103, 0.10545684072897103, 0.21091368145794206, 0.17621131586658434, 0.3524226317331687, 0.08810565793329217, 0.08810565793329217, 0.08810565793329217, 0.08810565793329217, 0.08810565793329217, 0.08810565793329217, 0.24553357073734994, 0.24553357073734994, 0.24553357073734994, 0.4201762649036555, 0.4201762649036555, 0.15947697890821189, 0.31895395781642377, 0.27908471308937083, 0.03986924472705297, 0.11960773418115891, 0.03986924472705297, 0.03986924472705297, 0.3226575579840896, 0.3226575579840896, 0.3226575579840896, 0.3774999298659824, 0.3774999298659824, 0.10403829048282566, 0.41615316193130264, 0.10403829048282566, 0.10403829048282566, 0.10403829048282566, 0.10403829048282566, 0.10403829048282566, 0.29690335366522685, 0.29690335366522685, 0.29690335366522685, 0.29834735912091237, 0.2512398813649788, 0.11305794661424048, 0.09421495551186707, 0.07223146589243142, 0.0628099703412447, 0.06909096737536918, 0.025123988136497886, 0.012561994068248943, 0.0031404985170622357, 0.22024252632850302, 0.27146171849792233, 0.10243838433883862, 0.11780414198966441, 0.07170686903718702, 0.11780414198966441, 0.061463030603303166, 0.025609596084709654, 0.010243838433883861, 0.324056486190012, 0.18517513496572113, 0.13888135122429085, 0.10801882873000399, 0.06172504498857371, 0.030862522494286854, 0.09258756748286057, 0.04629378374143028, 0.015431261247143427, 0.28145447190251976, 0.28145447190251976, 0.14072723595125988, 0.14072723595125988, 0.14072723595125988, 0.14072723595125988, 0.14072723595125988, 0.18607092941661785, 0.18607092941661785, 0.09303546470830892, 0.09303546470830892, 0.09303546470830892, 0.27910639412492677, 0.09303546470830892, 0.49573465598346067, 0.18026714763034932, 0.04506678690758733, 0.07511131151264555, 0.060089049210116444, 0.060089049210116444, 0.04506678690758733, 0.030044524605058222, 0.015022262302529111, 0.24062128398818497, 0.24062128398818497, 0.24062128398818497, 0.3656130493578913, 0.18280652467894565, 0.09140326233947282, 0.18280652467894565, 0.09140326233947282, 0.09140326233947282, 0.09140326233947282, 0.22823638949775582, 0.21193379024791614, 0.04890779774951911, 0.13042079399871762, 0.04890779774951911, 0.09781559549903822, 0.16302599249839703, 0.04890779774951911, 0.016302599249839702, 0.17095509328436065, 0.512865279853082, 0.17095509328436065, 0.31451105575002536, 0.31451105575002536, 0.31451105575002536, 0.28517353339247065, 0.23056583550880605, 0.060675219870738435, 0.14562052768977224, 0.060675219870738435, 0.12135043974147687, 0.060675219870738435, 0.01820256596122153, 0.024270087948295372, 0.23236033047842666, 0.11618016523921333, 0.23236033047842666, 0.11618016523921333, 0.11618016523921333, 0.11618016523921333, 0.11618016523921333, 0.41223754435390975, 0.18263688673907394, 0.06261836116768249, 0.07305475469562958, 0.09392754175152375, 0.046963770875761875, 0.06783655793165604, 0.03652737734781479, 0.031309180583841245, 0.14976749970122707, 0.38689937422816995, 0.03744187492530677, 0.07488374985061354, 0.024961249950204514, 0.12480624975102256, 0.0873643748257158, 0.04992249990040903, 0.04992249990040903, 0.18646126843753616, 0.3729225368750723, 0.2961020298312299, 0.16151019808976178, 0.13459183174146813, 0.13459183174146813, 0.08075509904488089, 0.05383673269658726, 0.08075509904488089, 0.05383673269658726, 0.1521994702312189, 0.4565984106936567, 0.07609973511560945, 0.07609973511560945, 0.07609973511560945, 0.07609973511560945, 0.07609973511560945, 0.07609973511560945, 0.2264057828512317, 0.2264057828512317, 0.2264057828512317, 0.2264057828512317, 0.3924177913742013, 0.2562127877977434, 0.20962864456179006, 0.08152225066291836, 0.15139846551684838, 0.0931682864719067, 0.08152225066291836, 0.0931682864719067, 0.03493810742696501, 0.011646035808988337, 0.21202688585789645, 0.21202688585789645, 0.17227184475954085, 0.10601344292894822, 0.09276176256282968, 0.07951008219671116, 0.07951008219671116, 0.026503360732237056, 0.026503360732237056, 0.281677912149408, 0.281677912149408, 0.281677912149408, 0.281677912149408, 0.1338015943953339, 0.3010535873895013, 0.2676031887906678, 0.033450398598833475, 0.1338015943953339, 0.033450398598833475, 0.06690079719766695, 0.37662600715319605, 0.37662600715319605, 0.3445261979750503, 0.23349405834328055, 0.06531302331280575, 0.09470388380356834, 0.08327410472382733, 0.06531302331280575, 0.0669458488956259, 0.024492383742302157, 0.02122673257666187, 0.0032656511656402876, 0.12875366461560536, 0.4720967702572197, 0.04291788820520179, 0.04291788820520179, 0.04291788820520179, 0.04291788820520179, 0.08583577641040357, 0.04291788820520179, 0.08583577641040357, 0.3092291528605581, 0.4158895013763022, 0.09969693332296918, 0.19939386664593836, 0.09969693332296918, 0.09969693332296918, 0.09969693332296918, 0.19939386664593836, 0.19939386664593836, 0.1883054138847014, 0.4331024519348132, 0.03766108277694028, 0.07532216555388056, 0.05649162416541042, 0.0941527069423507, 0.07532216555388056, 0.01883054138847014, 0.01883054138847014, 0.2436788509118773, 0.2436788509118773, 0.2436788509118773, 0.2436788509118773, 0.09113036941629128, 0.4556518470814564, 0.09113036941629128, 0.09113036941629128, 0.09113036941629128, 0.09113036941629128, 0.09113036941629128, 0.41120476367862074, 0.21061707407929353, 0.0735488195197533, 0.0735488195197533, 0.05349005055982058, 0.05683317871980937, 0.0735488195197533, 0.02674502527991029, 0.020058768959932718, 0.21651150113158685, 0.21651150113158685, 0.21651150113158685, 0.21651150113158685, 0.1288272499377207, 0.22544768739101123, 0.1288272499377207, 0.06441362496886036, 0.22544768739101123, 0.1288272499377207, 0.03220681248443018, 0.03220681248443018, 0.26604260661485934, 0.28267026952828805, 0.08313831456714355, 0.06651065165371484, 0.09976597748057224, 0.07482448311042919, 0.07482448311042919, 0.03325532582685742, 0.01662766291342871, 0.3685721694962941, 0.2156272556236823, 0.07772610377132734, 0.08524798478145579, 0.1002917468017127, 0.057667754410984796, 0.06017504808102762, 0.015043762020256904, 0.02005834936034254, 0.0025072936700428175, 0.312131573971173, 0.22524958946373308, 0.11262479473186654, 0.0740105793952266, 0.09331768706354657, 0.06757487683911993, 0.07722843067327992, 0.01930710766831998, 0.01930710766831998, 0.00321785127805333, 0.37762444936223233, 0.17725229255778252, 0.09247945698666914, 0.1001860784022249, 0.07706621415555762, 0.04623972849333457, 0.061652971324446096, 0.023119864246667284, 0.03853310707777881, 0.2076046087976618, 0.2076046087976618, 0.2076046087976618, 0.2076046087976618, 0.13846855712973377, 0.23078092854955626, 0.0923123714198225, 0.13846855712973377, 0.04615618570991125, 0.184624742839645, 0.0923123714198225, 0.04615618570991125, 0.5450087189058782, 0.09083478648431305, 0.09083478648431305, 0.13625217972646955, 0.045417393242156524, 0.045417393242156524, 0.045417393242156524, 0.6218625391309751, 0.08883750559013931, 0.08883750559013931, 0.08883750559013931, 0.08883750559013931, 0.08883750559013931, 0.2985581725680057, 0.2660419359516883, 0.0827686022960808, 0.09065132632427897, 0.07094451625378353, 0.07882724028198171, 0.06404713272911014, 0.02660419359516883, 0.019706810070495427, 0.001970681007049543, 0.22160621489940507, 0.22160621489940507, 0.22160621489940507, 0.22160621489940507, 0.34044965533292615, 0.22504299250820542, 0.08944016368915857, 0.09232533025977659, 0.08655499711854055, 0.0634736645535964, 0.0634736645535964, 0.020196165994326127, 0.017310999423708107, 0.0028851665706180183, 0.2645189307141896, 0.1763459538094597, 0.11021622113091231, 0.11021622113091231, 0.08817297690472985, 0.04408648845236492, 0.1322594653570948, 0.04408648845236492, 0.02204324422618246, 0.1659215624553044, 0.30167556810055346, 0.18100534086033207, 0.09050267043016604, 0.04525133521508302, 0.060335113620110695, 0.09050267043016604, 0.030167556810055347, 0.015083778405027674, 0.3292510156636103, 0.18931933400657594, 0.09877530469908309, 0.0699658408285172, 0.09054402930749283, 0.08642839161169771, 0.09465966700328797, 0.020578188478975645, 0.020578188478975645, 0.21649988871641362, 0.24742844424732985, 0.15464277765458115, 0.21649988871641362, 0.06185711106183246, 0.03092855553091623, 0.06185711106183246, 0.2596917780527206, 0.2097510515041205, 0.14982217964580033, 0.03995258123888009, 0.07990516247776018, 0.08989330778748021, 0.13983403433608033, 0.019976290619440045, 0.019976290619440045, 0.28024817301708654, 0.17126277239933066, 0.12455474356314958, 0.04670802883618109, 0.07784671472696848, 0.10898540061775587, 0.14012408650854327, 0.015569342945393697, 0.015569342945393697, 0.39184264541859193, 0.39184264541859193, 0.257073581456994, 0.19280518609274547, 0.128536790728497, 0.128536790728497, 0.0642683953642485, 0.128536790728497, 0.0642683953642485, 0.30435661487718363, 0.2179343662083537, 0.08642224866882992, 0.09393722681394556, 0.08642224866882992, 0.07139229237859862, 0.09393722681394556, 0.03005991258046258, 0.01502995629023129, 0.0037574890725578223, 0.3134455967929944, 0.223889711994996, 0.08955588479799839, 0.08955588479799839, 0.08955588479799839, 0.08955588479799839, 0.08955588479799839, 0.044777942398999196, 0.5308460281138246, 0.15925380843414738, 0.05308460281138246, 0.05308460281138246, 0.10616920562276493, 0.05308460281138246, 0.05308460281138246, 0.05308460281138246, 0.24296166539212372, 0.2871365136452371, 0.1325245447593402, 0.0883496965062268, 0.0662622723796701, 0.0662622723796701, 0.0662622723796701, 0.0220874241265567, 0.0220874241265567, 0.23630907244382132, 0.2987303368629439, 0.07133858790756871, 0.10254922011713001, 0.07133858790756871, 0.08025591139601479, 0.08471457314023784, 0.040127955698007395, 0.017834646976892177, 0.24162560965875587, 0.3373189204146988, 0.07655464860475433, 0.05502365368466718, 0.06698531752916005, 0.07655464860475433, 0.09808564352484149, 0.02870799322678288, 0.01435399661339144, 0.002392332768898573, 0.20915897536738653, 0.41831795073477307, 0.10457948768369327, 0.10457948768369327, 0.10457948768369327, 0.10457948768369327, 0.1746857206990186, 0.23291429426535815, 0.10190000374109419, 0.11645714713267907, 0.058228573566339537, 0.18924286409060348, 0.07278571695792442, 0.029114286783169768, 0.029114286783169768, 0.5738367322648389, 0.08828257419459061, 0.044141287097295305, 0.1324238612918859, 0.044141287097295305, 0.044141287097295305, 0.044141287097295305, 0.5899998704271009, 0.09833331173785016, 0.09833331173785016, 0.09833331173785016, 0.5559862284976581, 0.18532874283255268, 0.22269100532687136, 0.22269100532687136, 0.22269100532687136, 0.11134550266343568, 0.11134550266343568, 0.11134550266343568, 0.11134550266343568, 0.1102538613295948, 0.3307615839887844, 0.1102538613295948, 0.1102538613295948, 0.2205077226591896, 0.27793081595706687, 0.18528721063804457, 0.12352480709202972, 0.03088120177300743, 0.06176240354601486, 0.12352480709202972, 0.15440600886503714, 0.03088120177300743, 0.31802377028549433, 0.19447496744076992, 0.1395643883986702, 0.07550204618288715, 0.0800779277697288, 0.06635028300920386, 0.08465380935657044, 0.020591467140787405, 0.01830352634736658, 0.0022879407934208226, 0.345206561674753, 0.16382684282869633, 0.1287210907939757, 0.09361533875925505, 0.11116821477661537, 0.05850958672453441, 0.06436054539698785, 0.017552876017360323, 0.023403834689813763, 0.28462373691982257, 0.28462373691982257, 0.28462373691982257, 0.23086693431604216, 0.23086693431604216, 0.23086693431604216, 0.07695564477201405, 0.07695564477201405, 0.07695564477201405, 0.07695564477201405, 0.16490238321379605, 0.20612797901724508, 0.08245119160689802, 0.14428958531207156, 0.04122559580344901, 0.12367678741034704, 0.14428958531207156, 0.04122559580344901, 0.04122559580344901, 0.292907558330639, 0.26501160039438765, 0.11158383174500533, 0.07322688958265974, 0.1080968370029739, 0.0627659053565655, 0.04533093164640841, 0.017434973710157082, 0.017434973710157082, 0.22605133974900055, 0.22605133974900055, 0.22605133974900055, 0.22605133974900055, 0.26869542701738947, 0.17913028467825964, 0.08956514233912982, 0.08956514233912982, 0.08956514233912982, 0.08956514233912982, 0.08956514233912982, 0.08956514233912982, 0.18832626755315612, 0.28248940132973416, 0.18832626755315612, 0.09416313377657806, 0.09416313377657806, 0.09416313377657806, 0.09416313377657806, 0.15924445395672196, 0.15924445395672196, 0.15924445395672196, 0.15924445395672196, 0.15924445395672196, 0.15924445395672196, 0.2860884236220632, 0.17165305417323792, 0.11443536944882529, 0.22887073889765058, 0.057217684724412646, 0.057217684724412646, 0.057217684724412646, 0.38187181538654996, 0.1253753426688134, 0.43881369934084685, 0.0626876713344067, 0.1253753426688134, 0.0626876713344067, 0.0626876713344067, 0.0626876713344067, 0.33248680019466925, 0.15585318759125122, 0.12468255007300097, 0.0727314875425839, 0.0727314875425839, 0.062341275036500485, 0.09351191255475073, 0.05195106253041707, 0.020780425012166828, 0.44158401766236205, 0.19511851943220648, 0.06161637455753889, 0.051346978797949076, 0.11296335335548797, 0.03594288515856436, 0.051346978797949076, 0.030808187278769446, 0.015404093639384723, 0.6151671950649261, 0.07237261118410895, 0.07237261118410895, 0.10855891677616343, 0.03618630559205448, 0.03618630559205448, 0.03618630559205448, 0.4282005123697814, 0.19759252058928733, 0.33411098936006767, 0.08262959951915652, 0.0898147820860397, 0.10418514721980605, 0.061074051818506996, 0.08262959951915652, 0.025148138984091118, 0.01796295641720794, 0.003592591283441588, 0.407053670534459, 0.417176992093204, 0.18465211125436898, 0.08890657208543692, 0.07659643133514565, 0.06975746425165051, 0.053343943251262155, 0.07659643133514565, 0.013677934166990296, 0.019149107833786413, 0.0013677934166990296, 0.36049845848253564, 0.12016615282751189, 0.10013846068959324, 0.18024922924126782, 0.14019384496543053, 0.0400553842758373, 0.0400553842758373, 0.02002769213791865, 0.02002769213791865, 0.3373226494630742, 0.12649599354865285, 0.08433066236576856, 0.1686613247315371, 0.12649599354865285, 0.04216533118288428, 0.04216533118288428, 0.14029954745634157, 0.14029954745634157, 0.14029954745634157, 0.28059909491268314, 0.14029954745634157, 0.14029954745634157, 0.25359621439740004, 0.25359621439740004, 0.25359621439740004, 0.25359621439740004, 0.3018097904107187, 0.3018097904107187, 0.2599947115264893, 0.12999735576324464, 0.12999735576324464, 0.19499603364486698, 0.06499867788162232, 0.06499867788162232, 0.12999735576324464, 0.34346801429805607, 0.19542145641096292, 0.09178886588999774, 0.12435910862515823, 0.0710623477858047, 0.06958188220693377, 0.06810141662806284, 0.02072651810419304, 0.014804655788709313, 0.0014804655788709313, 0.5640935628336567, 0.1410233907084142, 0.0705116953542071, 0.0705116953542071, 0.0705116953542071, 0.30217933623673565, 0.14389492201749315, 0.12950542981574384, 0.08633695321049589, 0.08633695321049589, 0.08633695321049589, 0.12950542981574384, 0.02877898440349863, 0.014389492201749315, 0.23799886797985942, 0.25499878712127794, 0.11899943398992971, 0.08499959570709266, 0.06799967656567413, 0.05099975742425559, 0.11899943398992971, 0.016999919141418532, 0.033999838282837064, 0.24717597961273963, 0.18538198470955472, 0.12358798980636981, 0.06179399490318491, 0.12358798980636981, 0.06179399490318491, 0.18538198470955472, 0.06179399490318491, 0.19202538380264925, 0.19202538380264925, 0.19202538380264925, 0.19202538380264925, 0.3442094552579734, 0.15298198011465486, 0.07649099005732743, 0.038245495028663715, 0.11473648508599113, 0.07649099005732743, 0.15298198011465486, 0.038245495028663715, 0.15116932467449953, 0.15116932467449953, 0.15116932467449953, 0.15116932467449953, 0.15116932467449953, 0.15116932467449953, 0.3302303851878562, 0.2514725108961917, 0.0829030255701731, 0.06217726917762983, 0.07323100592031957, 0.0829030255701731, 0.07184928882748336, 0.02625262476388815, 0.01658060511403462, 0.0027634341856724366, 0.35699710238841054, 0.15984944883063157, 0.07992472441531578, 0.12787955906450527, 0.09590966929837895, 0.06926809449327369, 0.07459640945429473, 0.015984944883063158, 0.015984944883063158, 0.0053283149610210525, 0.2765911022450083, 0.20212426702519837, 0.15957178975673555, 0.07446683521980993, 0.13829555112250416, 0.06382871590269422, 0.06382871590269422, 0.02127623863423141, 0.02127623863423141, 0.30989737656230315, 0.26246410463950165, 0.0853798894610427, 0.08379878039694932, 0.058501035371455186, 0.08063656226876256, 0.07747434414057579, 0.02371663596140075, 0.015811090640933834, 0.0015811090640933834, 0.21732962815138854, 0.36851545643061534, 0.09449114267451676, 0.06614379987216173, 0.056694685604710054, 0.08504202840706508, 0.06614379987216173, 0.01889822853490335, 0.01889822853490335, 0.37989482226059046, 0.18763097928724284, 0.08686619411446428, 0.08107511450683333, 0.06833473937004524, 0.07528403489920238, 0.06601830752699285, 0.027797182116628568, 0.024322534352049997, 0.002316431843052381, 0.2997126476720274, 0.2491118110520747, 0.12066353347834868, 0.070062696858396, 0.07395506890608468, 0.06617032481070734, 0.07395506890608468, 0.023354232286132003, 0.019461860238443335, 0.003892372047688667, 0.3197327719883108, 0.18196174015594924, 0.1351715784015623, 0.10137868380117172, 0.09358032350877389, 0.057187975477584044, 0.057187975477584044, 0.02599453430799275, 0.023395080877193473, 0.002599453430799275, 0.3252389453918121, 0.2318192483111852, 0.09341969708062688, 0.08995970829986291, 0.06919977561527917, 0.11417962976521062, 0.03459988780763958, 0.024219921465347707, 0.013839955123055832, 0.3078927749792286, 0.2859004339092837, 0.0439846821398898, 0.0439846821398898, 0.0439846821398898, 0.1759387285595592, 0.0879693642797796, 0.0219923410699449, 0.3408324296611003, 0.24641263495768737, 0.10363148199155076, 0.07829934194917168, 0.06448181101696492, 0.04836135826272369, 0.06678473283899938, 0.027635061864413536, 0.02072629639831015, 0.002302921822034461, 0.2830013191142059, 0.2830013191142059, 0.3250693898465436, 0.21361702761344295, 0.06501387796930871, 0.09287696852758388, 0.06501387796930871, 0.13002775593861743, 0.06501387796930871, 0.018575393705516776, 0.018575393705516776, 0.29861171483992843, 0.29861171483992843, 0.2695671335187151, 0.17971142234581006, 0.08985571117290503, 0.08985571117290503, 0.17971142234581006, 0.08985571117290503, 0.08985571117290503, 0.30002845923133326, 0.2111311379776049, 0.10556556898880246, 0.0944534038320864, 0.08334123867537035, 0.05556082578358024, 0.10556556898880246, 0.01666824773507407, 0.022224330313432093, 0.26928135293206046, 0.24338891515013159, 0.06732033823301511, 0.10874823868410134, 0.08285580090217246, 0.06732033823301511, 0.09839126357132978, 0.02589243778192889, 0.031070925338314667, 0.2485092359758436, 0.2485092359758436, 0.2485092359758436, 0.2485092359758436, 0.2868253599839899, 0.20487525713142135, 0.11609597904113876, 0.09560845332799663, 0.12292515427885281, 0.061462577139426405, 0.061462577139426405, 0.020487525713142135, 0.020487525713142135, 0.22657227971504149, 0.18881023309586792, 0.07552409323834716, 0.15104818647669432, 0.03776204661917358, 0.07552409323834716, 0.15104818647669432, 0.03776204661917358, 0.03776204661917358, 0.11578912753904504, 0.23157825507809007, 0.11578912753904504, 0.11578912753904504, 0.11578912753904504, 0.11578912753904504, 0.11578912753904504, 0.3127941407951661, 0.3127941407951661, 0.5495940167657283, 0.1697275640011808, 0.06465811961949745, 0.088904914476809, 0.0404113247621859, 0.03232905980974873, 0.03232905980974873, 0.008082264952437182, 0.008082264952437182, 0.3561885521013166, 0.16071922472864286, 0.09556278227108493, 0.13031288491511583, 0.08253149377957336, 0.06081267962705405, 0.07384396811856564, 0.026062576983023163, 0.017375051322015445, 0.31880571619312054, 0.31880571619312054, 0.31880571619312054, 0.37473863143053604, 0.09368465785763401, 0.09368465785763401, 0.18736931571526802, 0.09368465785763401, 0.09368465785763401, 0.09368465785763401, 0.30561324113247584, 0.30638680875764324, 0.30638680875764324, 0.47622864416965655, 0.23811432208482827, 0.4451542157420952, 0.1483847385806984, 0.1483847385806984, 0.1483847385806984, 0.1483847385806984, 0.37891523681577677, 0.20433648896348172, 0.09125707274097242, 0.07538627748167287, 0.0674508798520231, 0.04562853637048621, 0.08530552451873509, 0.03372543992601155, 0.01983849407412444, 0.001983849407412444, 0.30854392501820754, 0.22342835949594342, 0.09575501121254718, 0.07447611983198113, 0.08511556552226415, 0.06383667414169812, 0.09575501121254718, 0.03191833707084906, 0.02659861422570755, 0.0053197228451415095, 0.40471654168378646, 0.40471654168378646, 0.360470104796016, 0.120156701598672, 0.120156701598672, 0.240313403197344, 0.120156701598672, 0.46430813948337424, 0.3242341825464219, 0.3242341825464219, 0.23978234785392524, 0.39237111467005953, 0.06539518577834325, 0.06539518577834325, 0.02179839525944775, 0.02179839525944775, 0.10899197629723875, 0.0435967905188955, 0.02179839525944775, 0.34320711395345893, 0.17752092101040978, 0.08284309647152457, 0.059173640336803265, 0.11834728067360653, 0.059173640336803265, 0.08284309647152457, 0.04733891226944261, 0.023669456134721305, 0.30804391019101296, 0.30804391019101296, 0.30804391019101296, 0.17953077924508185, 0.17953077924508185, 0.17953077924508185, 0.17953077924508185, 0.17953077924508185, 0.17953077924508185], \"Term\": [\"abstention\", \"abstention\", \"abstention\", \"abstention\", \"abstention\", \"abstention\", \"abstention\", \"abstention\", \"acoustics\", \"acoustics\", \"acoustics\", \"acoustics\", \"acoustics\", \"acoustics\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"adaboost\", \"advice\", \"advice\", \"advice\", \"advice\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"agesq\", \"agesq\", \"agesq\", \"agesq\", \"agesq\", \"agesq\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"al\", \"alcohol\", \"alcohol\", \"alcohol\", \"alcohol\", \"alcohol\", \"alcohol\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"amin\", \"amin\", \"amin\", \"amin\", \"amin\", \"amin\", \"amplification\", \"amplification\", \"amplification\", \"amplification\", \"amplification\", \"amplification\", \"amplification\", \"amplification\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"arc\", \"ard\", \"ard\", \"ard\", \"ard\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asap\", \"asthma\", \"asthma\", \"asthma\", \"asthma\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"asymptotically\", \"attained\", \"attained\", \"attained\", \"attained\", \"attained\", \"attained\", \"attained\", \"bacon\", \"bacon\", \"bacon\", \"bacon\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"basket\", \"basket\", \"basket\", \"basket\", \"basketball\", \"basketball\", \"basketball\", \"basketball\", \"basketball\", \"basketball\", \"basketball\", \"basketball\", \"basketball\", \"battery\", \"battery\", \"battery\", \"bianchi\", \"bianchi\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"boosting\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"boundaries\", \"boundaries\", \"boundaries\", \"boundaries\", \"boundaries\", \"boundaries\", \"boundaries\", \"boundaries\", \"bsrm\", \"bsrm\", \"bsrm\", \"bsrm\", \"bsrm\", \"bsrm\", \"bsrm\", \"buffer\", \"buffer\", \"buffer\", \"bunching\", \"bunching\", \"bunching\", \"bunching\", \"bunching\", \"bunching\", \"bunching\", \"cancers\", \"capacitance\", \"capacitance\", \"capacitance\", \"capacitance\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"cart\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"cells\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"center\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"cfgj\", \"cfn\", \"cfn\", \"cfn\", \"cfn\", \"cfn\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"church\", \"church\", \"church\", \"church\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"clock\", \"clock\", \"clock\", \"clock\", \"clock\", \"clock\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"clusters\", \"cmtt\", \"cmtt\", \"cmtt\", \"cmtt\", \"cmtt\", \"cmtt\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coins\", \"coins\", \"coins\", \"coins\", \"cole\", \"cole\", \"collard\", \"collard\", \"collard\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"composes\", \"composes\", \"composes\", \"concisely\", \"concisely\", \"concisely\", \"concisely\", \"conjugation\", \"consisted\", \"consisted\", \"consisted\", \"consisted\", \"consisted\", \"consisted\", \"contract\", \"contract\", \"contract\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"counterphase\", \"counterphase\", \"counterphase\", \"counterphase\", \"counterphase\", \"counterphase\", \"cowan\", \"cowan\", \"cowan\", \"cowan\", \"cowan\", \"cowan\", \"cq\", \"cq\", \"cq\", \"cq\", \"cse\", \"cse\", \"ct\", \"ct\", \"ct\", \"ct\", \"ct\", \"ct\", \"ct\", \"ct\", \"ct\", \"cumulative\", \"cumulative\", \"cumulative\", \"cumulative\", \"cumulative\", \"cumulative\", \"cumulative\", \"cumulative\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cy\", \"cyj\", \"cyj\", \"cyj\", \"cyj\", \"cyj\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"days\", \"days\", \"days\", \"days\", \"days\", \"days\", \"days\", \"defender\", \"defender\", \"defender\", \"defender\", \"defender\", \"defender\", \"deg\", \"deg\", \"deg\", \"deg\", \"deg\", \"deg\", \"delayed\", \"delayed\", \"delayed\", \"delayed\", \"delayed\", \"delayed\", \"delayed\", \"deletion\", \"deletion\", \"deletion\", \"deletion\", \"deletion\", \"deletion\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"density\", \"desirability\", \"desirability\", \"desirability\", \"desirability\", \"desirability\", \"desirability\", \"destsp\", \"destsp\", \"destsp\", \"destsp\", \"destsp\", \"deviance\", \"deviance\", \"deviance\", \"deviance\", \"deviance\", \"deviance\", \"deviance\", \"diameter\", \"diameter\", \"diameter\", \"diameter\", \"diameter\", \"diameter\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"diffeomorphic\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"distribute\", \"distribute\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"dk\", \"dk\", \"dk\", \"dk\", \"dk\", \"dk\", \"dk\", \"dk\", \"drifting\", \"drifting\", \"drifting\", \"drifting\", \"drifting\", \"drifting\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dropout\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dt\", \"dtol\", \"dtol\", \"dtol\", \"dtol\", \"dtol\", \"dtol\", \"dtol\", \"dtol\", \"dtw\", \"dtw\", \"dtw\", \"dtw\", \"dtw\", \"dtw\", \"dyadic\", \"dyadic\", \"dyadic\", \"dyadic\", \"dyadic\", \"dyadic\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"ecoc\", \"eest\", \"eest\", \"eest\", \"eest\", \"eest\", \"eest\", \"eht\", \"eht\", \"eht\", \"eht\", \"eht\", \"eht\", \"eht\", \"eighteen\", \"eighteen\", \"ell\", \"ell\", \"ell\", \"emu\", \"entropic\", \"entropic\", \"entropic\", \"entropic\", \"entropic\", \"entropic\", \"entropic\", \"entropic\", \"epidemiological\", \"epidemiological\", \"epidemiological\", \"epidemiological\", \"equalization\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"est\", \"est\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"ex_sf\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excerpted\", \"excerpted\", \"excerpted\", \"excite\", \"executed\", \"executed\", \"executed\", \"executed\", \"executed\", \"executed\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"fanty\", \"fanty\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"ferster\", \"ferster\", \"ferster\", \"ferster\", \"ferster\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"flow\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"fpras\", \"fpras\", \"fpras\", \"fpras\", \"fpras\", \"fractional\", \"fractional\", \"fractional\", \"fractional\", \"fractional\", \"fractional\", \"fractional\", \"fractional\", \"freq\", \"freq\", \"freq\", \"freq\", \"friendster\", \"friendster\", \"friendster\", \"friendster\", \"friendster\", \"friendster\", \"fu\", \"fu\", \"fu\", \"fu\", \"fu\", \"fu\", \"fu\", \"fu\", \"fu\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"gam\", \"gam\", \"gam\", \"gam\", \"gam\", \"gating\", \"gating\", \"gating\", \"gating\", \"gating\", \"gating\", \"gating\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gaze\", \"gbad\", \"gbad\", \"gbad\", \"gbad\", \"gbad\", \"gbad\", \"ggood\", \"ggood\", \"ggood\", \"ggood\", \"ggood\", \"ggood\", \"ggood\", \"ggood\", \"ggood\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"gj\", \"gj\", \"gj\", \"gj\", \"gj\", \"gj\", \"gj\", \"gj\", \"gj\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"gmax\", \"gmax\", \"gmax\", \"grating\", \"grating\", \"grating\", \"grating\", \"grating\", \"grating\", \"grating\", \"gratings\", \"gratings\", \"gratings\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"groups\", \"groups\", \"groups\", \"groups\", \"groups\", \"groups\", \"groups\", \"groups\", \"groups\", \"gti\", \"gti\", \"gti\", \"gti\", \"gti\", \"gti\", \"gti\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"gv\", \"gv\", \"gv\", \"gv\", \"gv\", \"gv\", \"gv\", \"gv\", \"hab\", \"hab\", \"hab\", \"hab\", \"hab\", \"haffner\", \"haffner\", \"haffner\", \"haffner\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"ham\", \"hammond\", \"hammond\", \"hamq\", \"hamq\", \"hamq\", \"hams\", \"hams\", \"hams\", \"hams\", \"hams\", \"hang\", \"hang\", \"hang\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"harmonic\", \"heading\", \"heading\", \"heading\", \"heading\", \"heading\", \"heading\", \"heading\", \"hedge\", \"hedge\", \"hedge\", \"hedge\", \"hedge\", \"hedge\", \"hedge\", \"hedge\", \"hedge\", \"heterogeneous\", \"heterogeneous\", \"heterogeneous\", \"heterogeneous\", \"heterogeneous\", \"hfw\", \"hfw\", \"hfw\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hild\", \"hild\", \"hild\", \"hm\", \"hm\", \"hm\", \"hm\", \"hm\", \"hm\", \"hm\", \"hm\", \"hm\", \"holdout\", \"holdout\", \"holdout\", \"holdout\", \"holdout\", \"holdout\", \"holdout\", \"holdout\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"htr\", \"htr\", \"htr\", \"hull\", \"hull\", \"hull\", \"hull\", \"hull\", \"hull\", \"hull\", \"hull\", \"hull\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hyperplane\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypotheses\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"iearning\", \"iearning\", \"iearning\", \"iearning\", \"iearning\", \"iin\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"ij\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"impossibility\", \"impossibility\", \"impossibility\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"infomax\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inp\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"interrupt\", \"interrupt\", \"interrupt\", \"interrupt\", \"interrupt\", \"interspike\", \"interspike\", \"interspike\", \"invariances\", \"invariances\", \"invariances\", \"invariances\", \"invariances\", \"invariances\", \"invariances\", \"investigations\", \"investigations\", \"items\", \"items\", \"items\", \"items\", \"items\", \"items\", \"items\", \"items\", \"items\", \"jfc\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kv\", \"kv\", \"kv\", \"kv\", \"kv\", \"kv\", \"kv\", \"kv\", \"kvi\", \"kvi\", \"kvkbad\", \"kxg\", \"kxg\", \"kxg\", \"kxg\", \"kxg\", \"kxg\", \"kxg\", \"kxg\", \"kxgj\", \"kxgj\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labeler\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"labels\", \"ladder\", \"ladder\", \"ladder\", \"ladder\", \"ladder\", \"ladder\", \"ladder\", \"ladder\", \"lancet\", \"lancet\", \"lancet\", \"lancet\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"lappe\", \"lappe\", \"lappe\", \"lappe\", \"lappe\", \"lappe\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"lexp\", \"lexp\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"llatlll\", \"llatlll\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"lo\", \"lo\", \"lo\", \"lo\", \"lo\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"longtin\", \"longtin\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lr\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsbm\", \"lsdd\", \"lsdd\", \"lsdd\", \"lsdd\", \"lsdd\", \"lsdd\", \"lsdd\", \"lsdd\", \"lungsq\", \"lungsq\", \"lungsq\", \"lungsq\", \"lungsq\", \"maas\", \"maas\", \"maas\", \"mann\", \"mann\", \"mann\", \"mann\", \"mann\", \"mann\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margins\", \"margins\", \"margins\", \"margins\", \"margins\", \"margins\", \"margins\", \"marmann\", \"marmann\", \"marmann\", \"marmann\", \"marmann\", \"marmann\", \"marmann\", \"marmann\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"maxgj\", \"maxgj\", \"maxgj\", \"maxgj\", \"maxgj\", \"maxgj\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"melscale\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"micro\", \"micro\", \"micro\", \"micro\", \"micro\", \"micro\", \"micro\", \"micro\", \"micro\", \"mika\", \"mika\", \"mika\", \"milton\", \"milton\", \"minima\", \"minima\", \"minima\", \"minima\", \"minima\", \"minima\", \"minima\", \"minima\", \"minima\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movshon\", \"movshon\", \"movshon\", \"movshon\", \"movshon\", \"mprove\", \"mprove\", \"mprove\", \"mprove\", \"mprove\", \"mprove\", \"mprove\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"muller\", \"muller\", \"muller\", \"muller\", \"muller\", \"muller\", \"muller\", \"muller\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multiresolution\", \"multitask\", \"multitask\", \"multitask\", \"multitask\", \"multitask\", \"multitask\", \"multitask\", \"multitask\", \"multitask\", \"mutual\", \"mutual\", \"mutual\", \"mutual\", \"mutual\", \"mutual\", \"mutual\", \"mutual\", \"mutual\", \"nav\", \"nav\", \"nav\", \"nav\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"nelson\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neurophysiol\", \"neurophysiol\", \"neurophysiol\", \"neurophysiol\", \"neurophysiol\", \"neurophysiol\", \"neurophysiol\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"next\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"normalhedge\", \"normalhedge\", \"normalhedge\", \"normalhedge\", \"normalhedge\", \"normalhedge\", \"normalhedge\", \"normalhedge\", \"null\", \"null\", \"null\", \"null\", \"null\", \"null\", \"null\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"ohira\", \"ohira\", \"ohira\", \"ohira\", \"ols\", \"ols\", \"ols\", \"ols\", \"ols\", \"ols\", \"ols\", \"ols\", \"omp\", \"omp\", \"omp\", \"omp\", \"omp\", \"omp\", \"omp\", \"omp\", \"omp\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"onoda\", \"onoda\", \"onoda\", \"onoda\", \"ook\", \"ook\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinions\", \"opinions\", \"opinions\", \"opinions\", \"opinions\", \"opinions\", \"opinions\", \"opinions\", \"opinions\", \"optic\", \"optic\", \"optic\", \"optic\", \"optic\", \"optic\", \"optic\", \"optic\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"ord\", \"ord\", \"ord\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"p_\", \"p_\", \"p_\", \"p_\", \"pac\", \"pac\", \"pac\", \"pac\", \"pac\", \"pac\", \"pac\", \"packets\", \"packets\", \"packets\", \"packets\", \"packets\", \"packets\", \"packets\", \"packets\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parity\", \"parr\", \"parr\", \"parr\", \"parr\", \"parr\", \"parr\", \"parr\", \"patrol\", \"patrol\", \"patrol\", \"patrol\", \"patrol\", \"patrol\", \"patrol\", \"pddp\", \"pddp\", \"pddp\", \"pddp\", \"pddp\", \"pddp\", \"penalization\", \"penalization\", \"penalization\", \"penalization\", \"penalization\", \"penalization\", \"penalization\", \"penalization\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"personalized\", \"perturbations\", \"perturbations\", \"perturbations\", \"perturbations\", \"perturbations\", \"perturbations\", \"perturbations\", \"perturbations\", \"pham\", \"pham\", \"pham\", \"pham\", \"pham\", \"pham\", \"pham\", \"pham\", \"pham\", \"phams\", \"phams\", \"phams\", \"phams\", \"phams\", \"phams\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phenotype\", \"phenotype\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"phoneme\", \"pit\", \"pit\", \"pjlll\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"pl\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"poly\", \"poly\", \"poly\", \"poly\", \"poly\", \"poly\", \"poly\", \"poly\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"population\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"pram\", \"pram\", \"pram\", \"pram\", \"pram\", \"pram\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"private\", \"private\", \"private\", \"private\", \"private\", \"private\", \"private\", \"private\", \"private\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"probw\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"projections\", \"projections\", \"projections\", \"projections\", \"projections\", \"projections\", \"projections\", \"projections\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"pupil\", \"pupil\", \"pupil\", \"pupil\", \"pupil\", \"pupil\", \"pupil\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pv\", \"pyk\", \"pyk\", \"pyk\", \"qiip\", \"qiip\", \"ql\", \"ql\", \"ql\", \"ql\", \"ql\", \"ql\", \"ql\", \"qllp\", \"qllp\", \"qllp\", \"qlq\", \"qlq\", \"quantile\", \"quantile\", \"quantile\", \"quantile\", \"quantile\", \"quantile\", \"quantile\", \"quantiles\", \"quantiles\", \"quantiles\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ranking\", \"ratsch\", \"ratsch\", \"ratsch\", \"ratsch\", \"ratsch\", \"ratsch\", \"ratsch\", \"rauschecker\", \"rauschecker\", \"rauschecker\", \"rauschecker\", \"rauschecker\", \"rauschecker\", \"rauschecker\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rbf\", \"rdj\", \"rdj\", \"rdj\", \"rdm\", \"rdm\", \"rdm\", \"rdm\", \"rdm\", \"rdm\", \"rdm\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"receptive\", \"recommender\", \"recommender\", \"recommender\", \"rectified\", \"rectified\", \"rectified\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"reductions\", \"reductions\", \"reductions\", \"reductions\", \"reductions\", \"reductions\", \"reductions\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regrets\", \"regrets\", \"relevance\", \"relevance\", \"relevance\", \"relevance\", \"relevance\", \"relevance\", \"relevance\", \"relevance\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replication\", \"replications\", \"replications\", \"replications\", \"replications\", \"reporting\", \"representations\", \"representations\", \"representations\", \"representations\", \"representations\", \"representations\", \"representations\", \"representations\", \"representations\", \"require\", \"require\", \"require\", \"require\", \"require\", \"require\", \"require\", \"require\", \"require\", \"residence\", \"residence\", \"residence\", \"residence\", \"resonance\", \"resonance\", \"resonance\", \"resonance\", \"resonance\", \"resonance\", \"resonance\", \"resonant\", \"resonant\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"rit\", \"rit\", \"rit\", \"rit\", \"rit\", \"rit\", \"rit\", \"rit\", \"rit\", \"rni\", \"ropp\", \"rotational\", \"rotational\", \"rotational\", \"rotational\", \"rotational\", \"rotational\", \"rotational\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rts\", \"rts\", \"rts\", \"rts\", \"saddle\", \"saddle\", \"saddle\", \"saddle\", \"saddle\", \"saddle\", \"saddle\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"samples\", \"sato\", \"sato\", \"sato\", \"sato\", \"sbm\", \"sbm\", \"sbm\", \"sbm\", \"sbm\", \"sbm\", \"sbm\", \"sbm\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selectivities\", \"selectivities\", \"selectivities\", \"selectivities\", \"selectivity\", \"selectivity\", \"selectivity\", \"selectivity\", \"selectivity\", \"selectivity\", \"selectivity\", \"selectivity\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"sentences\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"shapley\", \"shapley\", \"shapley\", \"shapley\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"sin\", \"sin\", \"sin\", \"sin\", \"sin\", \"sin\", \"sin\", \"sin\", \"sin\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"sites\", \"sites\", \"sites\", \"sites\", \"sites\", \"sites\", \"sites\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"skills\", \"slack\", \"slack\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smola\", \"smola\", \"smola\", \"smola\", \"smola\", \"smola\", \"smola\", \"smola\", \"snps\", \"snps\", \"snps\", \"snps\", \"snps\", \"snps\", \"snps\", \"snps\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spark\", \"spark\", \"spark\", \"spark\", \"spark\", \"spark\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"speakers\", \"speakers\", \"speakers\", \"speakers\", \"sphinx\", \"sphinx\", \"spiking\", \"spiking\", \"spiking\", \"spiking\", \"spiking\", \"spiking\", \"spiking\", \"splittable\", \"splittable\", \"splittable\", \"splittable\", \"splittable\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"sps\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"statsci\", \"statsci\", \"statsci\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"steady\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stopped\", \"stopped\", \"stopped\", \"stopped\", \"stops\", \"stops\", \"stops\", \"stops\", \"stops\", \"stops\", \"stops\", \"stops\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"streams\", \"striate\", \"striate\", \"striate\", \"striate\", \"striate\", \"striate\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"subjects\", \"suboptimality\", \"substances\", \"substances\", \"substances\", \"substances\", \"substances\", \"substances\", \"substances\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"svm\", \"tasks\", \"tasks\", \"tasks\", \"tasks\", \"tasks\", \"tasks\", \"tasks\", \"tasks\", \"tasks\", \"tdnn\", \"tdnn\", \"tdnn\", \"tdnn\", \"tdnn\", \"tdnn\", \"tdnn\", \"thalamic\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"therangeofc\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timeline\", \"timeline\", \"timeline\", \"timeline\", \"timeline\", \"timeline\", \"timeline\", \"timeline\", \"timeline\", \"timestamp\", \"timestamp\", \"timestamp\", \"timestamp\", \"timestamp\", \"timestamp\", \"timestamp\", \"tobacco\", \"tobacco\", \"tobacco\", \"tobacco\", \"tobacco\", \"tobacco\", \"todoor\", \"todoor\", \"todoor\", \"todoor\", \"trader\", \"trader\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"traits\", \"traits\", \"traits\", \"traits\", \"traits\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"transistors\", \"transistors\", \"transistors\", \"transistors\", \"transistors\", \"transistors\", \"transistors\", \"transistors\", \"transitivity\", \"transitivity\", \"transitivity\", \"transitivity\", \"triangulated\", \"triangulated\", \"triangulated\", \"triangulated\", \"triangulated\", \"triangulated\", \"triangulated\", \"triangulated\", \"triple\", \"triple\", \"triple\", \"triple\", \"triple\", \"triple\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"updates\", \"updates\", \"updates\", \"updates\", \"updates\", \"updates\", \"updates\", \"updates\", \"updates\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"users\", \"users\", \"users\", \"users\", \"users\", \"users\", \"users\", \"users\", \"users\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"values\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variables\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"vec\", \"vec\", \"vec\", \"vec\", \"vec\", \"vec\", \"vec\", \"vec\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vet\", \"vet\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vjm\", \"vjm\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"vw\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"winfo\", \"winfo\", \"winfo\", \"winfo\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"without\", \"wk\", \"wk\", \"wk\", \"wk\", \"wk\", \"wk\", \"wk\", \"wk\", \"wk\", \"wkt\", \"wkt\", \"wkt\", \"wkt\", \"wkt\", \"wkt\", \"wkt\", \"wli\", \"wli\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wtxt\", \"wtxt\", \"wtxt\", \"xg\", \"xg\", \"xg\", \"xg\", \"xg\", \"xg\", \"xg\", \"xgbad\", \"xggood\", \"xggood\", \"xgi\", \"xgi\", \"xgj\", \"xgj\", \"xgj\", \"xgj\", \"xgj\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xj\", \"xpt\", \"xpt\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yhi\", \"yishai\", \"yishai\", \"zeta\", \"zeta\", \"zeta\", \"zeta\", \"zeta\", \"zeta\", \"zeta\", \"zeta\", \"zeta\", \"zi\", \"zi\", \"zi\", \"zi\", \"zi\", \"zi\", \"zi\", \"zi\", \"zi\", \"zl\", \"zl\", \"zl\", \"zy\", \"zy\", \"zy\", \"zy\", \"zy\", \"zy\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 5, 7, 4, 2, 9, 10, 1, 6, 3]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el2241344626287585766110286463\", ldavis_el2241344626287585766110286463_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el2241344626287585766110286463\", ldavis_el2241344626287585766110286463_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el2241344626287585766110286463\", ldavis_el2241344626287585766110286463_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "os.makedirs(os.path.dirname(LDAvis_data_filepath), exist_ok=True)\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5OKWQS38JIv"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}